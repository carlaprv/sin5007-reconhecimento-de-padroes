{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of 06 - k-fold cross validation + naive Bayes v03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlaprv/sin5007-reconhecimento-de-padroes/blob/master/06%20-%20k-fold%20cross%20validation%20%2B%20naive%20Bayes%20v05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikayTX9-yVZk",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "-------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiZnnw89yVZm",
        "colab_type": "text"
      },
      "source": [
        "# Bibliotecas Necessárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xYCeWYlyVZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np\n",
        "import seaborn as sns # visualize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRJncBBWyVZt",
        "colab_type": "text"
      },
      "source": [
        "# Funções Auxiliares\n",
        "\n",
        "describe_dataset() : realiza o cálculo das proporções de classes do dataset original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KutB_XTYyVZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def describe_dataset(X, y, k):\n",
        "    # get dataset rows: instances , columns: features\n",
        "    rows, columns = X.shape\n",
        "    # get proportion from target\n",
        "    (unique, counts) = np.unique(y, return_counts=True) \n",
        "    # calculate proportion\n",
        "    prop_neg = int(counts[0]/rows*100)\n",
        "    prop_pos = int(counts[1]/rows*100)\n",
        "\n",
        "    print(\"k = {}, Dataset: {} positivas, {} negativas ({}% x {}%)\".format(k, counts[1], counts[0], prop_pos, prop_neg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z4wMlsnyVZz",
        "colab_type": "text"
      },
      "source": [
        "get_classes_from_index() : realiza o cálculo das proporções de classes dos folds criados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEZ4jxJKyVZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classes_from_index(y, skf):\n",
        "    _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)\n",
        "    y_counts = np.bincount(y_inv)\n",
        "    _, class_perm = np.unique(y_idx, return_inverse=True)\n",
        "    y_encoded = class_perm[y_inv]\n",
        "    y_order = np.sort(y_encoded)\n",
        "    n_classes = len(y_idx)\n",
        "    allocation = np.asarray(\n",
        "            [np.bincount(y_order[i::skf.n_splits], minlength=n_classes)\n",
        "             for i in range(skf.n_splits)])\n",
        "\n",
        "    for idx, f in enumerate(allocation):\n",
        "        count_neg = int(f[0])\n",
        "        count_pos = int(f[1])\n",
        "        total = count_neg+count_pos\n",
        "        prop_temp_neg = int(count_neg/total*100)\n",
        "        prop_temp_pos = int(count_pos/total*100)\n",
        "        print(\"Fold {}: Pos: {}, Neg: {}, Total: {}, Proporção: {}% x {}%\".format(idx, count_pos, count_neg, total, prop_temp_pos, prop_temp_neg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RsPfCv4yVZ4",
        "colab_type": "text"
      },
      "source": [
        "# Função que aplica o Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YezZQsp7yVZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stratified_k_fold(X, y, list_c, k, name):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------    \n",
        "       X : array-like, shape (n_samples, n_features)\n",
        "           Training data, where n_samples is the number of samples\n",
        "           and n_features is the number of features.\n",
        "       y : array-like, of length n_samples\n",
        "           The target variable for supervised learning problems.\n",
        "       k : int\n",
        "           Determines the number of folds.\n",
        "    name : method selection (string)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### Estratifica o dataset em k folds\n",
        "    skf = StratifiedKFold(n_splits=k)\n",
        "    describe_dataset(X, y, k)\n",
        "    get_classes_from_index(y, skf)\n",
        "    \n",
        "    \n",
        "    ### Lista para armazenar os resultados de cada valor de c\n",
        "    ### Armazena um array bidimensional, onde terá o valor do c e uma lista dos resultados de c\n",
        "    result = []\n",
        "    reports_g = []\n",
        "    \n",
        "    \n",
        "    ### Executa o treino e teste para cada valor do parametro c\n",
        "    for c in list_c:\n",
        "        print(\"c =  {}\" .format(c))\n",
        "\n",
        "        ### create naive bayes classifier\n",
        "        clf = GaussianNB(var_smoothing = c)\n",
        "        \n",
        "        \n",
        "        ### Array para guardar os resultados dos testes para o parametro c\n",
        "        \"\"\"\n",
        "        Coluna 0 : Armazena o valor de c\n",
        "        Coluna 1 : Armazena o resultado \n",
        "        \"\"\"\n",
        "        result_c = []\n",
        "\n",
        "                \n",
        "        ### resultado do fold-k\n",
        "        result_k = []\n",
        "        ### Executa o treino e teste para k folds\n",
        "        fold_k = 1\n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            \n",
        "            print(\"fold_k: {}\" .format(fold_k))\n",
        "            print(\"\\nTRAIN: {}  TEST: {}\".format(len(train_index), len(test_index)))\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "            ### train classifier\n",
        "            clf.fit(X_train, y_train)\n",
        "            \n",
        "            ### calculate metrics\n",
        "            y_predicted = clf.predict(X_test)\n",
        "            report_dict = metrics.classification_report(y_test, y_predicted, output_dict=True)\n",
        "            report_str = metrics.classification_report(y_test, y_predicted)\n",
        "                \n",
        "            ### Armazena o resultado do test do fold-k          \n",
        "            result_k.append(report_dict)\n",
        "            print(report_str)\n",
        "            \n",
        "            fold_k = fold_k + 1\n",
        "            \n",
        "\n",
        "        ### Guarda os resultados dos k fold do parametro c \n",
        "        reports_pos = pd.DataFrame(pd.DataFrame(result_k)['1.0'].to_list())\n",
        "        accuracy_reports = pd.DataFrame(pd.DataFrame(result_k)['accuracy'])\n",
        "        reports_pos['accuracy'] = accuracy_reports\n",
        "        reports_pos['pos/neg'] = 'pos'\n",
        "        reports_neg = pd.DataFrame(pd.DataFrame(result_k)['0.0'].to_list())\n",
        "        accuracy_reports = pd.DataFrame(pd.DataFrame(result_k)['accuracy'])\n",
        "        reports_neg['accuracy'] = accuracy_reports\n",
        "        reports_neg['pos/neg'] = 'neg'\n",
        "        \n",
        "        reports = pd.concat([reports_pos,reports_neg])\n",
        "        print(type(reports))\n",
        "        print(reports)\n",
        "        \n",
        "        ### Guarda o resultado para impressao grafica reports_g\n",
        "        reports_c = reports\n",
        "        reports_c['Param(c)'] = c\n",
        "        reports_c['method'] = str(name)\n",
        "        reports_g.append(reports_c)\n",
        "                \n",
        "        ### Guarda o resultado da execução para o parâmetro c\n",
        "        result_c = [c, reports]\n",
        "        result.append(result_c)\n",
        "        \n",
        "    \n",
        "    ### Retorna a lista com todos os resultado para cada c\n",
        "    return result , reports_g\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ14ict2yVZ-",
        "colab_type": "text"
      },
      "source": [
        "# Função para calcular a média das medidas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HpefPV4yVZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calcula a média das medidas de cada c\n",
        "def calcula_media(lista_result):\n",
        "    \n",
        "    mean_c = []\n",
        "    for result in lista_result:\n",
        "        \n",
        "        c = result[0]\n",
        "        result_c = result[1]\n",
        "        \n",
        "        # Calcula a média das medidas do parametro c\n",
        "        precision_mean = result_c['precision'].mean()\n",
        "        recall_mean = result_c['recall'].mean()\n",
        "        f1_score_mean = result_c['f1-score'].mean()\n",
        "        support_mean = result_c['support'].mean()\n",
        "        accuracy_mean = result_c['accuracy'].mean()\n",
        "        \n",
        "        # Armazena a média das medidas do parametro c\n",
        "        mean_c.append([c, precision_mean, recall_mean, f1_score_mean, support_mean, accuracy_mean])\n",
        "    \n",
        "    name_columns = ['c', 'precision_mean', 'recall_mean', 'f1_score_mean', 'support_mean', 'accuracy_mean']\n",
        "    mean_c = pd.DataFrame(mean_c, columns=name_columns)\n",
        "    return mean_c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OefDdR7WyVaD",
        "colab_type": "text"
      },
      "source": [
        "##### Parâmetros de execução do Naive Bayes\n",
        "list_c : valores do parâmetro de ajuste de probabilidade \n",
        "\n",
        "k_folds : número de folds para a estratificação do dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mss4hXMgyVaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_c = [0.001, 0.10, 0.25, 0.50, 0.75, 1]\n",
        "k_folds = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCMHkna8yVaJ",
        "colab_type": "text"
      },
      "source": [
        "# Execução base: Todas as características"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSyYoDy_yVaK",
        "colab_type": "code",
        "outputId": "e0b0dda9-d7f9-4f9c-bdf8-7527fd96605f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df = pd.read_csv('results/dataset-normalizado.csv', header = 0)\n",
        "X = df.drop('is_approved', axis=1).to_numpy() # DATASET\n",
        "y = df['is_approved'].to_numpy() # target\n",
        "result_all_features , result_all_features_g = stratified_k_fold(X, y, list_c, k=k_folds, name='All Features')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 10, Dataset: 348 positivas, 301 negativas (53% x 46%)\n",
            "Fold 0: Pos: 34, Neg: 31, Total: 65, Proporção: 52% x 47%\n",
            "Fold 1: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 2: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 3: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 4: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 5: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 6: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 7: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 8: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 9: Pos: 34, Neg: 30, Total: 64, Proporção: 53% x 46%\n",
            "c =  0.001\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.13      0.22        31\n",
            "         1.0       0.55      0.97      0.70        34\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.68      0.55      0.46        65\n",
            "weighted avg       0.67      0.57      0.47        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.40      0.51        30\n",
            "         1.0       0.62      0.86      0.72        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.67      0.63      0.62        65\n",
            "weighted avg       0.66      0.65      0.62        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.70      0.74        30\n",
            "         1.0       0.76      0.83      0.79        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.77      0.76      0.77        65\n",
            "weighted avg       0.77      0.77      0.77        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.53      0.60        30\n",
            "         1.0       0.67      0.80      0.73        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.63      0.68        30\n",
            "         1.0       0.72      0.80      0.76        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.72      0.72      0.72        65\n",
            "weighted avg       0.72      0.72      0.72        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.70      0.76        30\n",
            "         1.0       0.78      0.89      0.83        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.81      0.79      0.80        65\n",
            "weighted avg       0.81      0.80      0.80        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.80      0.86        30\n",
            "         1.0       0.85      0.94      0.89        35\n",
            "\n",
            "    accuracy                           0.88        65\n",
            "   macro avg       0.88      0.87      0.87        65\n",
            "weighted avg       0.88      0.88      0.88        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.63      0.63        30\n",
            "         1.0       0.69      0.69      0.69        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.66      0.66      0.66        65\n",
            "weighted avg       0.66      0.66      0.66        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.93      0.73        30\n",
            "         1.0       0.89      0.46      0.60        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.74      0.70      0.67        65\n",
            "weighted avg       0.75      0.68      0.66        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.77      0.68        30\n",
            "         1.0       0.73      0.56      0.63        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.67      0.66      0.65        64\n",
            "weighted avg       0.67      0.66      0.65        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.550000  0.970588  0.702128       34  0.569231     pos\n",
            "1   0.625000  0.857143  0.722892       35  0.646154     pos\n",
            "2   0.763158  0.828571  0.794521       35  0.769231     pos\n",
            "3   0.666667  0.800000  0.727273       35  0.676923     pos\n",
            "4   0.717949  0.800000  0.756757       35  0.723077     pos\n",
            "5   0.775000  0.885714  0.826667       35  0.800000     pos\n",
            "6   0.846154  0.942857  0.891892       35  0.876923     pos\n",
            "7   0.685714  0.685714  0.685714       35  0.661538     pos\n",
            "8   0.888889  0.457143  0.603774       35  0.676923     pos\n",
            "9   0.730769  0.558824  0.633333       34  0.656250     pos\n",
            "0   0.800000  0.129032  0.222222       31  0.569231     neg\n",
            "1   0.705882  0.400000  0.510638       30  0.646154     neg\n",
            "2   0.777778  0.700000  0.736842       30  0.769231     neg\n",
            "3   0.695652  0.533333  0.603774       30  0.676923     neg\n",
            "4   0.730769  0.633333  0.678571       30  0.723077     neg\n",
            "5   0.840000  0.700000  0.763636       30  0.800000     neg\n",
            "6   0.923077  0.800000  0.857143       30  0.876923     neg\n",
            "7   0.633333  0.633333  0.633333       30  0.661538     neg\n",
            "8   0.595745  0.933333  0.727273       30  0.676923     neg\n",
            "9   0.605263  0.766667  0.676471       30  0.656250     neg\n",
            "c =  0.1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.16      0.27        31\n",
            "         1.0       0.56      0.97      0.71        34\n",
            "\n",
            "    accuracy                           0.58        65\n",
            "   macro avg       0.70      0.57      0.49        65\n",
            "weighted avg       0.69      0.58      0.50        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.40      0.53        30\n",
            "         1.0       0.64      0.91      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.72      0.66      0.64        65\n",
            "weighted avg       0.71      0.68      0.65        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.67      0.71        30\n",
            "         1.0       0.74      0.83      0.78        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67        30\n",
            "         1.0       0.71      0.77      0.74        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.71        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.80      0.81        30\n",
            "         1.0       0.83      0.86      0.85        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.83      0.83      0.83        65\n",
            "weighted avg       0.83      0.83      0.83        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.87      0.90        30\n",
            "         1.0       0.89      0.94      0.92        35\n",
            "\n",
            "    accuracy                           0.91        65\n",
            "   macro avg       0.91      0.90      0.91        65\n",
            "weighted avg       0.91      0.91      0.91        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.70      0.68        30\n",
            "         1.0       0.73      0.69      0.71        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.69      0.69      0.69        65\n",
            "weighted avg       0.69      0.69      0.69        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.90      0.69        30\n",
            "         1.0       0.82      0.40      0.54        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.69      0.65      0.62        65\n",
            "weighted avg       0.70      0.63      0.61        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.77      0.69        30\n",
            "         1.0       0.74      0.59      0.66        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.68      0.68      0.67        64\n",
            "weighted avg       0.68      0.67      0.67        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.559322  0.970588  0.709677       34  0.584615     pos\n",
            "1   0.640000  0.914286  0.752941       35  0.676923     pos\n",
            "2   0.743590  0.828571  0.783784       35  0.753846     pos\n",
            "3   0.666667  0.857143  0.750000       35  0.692308     pos\n",
            "4   0.710526  0.771429  0.739726       35  0.707692     pos\n",
            "5   0.833333  0.857143  0.845070       35  0.830769     pos\n",
            "6   0.891892  0.942857  0.916667       35  0.907692     pos\n",
            "7   0.727273  0.685714  0.705882       35  0.692308     pos\n",
            "8   0.823529  0.400000  0.538462       35  0.630769     pos\n",
            "9   0.740741  0.588235  0.655738       34  0.671875     pos\n",
            "0   0.833333  0.161290  0.270270       31  0.584615     neg\n",
            "1   0.800000  0.400000  0.533333       30  0.676923     neg\n",
            "2   0.769231  0.666667  0.714286       30  0.753846     neg\n",
            "3   0.750000  0.500000  0.600000       30  0.692308     neg\n",
            "4   0.703704  0.633333  0.666667       30  0.707692     neg\n",
            "5   0.827586  0.800000  0.813559       30  0.830769     neg\n",
            "6   0.928571  0.866667  0.896552       30  0.907692     neg\n",
            "7   0.656250  0.700000  0.677419       30  0.692308     neg\n",
            "8   0.562500  0.900000  0.692308       30  0.630769     neg\n",
            "9   0.621622  0.766667  0.686567       30  0.671875     neg\n",
            "c =  0.25\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.13      0.22        31\n",
            "         1.0       0.55      0.97      0.70        34\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.68      0.55      0.46        65\n",
            "weighted avg       0.67      0.57      0.47        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.37      0.51        30\n",
            "         1.0       0.63      0.94      0.76        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.74      0.65      0.64        65\n",
            "weighted avg       0.73      0.68      0.64        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.67      0.73        30\n",
            "         1.0       0.75      0.86      0.80        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.78      0.76      0.76        65\n",
            "weighted avg       0.77      0.77      0.77        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.60      0.63        30\n",
            "         1.0       0.68      0.74      0.71        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.68        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.80      0.83        30\n",
            "         1.0       0.84      0.89      0.86        35\n",
            "\n",
            "    accuracy                           0.85        65\n",
            "   macro avg       0.85      0.84      0.84        65\n",
            "weighted avg       0.85      0.85      0.85        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.83      0.88        30\n",
            "         1.0       0.87      0.94      0.90        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.90      0.89      0.89        65\n",
            "weighted avg       0.89      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.67      0.69        30\n",
            "         1.0       0.73      0.77      0.75        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.72      0.72      0.72        65\n",
            "weighted avg       0.72      0.72      0.72        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.55      0.90      0.68        30\n",
            "         1.0       0.81      0.37      0.51        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.68      0.64      0.60        65\n",
            "weighted avg       0.69      0.62      0.59        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.73      0.67        30\n",
            "         1.0       0.71      0.59      0.65        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.67      0.66      0.66        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.550000  0.970588  0.702128       34  0.569231     pos\n",
            "1   0.634615  0.942857  0.758621       35  0.676923     pos\n",
            "2   0.750000  0.857143  0.800000       35  0.769231     pos\n",
            "3   0.666667  0.857143  0.750000       35  0.692308     pos\n",
            "4   0.684211  0.742857  0.712329       35  0.676923     pos\n",
            "5   0.837838  0.885714  0.861111       35  0.846154     pos\n",
            "6   0.868421  0.942857  0.904110       35  0.892308     pos\n",
            "7   0.729730  0.771429  0.750000       35  0.723077     pos\n",
            "8   0.812500  0.371429  0.509804       35  0.615385     pos\n",
            "9   0.714286  0.588235  0.645161       34  0.656250     pos\n",
            "0   0.800000  0.129032  0.222222       31  0.569231     neg\n",
            "1   0.846154  0.366667  0.511628       30  0.676923     neg\n",
            "2   0.800000  0.666667  0.727273       30  0.769231     neg\n",
            "3   0.750000  0.500000  0.600000       30  0.692308     neg\n",
            "4   0.666667  0.600000  0.631579       30  0.676923     neg\n",
            "5   0.857143  0.800000  0.827586       30  0.846154     neg\n",
            "6   0.925926  0.833333  0.877193       30  0.892308     neg\n",
            "7   0.714286  0.666667  0.689655       30  0.723077     neg\n",
            "8   0.551020  0.900000  0.683544       30  0.615385     neg\n",
            "9   0.611111  0.733333  0.666667       30  0.656250     neg\n",
            "c =  0.5\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.13      0.22        31\n",
            "         1.0       0.55      0.97      0.70        34\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.68      0.55      0.46        65\n",
            "weighted avg       0.67      0.57      0.47        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.30      0.45        30\n",
            "         1.0       0.62      0.97      0.76        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.76      0.64      0.60        65\n",
            "weighted avg       0.75      0.66      0.61        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.63      0.70        30\n",
            "         1.0       0.73      0.86      0.79        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.43      0.54        30\n",
            "         1.0       0.64      0.86      0.73        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.68      0.65      0.64        65\n",
            "weighted avg       0.68      0.66      0.64        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.57      0.62        30\n",
            "         1.0       0.68      0.77      0.72        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.80      0.84        30\n",
            "         1.0       0.84      0.91      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.86      0.86        65\n",
            "weighted avg       0.86      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.87      0.90        30\n",
            "         1.0       0.89      0.94      0.92        35\n",
            "\n",
            "    accuracy                           0.91        65\n",
            "   macro avg       0.91      0.90      0.91        65\n",
            "weighted avg       0.91      0.91      0.91        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.60      0.64        30\n",
            "         1.0       0.69      0.77      0.73        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.69      0.69      0.69        65\n",
            "weighted avg       0.69      0.69      0.69        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.55      0.87      0.68        30\n",
            "         1.0       0.78      0.40      0.53        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.67      0.63      0.60        65\n",
            "weighted avg       0.67      0.62      0.60        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.550000  0.970588  0.702128       34  0.569231     pos\n",
            "1   0.618182  0.971429  0.755556       35  0.661538     pos\n",
            "2   0.731707  0.857143  0.789474       35  0.753846     pos\n",
            "3   0.638298  0.857143  0.731707       35  0.661538     pos\n",
            "4   0.675000  0.771429  0.720000       35  0.676923     pos\n",
            "5   0.842105  0.914286  0.876712       35  0.861538     pos\n",
            "6   0.891892  0.942857  0.916667       35  0.907692     pos\n",
            "7   0.692308  0.771429  0.729730       35  0.692308     pos\n",
            "8   0.777778  0.400000  0.528302       35  0.615385     pos\n",
            "9   0.700000  0.617647  0.656250       34  0.656250     pos\n",
            "0   0.800000  0.129032  0.222222       31  0.569231     neg\n",
            "1   0.900000  0.300000  0.450000       30  0.661538     neg\n",
            "2   0.791667  0.633333  0.703704       30  0.753846     neg\n",
            "3   0.722222  0.433333  0.541667       30  0.661538     neg\n",
            "4   0.680000  0.566667  0.618182       30  0.676923     neg\n",
            "5   0.888889  0.800000  0.842105       30  0.861538     neg\n",
            "6   0.928571  0.866667  0.896552       30  0.907692     neg\n",
            "7   0.692308  0.600000  0.642857       30  0.692308     neg\n",
            "8   0.553191  0.866667  0.675325       30  0.615385     neg\n",
            "9   0.617647  0.700000  0.656250       30  0.656250     neg\n",
            "c =  0.75\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.13      0.22        31\n",
            "         1.0       0.55      0.97      0.70        34\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.68      0.55      0.46        65\n",
            "weighted avg       0.67      0.57      0.47        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.23      0.37        30\n",
            "         1.0       0.60      0.97      0.74        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.74      0.60      0.55        65\n",
            "weighted avg       0.73      0.63      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.63      0.70        30\n",
            "         1.0       0.73      0.86      0.79        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.37      0.48        30\n",
            "         1.0       0.61      0.86      0.71        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.65      0.61      0.60        65\n",
            "weighted avg       0.65      0.63      0.61        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.57      0.62        30\n",
            "         1.0       0.68      0.77      0.72        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.80      0.87        30\n",
            "         1.0       0.85      0.97      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.91      0.89      0.89        65\n",
            "weighted avg       0.90      0.89      0.89        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.90      0.93        30\n",
            "         1.0       0.92      0.97      0.94        35\n",
            "\n",
            "    accuracy                           0.94        65\n",
            "   macro avg       0.94      0.94      0.94        65\n",
            "weighted avg       0.94      0.94      0.94        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.60      0.65        30\n",
            "         1.0       0.70      0.80      0.75        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.70        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.58      0.87      0.69        30\n",
            "         1.0       0.80      0.46      0.58        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.69      0.66      0.64        65\n",
            "weighted avg       0.70      0.65      0.63        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.550000  0.970588  0.702128       34  0.569231     pos\n",
            "1   0.596491  0.971429  0.739130       35  0.630769     pos\n",
            "2   0.731707  0.857143  0.789474       35  0.753846     pos\n",
            "3   0.612245  0.857143  0.714286       35  0.630769     pos\n",
            "4   0.675000  0.771429  0.720000       35  0.676923     pos\n",
            "5   0.850000  0.971429  0.906667       35  0.892308     pos\n",
            "6   0.918919  0.971429  0.944444       35  0.938462     pos\n",
            "7   0.700000  0.800000  0.746667       35  0.707692     pos\n",
            "8   0.800000  0.457143  0.581818       35  0.646154     pos\n",
            "9   0.700000  0.617647  0.656250       34  0.656250     pos\n",
            "0   0.800000  0.129032  0.222222       31  0.569231     neg\n",
            "1   0.875000  0.233333  0.368421       30  0.630769     neg\n",
            "2   0.791667  0.633333  0.703704       30  0.753846     neg\n",
            "3   0.687500  0.366667  0.478261       30  0.630769     neg\n",
            "4   0.680000  0.566667  0.618182       30  0.676923     neg\n",
            "5   0.960000  0.800000  0.872727       30  0.892308     neg\n",
            "6   0.964286  0.900000  0.931034       30  0.938462     neg\n",
            "7   0.720000  0.600000  0.654545       30  0.707692     neg\n",
            "8   0.577778  0.866667  0.693333       30  0.646154     neg\n",
            "9   0.617647  0.700000  0.656250       30  0.656250     neg\n",
            "c =  1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.23      0.37        30\n",
            "         1.0       0.60      0.97      0.74        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.74      0.60      0.55        65\n",
            "weighted avg       0.73      0.63      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.60      0.68        30\n",
            "         1.0       0.71      0.86      0.78        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.75      0.73      0.73        65\n",
            "weighted avg       0.75      0.74      0.73        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.33      0.44        30\n",
            "         1.0       0.60      0.86      0.71        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.63      0.60      0.58        65\n",
            "weighted avg       0.63      0.62      0.59        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.53      0.60        30\n",
            "         1.0       0.67      0.80      0.73        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.77      0.85        30\n",
            "         1.0       0.83      0.97      0.89        35\n",
            "\n",
            "    accuracy                           0.88        65\n",
            "   macro avg       0.89      0.87      0.87        65\n",
            "weighted avg       0.89      0.88      0.87        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.87      0.91        30\n",
            "         1.0       0.89      0.97      0.93        35\n",
            "\n",
            "    accuracy                           0.92        65\n",
            "   macro avg       0.93      0.92      0.92        65\n",
            "weighted avg       0.93      0.92      0.92        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.53      0.62        30\n",
            "         1.0       0.67      0.83      0.74        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.70      0.68      0.68        65\n",
            "weighted avg       0.70      0.69      0.68        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.87      0.72        30\n",
            "         1.0       0.83      0.54      0.66        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.72      0.70      0.69        65\n",
            "weighted avg       0.73      0.69      0.69        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.540984  0.970588  0.694737       34  0.553846     pos\n",
            "1   0.596491  0.971429  0.739130       35  0.630769     pos\n",
            "2   0.714286  0.857143  0.779221       35  0.738462     pos\n",
            "3   0.600000  0.857143  0.705882       35  0.615385     pos\n",
            "4   0.666667  0.800000  0.727273       35  0.676923     pos\n",
            "5   0.829268  0.971429  0.894737       35  0.876923     pos\n",
            "6   0.894737  0.971429  0.931507       35  0.923077     pos\n",
            "7   0.674419  0.828571  0.743590       35  0.692308     pos\n",
            "8   0.826087  0.542857  0.655172       35  0.692308     pos\n",
            "9   0.700000  0.617647  0.656250       34  0.656250     pos\n",
            "0   0.750000  0.096774  0.171429       31  0.553846     neg\n",
            "1   0.875000  0.233333  0.368421       30  0.630769     neg\n",
            "2   0.782609  0.600000  0.679245       30  0.738462     neg\n",
            "3   0.666667  0.333333  0.444444       30  0.615385     neg\n",
            "4   0.695652  0.533333  0.603774       30  0.676923     neg\n",
            "5   0.958333  0.766667  0.851852       30  0.876923     neg\n",
            "6   0.962963  0.866667  0.912281       30  0.923077     neg\n",
            "7   0.727273  0.533333  0.615385       30  0.692308     neg\n",
            "8   0.619048  0.866667  0.722222       30  0.692308     neg\n",
            "9   0.617647  0.700000  0.656250       30  0.656250     neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AymSdHvyyVaQ",
        "colab_type": "text"
      },
      "source": [
        "Resultado dos k-fold para cada valor de c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-NvZLvfyVaR",
        "colab_type": "code",
        "outputId": "763af9f6-f6d5-492b-999a-b04c1fe38bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "result_all_features"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001,    precision    recall  f1-score  ...  pos/neg  Param(c)        method\n",
              "  0   0.550000  0.970588  0.702128  ...      pos     0.001  All Features\n",
              "  1   0.625000  0.857143  0.722892  ...      pos     0.001  All Features\n",
              "  2   0.763158  0.828571  0.794521  ...      pos     0.001  All Features\n",
              "  3   0.666667  0.800000  0.727273  ...      pos     0.001  All Features\n",
              "  4   0.717949  0.800000  0.756757  ...      pos     0.001  All Features\n",
              "  5   0.775000  0.885714  0.826667  ...      pos     0.001  All Features\n",
              "  6   0.846154  0.942857  0.891892  ...      pos     0.001  All Features\n",
              "  7   0.685714  0.685714  0.685714  ...      pos     0.001  All Features\n",
              "  8   0.888889  0.457143  0.603774  ...      pos     0.001  All Features\n",
              "  9   0.730769  0.558824  0.633333  ...      pos     0.001  All Features\n",
              "  0   0.800000  0.129032  0.222222  ...      neg     0.001  All Features\n",
              "  1   0.705882  0.400000  0.510638  ...      neg     0.001  All Features\n",
              "  2   0.777778  0.700000  0.736842  ...      neg     0.001  All Features\n",
              "  3   0.695652  0.533333  0.603774  ...      neg     0.001  All Features\n",
              "  4   0.730769  0.633333  0.678571  ...      neg     0.001  All Features\n",
              "  5   0.840000  0.700000  0.763636  ...      neg     0.001  All Features\n",
              "  6   0.923077  0.800000  0.857143  ...      neg     0.001  All Features\n",
              "  7   0.633333  0.633333  0.633333  ...      neg     0.001  All Features\n",
              "  8   0.595745  0.933333  0.727273  ...      neg     0.001  All Features\n",
              "  9   0.605263  0.766667  0.676471  ...      neg     0.001  All Features\n",
              "  \n",
              "  [20 rows x 8 columns]],\n",
              " [0.1,    precision    recall  f1-score  ...  pos/neg  Param(c)        method\n",
              "  0   0.559322  0.970588  0.709677  ...      pos       0.1  All Features\n",
              "  1   0.640000  0.914286  0.752941  ...      pos       0.1  All Features\n",
              "  2   0.743590  0.828571  0.783784  ...      pos       0.1  All Features\n",
              "  3   0.666667  0.857143  0.750000  ...      pos       0.1  All Features\n",
              "  4   0.710526  0.771429  0.739726  ...      pos       0.1  All Features\n",
              "  5   0.833333  0.857143  0.845070  ...      pos       0.1  All Features\n",
              "  6   0.891892  0.942857  0.916667  ...      pos       0.1  All Features\n",
              "  7   0.727273  0.685714  0.705882  ...      pos       0.1  All Features\n",
              "  8   0.823529  0.400000  0.538462  ...      pos       0.1  All Features\n",
              "  9   0.740741  0.588235  0.655738  ...      pos       0.1  All Features\n",
              "  0   0.833333  0.161290  0.270270  ...      neg       0.1  All Features\n",
              "  1   0.800000  0.400000  0.533333  ...      neg       0.1  All Features\n",
              "  2   0.769231  0.666667  0.714286  ...      neg       0.1  All Features\n",
              "  3   0.750000  0.500000  0.600000  ...      neg       0.1  All Features\n",
              "  4   0.703704  0.633333  0.666667  ...      neg       0.1  All Features\n",
              "  5   0.827586  0.800000  0.813559  ...      neg       0.1  All Features\n",
              "  6   0.928571  0.866667  0.896552  ...      neg       0.1  All Features\n",
              "  7   0.656250  0.700000  0.677419  ...      neg       0.1  All Features\n",
              "  8   0.562500  0.900000  0.692308  ...      neg       0.1  All Features\n",
              "  9   0.621622  0.766667  0.686567  ...      neg       0.1  All Features\n",
              "  \n",
              "  [20 rows x 8 columns]],\n",
              " [0.25,    precision    recall  f1-score  ...  pos/neg  Param(c)        method\n",
              "  0   0.550000  0.970588  0.702128  ...      pos      0.25  All Features\n",
              "  1   0.634615  0.942857  0.758621  ...      pos      0.25  All Features\n",
              "  2   0.750000  0.857143  0.800000  ...      pos      0.25  All Features\n",
              "  3   0.666667  0.857143  0.750000  ...      pos      0.25  All Features\n",
              "  4   0.684211  0.742857  0.712329  ...      pos      0.25  All Features\n",
              "  5   0.837838  0.885714  0.861111  ...      pos      0.25  All Features\n",
              "  6   0.868421  0.942857  0.904110  ...      pos      0.25  All Features\n",
              "  7   0.729730  0.771429  0.750000  ...      pos      0.25  All Features\n",
              "  8   0.812500  0.371429  0.509804  ...      pos      0.25  All Features\n",
              "  9   0.714286  0.588235  0.645161  ...      pos      0.25  All Features\n",
              "  0   0.800000  0.129032  0.222222  ...      neg      0.25  All Features\n",
              "  1   0.846154  0.366667  0.511628  ...      neg      0.25  All Features\n",
              "  2   0.800000  0.666667  0.727273  ...      neg      0.25  All Features\n",
              "  3   0.750000  0.500000  0.600000  ...      neg      0.25  All Features\n",
              "  4   0.666667  0.600000  0.631579  ...      neg      0.25  All Features\n",
              "  5   0.857143  0.800000  0.827586  ...      neg      0.25  All Features\n",
              "  6   0.925926  0.833333  0.877193  ...      neg      0.25  All Features\n",
              "  7   0.714286  0.666667  0.689655  ...      neg      0.25  All Features\n",
              "  8   0.551020  0.900000  0.683544  ...      neg      0.25  All Features\n",
              "  9   0.611111  0.733333  0.666667  ...      neg      0.25  All Features\n",
              "  \n",
              "  [20 rows x 8 columns]],\n",
              " [0.5,    precision    recall  f1-score  ...  pos/neg  Param(c)        method\n",
              "  0   0.550000  0.970588  0.702128  ...      pos       0.5  All Features\n",
              "  1   0.618182  0.971429  0.755556  ...      pos       0.5  All Features\n",
              "  2   0.731707  0.857143  0.789474  ...      pos       0.5  All Features\n",
              "  3   0.638298  0.857143  0.731707  ...      pos       0.5  All Features\n",
              "  4   0.675000  0.771429  0.720000  ...      pos       0.5  All Features\n",
              "  5   0.842105  0.914286  0.876712  ...      pos       0.5  All Features\n",
              "  6   0.891892  0.942857  0.916667  ...      pos       0.5  All Features\n",
              "  7   0.692308  0.771429  0.729730  ...      pos       0.5  All Features\n",
              "  8   0.777778  0.400000  0.528302  ...      pos       0.5  All Features\n",
              "  9   0.700000  0.617647  0.656250  ...      pos       0.5  All Features\n",
              "  0   0.800000  0.129032  0.222222  ...      neg       0.5  All Features\n",
              "  1   0.900000  0.300000  0.450000  ...      neg       0.5  All Features\n",
              "  2   0.791667  0.633333  0.703704  ...      neg       0.5  All Features\n",
              "  3   0.722222  0.433333  0.541667  ...      neg       0.5  All Features\n",
              "  4   0.680000  0.566667  0.618182  ...      neg       0.5  All Features\n",
              "  5   0.888889  0.800000  0.842105  ...      neg       0.5  All Features\n",
              "  6   0.928571  0.866667  0.896552  ...      neg       0.5  All Features\n",
              "  7   0.692308  0.600000  0.642857  ...      neg       0.5  All Features\n",
              "  8   0.553191  0.866667  0.675325  ...      neg       0.5  All Features\n",
              "  9   0.617647  0.700000  0.656250  ...      neg       0.5  All Features\n",
              "  \n",
              "  [20 rows x 8 columns]],\n",
              " [0.75,    precision    recall  f1-score  ...  pos/neg  Param(c)        method\n",
              "  0   0.550000  0.970588  0.702128  ...      pos      0.75  All Features\n",
              "  1   0.596491  0.971429  0.739130  ...      pos      0.75  All Features\n",
              "  2   0.731707  0.857143  0.789474  ...      pos      0.75  All Features\n",
              "  3   0.612245  0.857143  0.714286  ...      pos      0.75  All Features\n",
              "  4   0.675000  0.771429  0.720000  ...      pos      0.75  All Features\n",
              "  5   0.850000  0.971429  0.906667  ...      pos      0.75  All Features\n",
              "  6   0.918919  0.971429  0.944444  ...      pos      0.75  All Features\n",
              "  7   0.700000  0.800000  0.746667  ...      pos      0.75  All Features\n",
              "  8   0.800000  0.457143  0.581818  ...      pos      0.75  All Features\n",
              "  9   0.700000  0.617647  0.656250  ...      pos      0.75  All Features\n",
              "  0   0.800000  0.129032  0.222222  ...      neg      0.75  All Features\n",
              "  1   0.875000  0.233333  0.368421  ...      neg      0.75  All Features\n",
              "  2   0.791667  0.633333  0.703704  ...      neg      0.75  All Features\n",
              "  3   0.687500  0.366667  0.478261  ...      neg      0.75  All Features\n",
              "  4   0.680000  0.566667  0.618182  ...      neg      0.75  All Features\n",
              "  5   0.960000  0.800000  0.872727  ...      neg      0.75  All Features\n",
              "  6   0.964286  0.900000  0.931034  ...      neg      0.75  All Features\n",
              "  7   0.720000  0.600000  0.654545  ...      neg      0.75  All Features\n",
              "  8   0.577778  0.866667  0.693333  ...      neg      0.75  All Features\n",
              "  9   0.617647  0.700000  0.656250  ...      neg      0.75  All Features\n",
              "  \n",
              "  [20 rows x 8 columns]],\n",
              " [1,    precision    recall  f1-score  ...  pos/neg  Param(c)        method\n",
              "  0   0.540984  0.970588  0.694737  ...      pos         1  All Features\n",
              "  1   0.596491  0.971429  0.739130  ...      pos         1  All Features\n",
              "  2   0.714286  0.857143  0.779221  ...      pos         1  All Features\n",
              "  3   0.600000  0.857143  0.705882  ...      pos         1  All Features\n",
              "  4   0.666667  0.800000  0.727273  ...      pos         1  All Features\n",
              "  5   0.829268  0.971429  0.894737  ...      pos         1  All Features\n",
              "  6   0.894737  0.971429  0.931507  ...      pos         1  All Features\n",
              "  7   0.674419  0.828571  0.743590  ...      pos         1  All Features\n",
              "  8   0.826087  0.542857  0.655172  ...      pos         1  All Features\n",
              "  9   0.700000  0.617647  0.656250  ...      pos         1  All Features\n",
              "  0   0.750000  0.096774  0.171429  ...      neg         1  All Features\n",
              "  1   0.875000  0.233333  0.368421  ...      neg         1  All Features\n",
              "  2   0.782609  0.600000  0.679245  ...      neg         1  All Features\n",
              "  3   0.666667  0.333333  0.444444  ...      neg         1  All Features\n",
              "  4   0.695652  0.533333  0.603774  ...      neg         1  All Features\n",
              "  5   0.958333  0.766667  0.851852  ...      neg         1  All Features\n",
              "  6   0.962963  0.866667  0.912281  ...      neg         1  All Features\n",
              "  7   0.727273  0.533333  0.615385  ...      neg         1  All Features\n",
              "  8   0.619048  0.866667  0.722222  ...      neg         1  All Features\n",
              "  9   0.617647  0.700000  0.656250  ...      neg         1  All Features\n",
              "  \n",
              "  [20 rows x 8 columns]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEf_UxQYyVaX",
        "colab_type": "text"
      },
      "source": [
        "Calcula a média das medidas de cada parâmetro c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GPD7fz5yVaY",
        "colab_type": "code",
        "outputId": "428a9b1d-6989-484e-db04-840e83a0e04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "result_all_features_mean = calcula_media(result_all_features)\n",
        "result_all_features_mean"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.727840</td>\n",
              "      <td>0.700779</td>\n",
              "      <td>0.687743</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.705625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.739483</td>\n",
              "      <td>0.710530</td>\n",
              "      <td>0.697445</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.714880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250</td>\n",
              "      <td>0.738529</td>\n",
              "      <td>0.706298</td>\n",
              "      <td>0.691531</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.711779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.734588</td>\n",
              "      <td>0.698482</td>\n",
              "      <td>0.682769</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.705625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.740412</td>\n",
              "      <td>0.702054</td>\n",
              "      <td>0.684977</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.710240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.734906</td>\n",
              "      <td>0.695917</td>\n",
              "      <td>0.677640</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.705625</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "0  0.001        0.727840  ...         32.45       0.705625\n",
              "1  0.100        0.739483  ...         32.45       0.714880\n",
              "2  0.250        0.738529  ...         32.45       0.711779\n",
              "3  0.500        0.734588  ...         32.45       0.705625\n",
              "4  0.750        0.740412  ...         32.45       0.710240\n",
              "5  1.000        0.734906  ...         32.45       0.705625\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elz06yqgyVad",
        "colab_type": "text"
      },
      "source": [
        "Obtém as medidas da maior média de acurácia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPHbgy6ByVae",
        "colab_type": "code",
        "outputId": "db4c7ca2-ab8d-445e-aaae-b2403176e8e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "best_accuracy_all_features = pd.Series(result_all_features_mean.iloc[result_all_features_mean['accuracy_mean'].idxmax()], \n",
        "                          name='All Features')\n",
        "best_all_features = pd.DataFrame(best_accuracy_all_features)\n",
        "best_all_features"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>All Features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_mean</th>\n",
              "      <td>0.739483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_mean</th>\n",
              "      <td>0.710530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score_mean</th>\n",
              "      <td>0.697445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support_mean</th>\n",
              "      <td>32.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_mean</th>\n",
              "      <td>0.714880</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                All Features\n",
              "c                   0.100000\n",
              "precision_mean      0.739483\n",
              "recall_mean         0.710530\n",
              "f1_score_mean       0.697445\n",
              "support_mean       32.450000\n",
              "accuracy_mean       0.714880"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXEKLi39yVaj",
        "colab_type": "text"
      },
      "source": [
        "# Execução Base: PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiTGHSiMyVak",
        "colab_type": "code",
        "outputId": "086b596e-adbc-4aec-e8ea-8d568fb5005b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df = pd.read_csv('results/dataset-pca.csv', header = 0)\n",
        "X = df.drop('is_approved', axis=1).to_numpy() # DATASET\n",
        "y = df['is_approved'].to_numpy() # target\n",
        "result_pca , result_pca_g = stratified_k_fold(X, y, list_c, k=k_folds, name='PCA')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 10, Dataset: 348 positivas, 301 negativas (53% x 46%)\n",
            "Fold 0: Pos: 34, Neg: 31, Total: 65, Proporção: 52% x 47%\n",
            "Fold 1: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 2: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 3: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 4: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 5: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 6: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 7: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 8: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 9: Pos: 34, Neg: 30, Total: 64, Proporção: 53% x 46%\n",
            "c =  0.001\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.16      0.28        31\n",
            "         1.0       0.57      1.00      0.72        34\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.78      0.58      0.50        65\n",
            "weighted avg       0.77      0.60      0.51        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.33      0.47        30\n",
            "         1.0       0.62      0.91      0.74        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.69      0.62      0.60        65\n",
            "weighted avg       0.69      0.65      0.61        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.77      0.79        30\n",
            "         1.0       0.81      0.86      0.83        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.82      0.81      0.81        65\n",
            "weighted avg       0.82      0.82      0.81        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.40      0.50        30\n",
            "         1.0       0.62      0.83      0.71        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.64      0.61      0.60        65\n",
            "weighted avg       0.64      0.63      0.61        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.60      0.61        30\n",
            "         1.0       0.67      0.69      0.68        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.64      0.64      0.64        65\n",
            "weighted avg       0.65      0.65      0.65        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.80      0.79        30\n",
            "         1.0       0.82      0.80      0.81        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.80      0.80      0.80        65\n",
            "weighted avg       0.80      0.80      0.80        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.80      0.80        30\n",
            "         1.0       0.83      0.83      0.83        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.81      0.81      0.81        65\n",
            "weighted avg       0.82      0.82      0.82        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.73      0.64        30\n",
            "         1.0       0.69      0.51      0.59        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.63      0.62      0.61        65\n",
            "weighted avg       0.63      0.62      0.61        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.57      0.97      0.72        30\n",
            "         1.0       0.93      0.37      0.53        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.75      0.67      0.62        65\n",
            "weighted avg       0.76      0.65      0.62        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.83      0.67        30\n",
            "         1.0       0.74      0.41      0.53        34\n",
            "\n",
            "    accuracy                           0.61        64\n",
            "   macro avg       0.65      0.62      0.60        64\n",
            "weighted avg       0.65      0.61      0.59        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.566667  1.000000  0.723404       34  0.600000     pos\n",
            "1   0.615385  0.914286  0.735632       35  0.646154     pos\n",
            "2   0.810811  0.857143  0.833333       35  0.815385     pos\n",
            "3   0.617021  0.828571  0.707317       35  0.630769     pos\n",
            "4   0.666667  0.685714  0.676056       35  0.646154     pos\n",
            "5   0.823529  0.800000  0.811594       35  0.800000     pos\n",
            "6   0.828571  0.828571  0.828571       35  0.815385     pos\n",
            "7   0.692308  0.514286  0.590164       35  0.615385     pos\n",
            "8   0.928571  0.371429  0.530612       35  0.646154     pos\n",
            "9   0.736842  0.411765  0.528302       34  0.609375     pos\n",
            "0   1.000000  0.161290  0.277778       31  0.600000     neg\n",
            "1   0.769231  0.333333  0.465116       30  0.646154     neg\n",
            "2   0.821429  0.766667  0.793103       30  0.815385     neg\n",
            "3   0.666667  0.400000  0.500000       30  0.630769     neg\n",
            "4   0.620690  0.600000  0.610169       30  0.646154     neg\n",
            "5   0.774194  0.800000  0.786885       30  0.800000     neg\n",
            "6   0.800000  0.800000  0.800000       30  0.815385     neg\n",
            "7   0.564103  0.733333  0.637681       30  0.615385     neg\n",
            "8   0.568627  0.966667  0.716049       30  0.646154     neg\n",
            "9   0.555556  0.833333  0.666667       30  0.609375     neg\n",
            "c =  0.1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.06      0.12        31\n",
            "         1.0       0.54      1.00      0.70        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.77      0.53      0.41        65\n",
            "weighted avg       0.76      0.55      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.23      0.36        30\n",
            "         1.0       0.59      0.94      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.68      0.59      0.54        65\n",
            "weighted avg       0.68      0.62      0.56        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.67      0.75        30\n",
            "         1.0       0.76      0.91      0.83        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.82      0.79      0.79        65\n",
            "weighted avg       0.81      0.80      0.80        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.30      0.42        30\n",
            "         1.0       0.60      0.89      0.71        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.64      0.59      0.57        65\n",
            "weighted avg       0.64      0.62      0.58        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      0.43      0.50        30\n",
            "         1.0       0.60      0.74      0.67        35\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.60      0.59      0.58        65\n",
            "weighted avg       0.60      0.60      0.59        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.77      0.77        30\n",
            "         1.0       0.80      0.80      0.80        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.78      0.78      0.78        65\n",
            "weighted avg       0.78      0.78      0.78        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.80      0.81        30\n",
            "         1.0       0.83      0.86      0.85        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.83      0.83      0.83        65\n",
            "weighted avg       0.83      0.83      0.83        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.67      0.65        30\n",
            "         1.0       0.70      0.66      0.68        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.66      0.66      0.66        65\n",
            "weighted avg       0.66      0.66      0.66        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      0.90      0.71        30\n",
            "         1.0       0.84      0.46      0.59        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.71      0.68      0.65        65\n",
            "weighted avg       0.72      0.66      0.65        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.57      0.77      0.66        30\n",
            "         1.0       0.71      0.50      0.59        34\n",
            "\n",
            "    accuracy                           0.62        64\n",
            "   macro avg       0.64      0.63      0.62        64\n",
            "weighted avg       0.65      0.62      0.62        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.539683  1.000000  0.701031       34  0.553846     pos\n",
            "1   0.589286  0.942857  0.725275       35  0.615385     pos\n",
            "2   0.761905  0.914286  0.831169       35  0.800000     pos\n",
            "3   0.596154  0.885714  0.712644       35  0.615385     pos\n",
            "4   0.604651  0.742857  0.666667       35  0.600000     pos\n",
            "5   0.800000  0.800000  0.800000       35  0.784615     pos\n",
            "6   0.833333  0.857143  0.845070       35  0.830769     pos\n",
            "7   0.696970  0.657143  0.676471       35  0.661538     pos\n",
            "8   0.842105  0.457143  0.592593       35  0.661538     pos\n",
            "9   0.708333  0.500000  0.586207       34  0.625000     pos\n",
            "0   1.000000  0.064516  0.121212       31  0.553846     neg\n",
            "1   0.777778  0.233333  0.358974       30  0.615385     neg\n",
            "2   0.869565  0.666667  0.754717       30  0.800000     neg\n",
            "3   0.692308  0.300000  0.418605       30  0.615385     neg\n",
            "4   0.590909  0.433333  0.500000       30  0.600000     neg\n",
            "5   0.766667  0.766667  0.766667       30  0.784615     neg\n",
            "6   0.827586  0.800000  0.813559       30  0.830769     neg\n",
            "7   0.625000  0.666667  0.645161       30  0.661538     neg\n",
            "8   0.586957  0.900000  0.710526       30  0.661538     neg\n",
            "9   0.575000  0.766667  0.657143       30  0.625000     neg\n",
            "c =  0.25\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.03      0.06        31\n",
            "         1.0       0.53      1.00      0.69        34\n",
            "\n",
            "    accuracy                           0.54        65\n",
            "   macro avg       0.77      0.52      0.38        65\n",
            "weighted avg       0.75      0.54      0.39        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.20      0.32        30\n",
            "         1.0       0.59      0.97      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.72      0.59      0.53        65\n",
            "weighted avg       0.71      0.62      0.54        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.63      0.73        30\n",
            "         1.0       0.74      0.91      0.82        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.80      0.77      0.78        65\n",
            "weighted avg       0.80      0.78      0.78        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.20      0.31        30\n",
            "         1.0       0.57      0.91      0.70        35\n",
            "\n",
            "    accuracy                           0.58        65\n",
            "   macro avg       0.62      0.56      0.51        65\n",
            "weighted avg       0.62      0.58      0.52        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.40      0.49        30\n",
            "         1.0       0.61      0.80      0.69        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.62      0.60      0.59        65\n",
            "weighted avg       0.62      0.62      0.60        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.67      0.74        30\n",
            "         1.0       0.76      0.89      0.82        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.79      0.78      0.78        65\n",
            "weighted avg       0.79      0.78      0.78        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.73      0.80        30\n",
            "         1.0       0.80      0.91      0.85        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.84      0.82      0.83        65\n",
            "weighted avg       0.84      0.83      0.83        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.63      0.68        30\n",
            "         1.0       0.72      0.80      0.76        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.72      0.72      0.72        65\n",
            "weighted avg       0.72      0.72      0.72        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.90      0.73        30\n",
            "         1.0       0.86      0.51      0.64        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.74      0.71      0.69        65\n",
            "weighted avg       0.74      0.69      0.68        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.67      0.65        30\n",
            "         1.0       0.69      0.65      0.67        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.531250  1.000000  0.693878       34  0.538462     pos\n",
            "1   0.586207  0.971429  0.731183       35  0.615385     pos\n",
            "2   0.744186  0.914286  0.820513       35  0.784615     pos\n",
            "3   0.571429  0.914286  0.703297       35  0.584615     pos\n",
            "4   0.608696  0.800000  0.691358       35  0.615385     pos\n",
            "5   0.756098  0.885714  0.815789       35  0.784615     pos\n",
            "6   0.800000  0.914286  0.853333       35  0.830769     pos\n",
            "7   0.717949  0.800000  0.756757       35  0.723077     pos\n",
            "8   0.857143  0.514286  0.642857       35  0.692308     pos\n",
            "9   0.687500  0.647059  0.666667       34  0.656250     pos\n",
            "0   1.000000  0.032258  0.062500       31  0.538462     neg\n",
            "1   0.857143  0.200000  0.324324       30  0.615385     neg\n",
            "2   0.863636  0.633333  0.730769       30  0.784615     neg\n",
            "3   0.666667  0.200000  0.307692       30  0.584615     neg\n",
            "4   0.631579  0.400000  0.489796       30  0.615385     neg\n",
            "5   0.833333  0.666667  0.740741       30  0.784615     neg\n",
            "6   0.880000  0.733333  0.800000       30  0.830769     neg\n",
            "7   0.730769  0.633333  0.678571       30  0.723077     neg\n",
            "8   0.613636  0.900000  0.729730       30  0.692308     neg\n",
            "9   0.625000  0.666667  0.645161       30  0.656250     neg\n",
            "c =  0.5\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.03      0.06        31\n",
            "         1.0       0.53      1.00      0.69        34\n",
            "\n",
            "    accuracy                           0.54        65\n",
            "   macro avg       0.77      0.52      0.38        65\n",
            "weighted avg       0.75      0.54      0.39        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.20      0.32        30\n",
            "         1.0       0.59      0.97      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.72      0.59      0.53        65\n",
            "weighted avg       0.71      0.62      0.54        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.53      0.65        30\n",
            "         1.0       0.70      0.91      0.79        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.77      0.72      0.72        65\n",
            "weighted avg       0.76      0.74      0.73        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.13      0.22        30\n",
            "         1.0       0.56      0.94      0.70        35\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.61      0.54      0.46        65\n",
            "weighted avg       0.61      0.57      0.48        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.40      0.51        30\n",
            "         1.0       0.62      0.86      0.72        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.67      0.63      0.62        65\n",
            "weighted avg       0.66      0.65      0.62        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.63      0.75        30\n",
            "         1.0       0.75      0.94      0.84        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.83      0.79      0.79        65\n",
            "weighted avg       0.82      0.80      0.79        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.73      0.83        30\n",
            "         1.0       0.81      0.97      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.88      0.85      0.86        65\n",
            "weighted avg       0.88      0.86      0.86        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.53      0.63        30\n",
            "         1.0       0.68      0.86      0.76        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.72      0.70      0.69        65\n",
            "weighted avg       0.72      0.71      0.70        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.64      0.90      0.75        30\n",
            "         1.0       0.87      0.57      0.69        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.76      0.74      0.72        65\n",
            "weighted avg       0.76      0.72      0.72        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.65      0.67      0.66        30\n",
            "         1.0       0.70      0.68      0.69        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.67      0.67      0.67        64\n",
            "weighted avg       0.67      0.67      0.67        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.531250  1.000000  0.693878       34  0.538462     pos\n",
            "1   0.586207  0.971429  0.731183       35  0.615385     pos\n",
            "2   0.695652  0.914286  0.790123       35  0.738462     pos\n",
            "3   0.559322  0.942857  0.702128       35  0.569231     pos\n",
            "4   0.625000  0.857143  0.722892       35  0.646154     pos\n",
            "5   0.750000  0.942857  0.835443       35  0.800000     pos\n",
            "6   0.809524  0.971429  0.883117       35  0.861538     pos\n",
            "7   0.681818  0.857143  0.759494       35  0.707692     pos\n",
            "8   0.869565  0.571429  0.689655       35  0.723077     pos\n",
            "9   0.696970  0.676471  0.686567       34  0.671875     pos\n",
            "0   1.000000  0.032258  0.062500       31  0.538462     neg\n",
            "1   0.857143  0.200000  0.324324       30  0.615385     neg\n",
            "2   0.842105  0.533333  0.653061       30  0.738462     neg\n",
            "3   0.666667  0.133333  0.222222       30  0.569231     neg\n",
            "4   0.705882  0.400000  0.510638       30  0.646154     neg\n",
            "5   0.904762  0.633333  0.745098       30  0.800000     neg\n",
            "6   0.956522  0.733333  0.830189       30  0.861538     neg\n",
            "7   0.761905  0.533333  0.627451       30  0.707692     neg\n",
            "8   0.642857  0.900000  0.750000       30  0.723077     neg\n",
            "9   0.645161  0.666667  0.655738       30  0.671875     neg\n",
            "c =  0.75\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        31\n",
            "         1.0       0.52      1.00      0.69        34\n",
            "\n",
            "    accuracy                           0.52        65\n",
            "   macro avg       0.26      0.50      0.34        65\n",
            "weighted avg       0.27      0.52      0.36        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.20      0.32        30\n",
            "         1.0       0.59      0.97      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.72      0.59      0.53        65\n",
            "weighted avg       0.71      0.62      0.54        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.50      0.62        30\n",
            "         1.0       0.68      0.91      0.78        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.76      0.71      0.70        65\n",
            "weighted avg       0.75      0.72      0.71        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.13      0.22        30\n",
            "         1.0       0.56      0.94      0.70        35\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.61      0.54      0.46        65\n",
            "weighted avg       0.61      0.57      0.48        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.37      0.49        30\n",
            "         1.0       0.62      0.89      0.73        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.68      0.63      0.61        65\n",
            "weighted avg       0.67      0.65      0.62        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.60      0.72        30\n",
            "         1.0       0.73      0.94      0.83        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.82      0.77      0.77        65\n",
            "weighted avg       0.81      0.78      0.78        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.70      0.82        30\n",
            "         1.0       0.80      1.00      0.89        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.90      0.85      0.85        65\n",
            "weighted avg       0.89      0.86      0.86        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.87      0.75        30\n",
            "         1.0       0.85      0.63      0.72        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.76      0.75      0.74        65\n",
            "weighted avg       0.76      0.74      0.74        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.63      0.63        30\n",
            "         1.0       0.68      0.68      0.68        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.65      0.65      0.65        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.523077  1.000000  0.686869       34  0.523077     pos\n",
            "1   0.586207  0.971429  0.731183       35  0.615385     pos\n",
            "2   0.680851  0.914286  0.780488       35  0.723077     pos\n",
            "3   0.559322  0.942857  0.702128       35  0.569231     pos\n",
            "4   0.620000  0.885714  0.729412       35  0.646154     pos\n",
            "5   0.733333  0.942857  0.825000       35  0.784615     pos\n",
            "6   0.795455  1.000000  0.886076       35  0.861538     pos\n",
            "7   0.666667  0.857143  0.750000       35  0.692308     pos\n",
            "8   0.846154  0.628571  0.721311       35  0.738462     pos\n",
            "9   0.676471  0.676471  0.676471       34  0.656250     pos\n",
            "0   0.000000  0.000000  0.000000       31  0.523077     neg\n",
            "1   0.857143  0.200000  0.324324       30  0.615385     neg\n",
            "2   0.833333  0.500000  0.625000       30  0.723077     neg\n",
            "3   0.666667  0.133333  0.222222       30  0.569231     neg\n",
            "4   0.733333  0.366667  0.488889       30  0.646154     neg\n",
            "5   0.900000  0.600000  0.720000       30  0.784615     neg\n",
            "6   1.000000  0.700000  0.823529       30  0.861538     neg\n",
            "7   0.750000  0.500000  0.600000       30  0.692308     neg\n",
            "8   0.666667  0.866667  0.753623       30  0.738462     neg\n",
            "9   0.633333  0.633333  0.633333       30  0.656250     neg\n",
            "c =  1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        31\n",
            "         1.0       0.52      1.00      0.69        34\n",
            "\n",
            "    accuracy                           0.52        65\n",
            "   macro avg       0.26      0.50      0.34        65\n",
            "weighted avg       0.27      0.52      0.36        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.20      0.32        30\n",
            "         1.0       0.59      0.97      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.72      0.59      0.53        65\n",
            "weighted avg       0.71      0.62      0.54        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.47      0.60        30\n",
            "         1.0       0.67      0.91      0.77        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.75      0.69      0.68        65\n",
            "weighted avg       0.74      0.71      0.69        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.10      0.17        30\n",
            "         1.0       0.55      0.94      0.69        35\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.57      0.52      0.43        65\n",
            "weighted avg       0.57      0.55      0.45        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.37      0.50        30\n",
            "         1.0       0.63      0.91      0.74        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.71      0.64      0.62        65\n",
            "weighted avg       0.70      0.66      0.63        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.53      0.67        30\n",
            "         1.0       0.70      0.94      0.80        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.80      0.74      0.74        65\n",
            "weighted avg       0.79      0.75      0.74        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.70      0.82        30\n",
            "         1.0       0.80      1.00      0.89        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.90      0.85      0.85        65\n",
            "weighted avg       0.89      0.86      0.86        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.43      0.54        30\n",
            "         1.0       0.64      0.86      0.73        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.68      0.65      0.64        65\n",
            "weighted avg       0.68      0.66      0.64        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.83      0.77        30\n",
            "         1.0       0.83      0.71      0.77        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.77      0.77      0.77        65\n",
            "weighted avg       0.78      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.63      0.64        30\n",
            "         1.0       0.69      0.71      0.70        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.67      0.67      0.67        64\n",
            "weighted avg       0.67      0.67      0.67        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.523077  1.000000  0.686869       34  0.523077     pos\n",
            "1   0.586207  0.971429  0.731183       35  0.615385     pos\n",
            "2   0.666667  0.914286  0.771084       35  0.707692     pos\n",
            "3   0.550000  0.942857  0.694737       35  0.553846     pos\n",
            "4   0.627451  0.914286  0.744186       35  0.661538     pos\n",
            "5   0.702128  0.942857  0.804878       35  0.753846     pos\n",
            "6   0.795455  1.000000  0.886076       35  0.861538     pos\n",
            "7   0.638298  0.857143  0.731707       35  0.661538     pos\n",
            "8   0.833333  0.714286  0.769231       35  0.769231     pos\n",
            "9   0.685714  0.705882  0.695652       34  0.671875     pos\n",
            "0   0.000000  0.000000  0.000000       31  0.523077     neg\n",
            "1   0.857143  0.200000  0.324324       30  0.615385     neg\n",
            "2   0.823529  0.466667  0.595745       30  0.707692     neg\n",
            "3   0.600000  0.100000  0.171429       30  0.553846     neg\n",
            "4   0.785714  0.366667  0.500000       30  0.661538     neg\n",
            "5   0.888889  0.533333  0.666667       30  0.753846     neg\n",
            "6   1.000000  0.700000  0.823529       30  0.861538     neg\n",
            "7   0.722222  0.433333  0.541667       30  0.661538     neg\n",
            "8   0.714286  0.833333  0.769231       30  0.769231     neg\n",
            "9   0.655172  0.633333  0.644068       30  0.671875     neg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFhxcmvRyVap",
        "colab_type": "text"
      },
      "source": [
        "Resultado dos k-fold para cada valor de c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "5BhEkmnkyVaq",
        "colab_type": "code",
        "outputId": "637ff96d-b2d2-4dde-8a1b-7d2c9617c562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "result_pca"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001,\n",
              "     precision    recall  f1-score  support  accuracy pos/neg  Param(c) method\n",
              "  0   0.566667  1.000000  0.723404       34  0.600000     pos     0.001    PCA\n",
              "  1   0.615385  0.914286  0.735632       35  0.646154     pos     0.001    PCA\n",
              "  2   0.810811  0.857143  0.833333       35  0.815385     pos     0.001    PCA\n",
              "  3   0.617021  0.828571  0.707317       35  0.630769     pos     0.001    PCA\n",
              "  4   0.666667  0.685714  0.676056       35  0.646154     pos     0.001    PCA\n",
              "  5   0.823529  0.800000  0.811594       35  0.800000     pos     0.001    PCA\n",
              "  6   0.828571  0.828571  0.828571       35  0.815385     pos     0.001    PCA\n",
              "  7   0.692308  0.514286  0.590164       35  0.615385     pos     0.001    PCA\n",
              "  8   0.928571  0.371429  0.530612       35  0.646154     pos     0.001    PCA\n",
              "  9   0.736842  0.411765  0.528302       34  0.609375     pos     0.001    PCA\n",
              "  0   1.000000  0.161290  0.277778       31  0.600000     neg     0.001    PCA\n",
              "  1   0.769231  0.333333  0.465116       30  0.646154     neg     0.001    PCA\n",
              "  2   0.821429  0.766667  0.793103       30  0.815385     neg     0.001    PCA\n",
              "  3   0.666667  0.400000  0.500000       30  0.630769     neg     0.001    PCA\n",
              "  4   0.620690  0.600000  0.610169       30  0.646154     neg     0.001    PCA\n",
              "  5   0.774194  0.800000  0.786885       30  0.800000     neg     0.001    PCA\n",
              "  6   0.800000  0.800000  0.800000       30  0.815385     neg     0.001    PCA\n",
              "  7   0.564103  0.733333  0.637681       30  0.615385     neg     0.001    PCA\n",
              "  8   0.568627  0.966667  0.716049       30  0.646154     neg     0.001    PCA\n",
              "  9   0.555556  0.833333  0.666667       30  0.609375     neg     0.001    PCA],\n",
              " [0.1,\n",
              "     precision    recall  f1-score  support  accuracy pos/neg  Param(c) method\n",
              "  0   0.539683  1.000000  0.701031       34  0.553846     pos       0.1    PCA\n",
              "  1   0.589286  0.942857  0.725275       35  0.615385     pos       0.1    PCA\n",
              "  2   0.761905  0.914286  0.831169       35  0.800000     pos       0.1    PCA\n",
              "  3   0.596154  0.885714  0.712644       35  0.615385     pos       0.1    PCA\n",
              "  4   0.604651  0.742857  0.666667       35  0.600000     pos       0.1    PCA\n",
              "  5   0.800000  0.800000  0.800000       35  0.784615     pos       0.1    PCA\n",
              "  6   0.833333  0.857143  0.845070       35  0.830769     pos       0.1    PCA\n",
              "  7   0.696970  0.657143  0.676471       35  0.661538     pos       0.1    PCA\n",
              "  8   0.842105  0.457143  0.592593       35  0.661538     pos       0.1    PCA\n",
              "  9   0.708333  0.500000  0.586207       34  0.625000     pos       0.1    PCA\n",
              "  0   1.000000  0.064516  0.121212       31  0.553846     neg       0.1    PCA\n",
              "  1   0.777778  0.233333  0.358974       30  0.615385     neg       0.1    PCA\n",
              "  2   0.869565  0.666667  0.754717       30  0.800000     neg       0.1    PCA\n",
              "  3   0.692308  0.300000  0.418605       30  0.615385     neg       0.1    PCA\n",
              "  4   0.590909  0.433333  0.500000       30  0.600000     neg       0.1    PCA\n",
              "  5   0.766667  0.766667  0.766667       30  0.784615     neg       0.1    PCA\n",
              "  6   0.827586  0.800000  0.813559       30  0.830769     neg       0.1    PCA\n",
              "  7   0.625000  0.666667  0.645161       30  0.661538     neg       0.1    PCA\n",
              "  8   0.586957  0.900000  0.710526       30  0.661538     neg       0.1    PCA\n",
              "  9   0.575000  0.766667  0.657143       30  0.625000     neg       0.1    PCA],\n",
              " [0.25,\n",
              "     precision    recall  f1-score  support  accuracy pos/neg  Param(c) method\n",
              "  0   0.531250  1.000000  0.693878       34  0.538462     pos      0.25    PCA\n",
              "  1   0.586207  0.971429  0.731183       35  0.615385     pos      0.25    PCA\n",
              "  2   0.744186  0.914286  0.820513       35  0.784615     pos      0.25    PCA\n",
              "  3   0.571429  0.914286  0.703297       35  0.584615     pos      0.25    PCA\n",
              "  4   0.608696  0.800000  0.691358       35  0.615385     pos      0.25    PCA\n",
              "  5   0.756098  0.885714  0.815789       35  0.784615     pos      0.25    PCA\n",
              "  6   0.800000  0.914286  0.853333       35  0.830769     pos      0.25    PCA\n",
              "  7   0.717949  0.800000  0.756757       35  0.723077     pos      0.25    PCA\n",
              "  8   0.857143  0.514286  0.642857       35  0.692308     pos      0.25    PCA\n",
              "  9   0.687500  0.647059  0.666667       34  0.656250     pos      0.25    PCA\n",
              "  0   1.000000  0.032258  0.062500       31  0.538462     neg      0.25    PCA\n",
              "  1   0.857143  0.200000  0.324324       30  0.615385     neg      0.25    PCA\n",
              "  2   0.863636  0.633333  0.730769       30  0.784615     neg      0.25    PCA\n",
              "  3   0.666667  0.200000  0.307692       30  0.584615     neg      0.25    PCA\n",
              "  4   0.631579  0.400000  0.489796       30  0.615385     neg      0.25    PCA\n",
              "  5   0.833333  0.666667  0.740741       30  0.784615     neg      0.25    PCA\n",
              "  6   0.880000  0.733333  0.800000       30  0.830769     neg      0.25    PCA\n",
              "  7   0.730769  0.633333  0.678571       30  0.723077     neg      0.25    PCA\n",
              "  8   0.613636  0.900000  0.729730       30  0.692308     neg      0.25    PCA\n",
              "  9   0.625000  0.666667  0.645161       30  0.656250     neg      0.25    PCA],\n",
              " [0.5,\n",
              "     precision    recall  f1-score  support  accuracy pos/neg  Param(c) method\n",
              "  0   0.531250  1.000000  0.693878       34  0.538462     pos       0.5    PCA\n",
              "  1   0.586207  0.971429  0.731183       35  0.615385     pos       0.5    PCA\n",
              "  2   0.695652  0.914286  0.790123       35  0.738462     pos       0.5    PCA\n",
              "  3   0.559322  0.942857  0.702128       35  0.569231     pos       0.5    PCA\n",
              "  4   0.625000  0.857143  0.722892       35  0.646154     pos       0.5    PCA\n",
              "  5   0.750000  0.942857  0.835443       35  0.800000     pos       0.5    PCA\n",
              "  6   0.809524  0.971429  0.883117       35  0.861538     pos       0.5    PCA\n",
              "  7   0.681818  0.857143  0.759494       35  0.707692     pos       0.5    PCA\n",
              "  8   0.869565  0.571429  0.689655       35  0.723077     pos       0.5    PCA\n",
              "  9   0.696970  0.676471  0.686567       34  0.671875     pos       0.5    PCA\n",
              "  0   1.000000  0.032258  0.062500       31  0.538462     neg       0.5    PCA\n",
              "  1   0.857143  0.200000  0.324324       30  0.615385     neg       0.5    PCA\n",
              "  2   0.842105  0.533333  0.653061       30  0.738462     neg       0.5    PCA\n",
              "  3   0.666667  0.133333  0.222222       30  0.569231     neg       0.5    PCA\n",
              "  4   0.705882  0.400000  0.510638       30  0.646154     neg       0.5    PCA\n",
              "  5   0.904762  0.633333  0.745098       30  0.800000     neg       0.5    PCA\n",
              "  6   0.956522  0.733333  0.830189       30  0.861538     neg       0.5    PCA\n",
              "  7   0.761905  0.533333  0.627451       30  0.707692     neg       0.5    PCA\n",
              "  8   0.642857  0.900000  0.750000       30  0.723077     neg       0.5    PCA\n",
              "  9   0.645161  0.666667  0.655738       30  0.671875     neg       0.5    PCA],\n",
              " [0.75,\n",
              "     precision    recall  f1-score  support  accuracy pos/neg  Param(c) method\n",
              "  0   0.523077  1.000000  0.686869       34  0.523077     pos      0.75    PCA\n",
              "  1   0.586207  0.971429  0.731183       35  0.615385     pos      0.75    PCA\n",
              "  2   0.680851  0.914286  0.780488       35  0.723077     pos      0.75    PCA\n",
              "  3   0.559322  0.942857  0.702128       35  0.569231     pos      0.75    PCA\n",
              "  4   0.620000  0.885714  0.729412       35  0.646154     pos      0.75    PCA\n",
              "  5   0.733333  0.942857  0.825000       35  0.784615     pos      0.75    PCA\n",
              "  6   0.795455  1.000000  0.886076       35  0.861538     pos      0.75    PCA\n",
              "  7   0.666667  0.857143  0.750000       35  0.692308     pos      0.75    PCA\n",
              "  8   0.846154  0.628571  0.721311       35  0.738462     pos      0.75    PCA\n",
              "  9   0.676471  0.676471  0.676471       34  0.656250     pos      0.75    PCA\n",
              "  0   0.000000  0.000000  0.000000       31  0.523077     neg      0.75    PCA\n",
              "  1   0.857143  0.200000  0.324324       30  0.615385     neg      0.75    PCA\n",
              "  2   0.833333  0.500000  0.625000       30  0.723077     neg      0.75    PCA\n",
              "  3   0.666667  0.133333  0.222222       30  0.569231     neg      0.75    PCA\n",
              "  4   0.733333  0.366667  0.488889       30  0.646154     neg      0.75    PCA\n",
              "  5   0.900000  0.600000  0.720000       30  0.784615     neg      0.75    PCA\n",
              "  6   1.000000  0.700000  0.823529       30  0.861538     neg      0.75    PCA\n",
              "  7   0.750000  0.500000  0.600000       30  0.692308     neg      0.75    PCA\n",
              "  8   0.666667  0.866667  0.753623       30  0.738462     neg      0.75    PCA\n",
              "  9   0.633333  0.633333  0.633333       30  0.656250     neg      0.75    PCA],\n",
              " [1,\n",
              "     precision    recall  f1-score  support  accuracy pos/neg  Param(c) method\n",
              "  0   0.523077  1.000000  0.686869       34  0.523077     pos         1    PCA\n",
              "  1   0.586207  0.971429  0.731183       35  0.615385     pos         1    PCA\n",
              "  2   0.666667  0.914286  0.771084       35  0.707692     pos         1    PCA\n",
              "  3   0.550000  0.942857  0.694737       35  0.553846     pos         1    PCA\n",
              "  4   0.627451  0.914286  0.744186       35  0.661538     pos         1    PCA\n",
              "  5   0.702128  0.942857  0.804878       35  0.753846     pos         1    PCA\n",
              "  6   0.795455  1.000000  0.886076       35  0.861538     pos         1    PCA\n",
              "  7   0.638298  0.857143  0.731707       35  0.661538     pos         1    PCA\n",
              "  8   0.833333  0.714286  0.769231       35  0.769231     pos         1    PCA\n",
              "  9   0.685714  0.705882  0.695652       34  0.671875     pos         1    PCA\n",
              "  0   0.000000  0.000000  0.000000       31  0.523077     neg         1    PCA\n",
              "  1   0.857143  0.200000  0.324324       30  0.615385     neg         1    PCA\n",
              "  2   0.823529  0.466667  0.595745       30  0.707692     neg         1    PCA\n",
              "  3   0.600000  0.100000  0.171429       30  0.553846     neg         1    PCA\n",
              "  4   0.785714  0.366667  0.500000       30  0.661538     neg         1    PCA\n",
              "  5   0.888889  0.533333  0.666667       30  0.753846     neg         1    PCA\n",
              "  6   1.000000  0.700000  0.823529       30  0.861538     neg         1    PCA\n",
              "  7   0.722222  0.433333  0.541667       30  0.661538     neg         1    PCA\n",
              "  8   0.714286  0.833333  0.769231       30  0.769231     neg         1    PCA\n",
              "  9   0.655172  0.633333  0.644068       30  0.671875     neg         1    PCA]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPSXY9-tyVau",
        "colab_type": "text"
      },
      "source": [
        "Calcula a média das medidas de cada parâmetro c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tTeRvhucyVaw",
        "colab_type": "code",
        "outputId": "cd0164fc-3e0d-4ae0-ad88-6efe38e859d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "result_pca_mean = calcula_media(result_pca)\n",
        "result_pca_mean"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.721343</td>\n",
              "      <td>0.680319</td>\n",
              "      <td>0.660922</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.682476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.714209</td>\n",
              "      <td>0.667750</td>\n",
              "      <td>0.644184</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.674808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250</td>\n",
              "      <td>0.728111</td>\n",
              "      <td>0.671347</td>\n",
              "      <td>0.644246</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.682548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.739416</td>\n",
              "      <td>0.673532</td>\n",
              "      <td>0.643785</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.687187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.686401</td>\n",
              "      <td>0.665966</td>\n",
              "      <td>0.633993</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.681010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.682764</td>\n",
              "      <td>0.661485</td>\n",
              "      <td>0.627613</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.677957</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "0  0.001        0.721343  ...         32.45       0.682476\n",
              "1  0.100        0.714209  ...         32.45       0.674808\n",
              "2  0.250        0.728111  ...         32.45       0.682548\n",
              "3  0.500        0.739416  ...         32.45       0.687187\n",
              "4  0.750        0.686401  ...         32.45       0.681010\n",
              "5  1.000        0.682764  ...         32.45       0.677957\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n21hC2IuyVa2",
        "colab_type": "text"
      },
      "source": [
        "Obtém o resultado da maior média de acurácia "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7slQvTAkyVa3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "d7febdd6-a10d-4d09-f196-414d9f6476fb"
      },
      "source": [
        "best_accuracy_pca = pd.Series(result_pca_mean.iloc[result_pca_mean['accuracy_mean'].idxmax()], \n",
        "                          name='PCA')\n",
        "best_pca = pd.DataFrame(best_accuracy_pca)\n",
        "best_pca"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PCA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_mean</th>\n",
              "      <td>0.739416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_mean</th>\n",
              "      <td>0.673532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score_mean</th>\n",
              "      <td>0.643785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support_mean</th>\n",
              "      <td>32.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_mean</th>\n",
              "      <td>0.687187</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      PCA\n",
              "c                0.500000\n",
              "precision_mean   0.739416\n",
              "recall_mean      0.673532\n",
              "f1_score_mean    0.643785\n",
              "support_mean    32.450000\n",
              "accuracy_mean    0.687187"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ale_cv-gyVa8",
        "colab_type": "text"
      },
      "source": [
        "# Execução Base: Chi Squared (K-Best)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9dnzjgTyVa9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a708381a-34ee-4e89-b9fd-d7ebe5e9f379"
      },
      "source": [
        "df = pd.read_csv('results/dataset-fs-chi-squared.csv', header = 0)\n",
        "X = df.drop('is_approved', axis=1).to_numpy() # DATASET\n",
        "y = df['is_approved'].to_numpy() # target\n",
        "result_chi , result_chi_g  = stratified_k_fold(X, y, list_c, k=k_folds, name='CHI')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 10, Dataset: 348 positivas, 301 negativas (53% x 46%)\n",
            "Fold 0: Pos: 34, Neg: 31, Total: 65, Proporção: 52% x 47%\n",
            "Fold 1: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 2: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 3: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 4: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 5: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 6: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 7: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 8: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 9: Pos: 34, Neg: 30, Total: 64, Proporção: 53% x 46%\n",
            "c =  0.001\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.16      0.27        31\n",
            "         1.0       0.56      0.97      0.71        34\n",
            "\n",
            "    accuracy                           0.58        65\n",
            "   macro avg       0.70      0.57      0.49        65\n",
            "weighted avg       0.69      0.58      0.50        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.40      0.55        30\n",
            "         1.0       0.65      0.94      0.77        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.75      0.67      0.66        65\n",
            "weighted avg       0.74      0.69      0.66        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.77      0.79        30\n",
            "         1.0       0.81      0.86      0.83        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.82      0.81      0.81        65\n",
            "weighted avg       0.82      0.82      0.81        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.53      0.64        30\n",
            "         1.0       0.69      0.89      0.78        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.74      0.71      0.71        65\n",
            "weighted avg       0.74      0.72      0.71        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67        30\n",
            "         1.0       0.71      0.77      0.74        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.71        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.80      0.84        30\n",
            "         1.0       0.84      0.91      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.86      0.86        65\n",
            "weighted avg       0.86      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.83      0.89        30\n",
            "         1.0       0.87      0.97      0.92        35\n",
            "\n",
            "    accuracy                           0.91        65\n",
            "   macro avg       0.92      0.90      0.91        65\n",
            "weighted avg       0.91      0.91      0.91        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.70      0.70        30\n",
            "         1.0       0.74      0.74      0.74        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.72      0.72      0.72        65\n",
            "weighted avg       0.72      0.72      0.72        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.97      0.75        30\n",
            "         1.0       0.94      0.49      0.64        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.78      0.73      0.70        65\n",
            "weighted avg       0.79      0.71      0.69        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.64      0.77      0.70        30\n",
            "         1.0       0.75      0.62      0.68        34\n",
            "\n",
            "    accuracy                           0.69        64\n",
            "   macro avg       0.69      0.69      0.69        64\n",
            "weighted avg       0.70      0.69      0.69        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.559322  0.970588  0.709677       34  0.584615     pos\n",
            "1   0.647059  0.942857  0.767442       35  0.692308     pos\n",
            "2   0.810811  0.857143  0.833333       35  0.815385     pos\n",
            "3   0.688889  0.885714  0.775000       35  0.723077     pos\n",
            "4   0.710526  0.771429  0.739726       35  0.707692     pos\n",
            "5   0.842105  0.914286  0.876712       35  0.861538     pos\n",
            "6   0.871795  0.971429  0.918919       35  0.907692     pos\n",
            "7   0.742857  0.742857  0.742857       35  0.723077     pos\n",
            "8   0.944444  0.485714  0.641509       35  0.707692     pos\n",
            "9   0.750000  0.617647  0.677419       34  0.687500     pos\n",
            "0   0.833333  0.161290  0.270270       31  0.584615     neg\n",
            "1   0.857143  0.400000  0.545455       30  0.692308     neg\n",
            "2   0.821429  0.766667  0.793103       30  0.815385     neg\n",
            "3   0.800000  0.533333  0.640000       30  0.723077     neg\n",
            "4   0.703704  0.633333  0.666667       30  0.707692     neg\n",
            "5   0.888889  0.800000  0.842105       30  0.861538     neg\n",
            "6   0.961538  0.833333  0.892857       30  0.907692     neg\n",
            "7   0.700000  0.700000  0.700000       30  0.723077     neg\n",
            "8   0.617021  0.966667  0.753247       30  0.707692     neg\n",
            "9   0.638889  0.766667  0.696970       30  0.687500     neg\n",
            "c =  0.1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.33      0.47        30\n",
            "         1.0       0.62      0.91      0.74        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.69      0.62      0.60        65\n",
            "weighted avg       0.69      0.65      0.61        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.70      0.72        30\n",
            "         1.0       0.76      0.80      0.78        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.75      0.75      0.75        65\n",
            "weighted avg       0.75      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67        30\n",
            "         1.0       0.71      0.77      0.74        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.71        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.80      0.84        30\n",
            "         1.0       0.84      0.91      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.86      0.86        65\n",
            "weighted avg       0.86      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.97      0.97        30\n",
            "         1.0       0.97      0.97      0.97        35\n",
            "\n",
            "    accuracy                           0.97        65\n",
            "   macro avg       0.97      0.97      0.97        65\n",
            "weighted avg       0.97      0.97      0.97        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.77      0.73        30\n",
            "         1.0       0.78      0.71      0.75        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.74      0.74      0.74        65\n",
            "weighted avg       0.74      0.74      0.74        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      0.97      0.73        30\n",
            "         1.0       0.94      0.43      0.59        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.76      0.70      0.66        65\n",
            "weighted avg       0.78      0.68      0.66        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.64      0.77      0.70        30\n",
            "         1.0       0.75      0.62      0.68        34\n",
            "\n",
            "    accuracy                           0.69        64\n",
            "   macro avg       0.69      0.69      0.69        64\n",
            "weighted avg       0.70      0.69      0.69        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.540984  0.970588  0.694737       34  0.553846     pos\n",
            "1   0.615385  0.914286  0.735632       35  0.646154     pos\n",
            "2   0.756757  0.800000  0.777778       35  0.753846     pos\n",
            "3   0.666667  0.857143  0.750000       35  0.692308     pos\n",
            "4   0.710526  0.771429  0.739726       35  0.707692     pos\n",
            "5   0.842105  0.914286  0.876712       35  0.861538     pos\n",
            "6   0.971429  0.971429  0.971429       35  0.969231     pos\n",
            "7   0.781250  0.714286  0.746269       35  0.738462     pos\n",
            "8   0.937500  0.428571  0.588235       35  0.676923     pos\n",
            "9   0.750000  0.617647  0.677419       34  0.687500     pos\n",
            "0   0.750000  0.096774  0.171429       31  0.553846     neg\n",
            "1   0.769231  0.333333  0.465116       30  0.646154     neg\n",
            "2   0.750000  0.700000  0.724138       30  0.753846     neg\n",
            "3   0.750000  0.500000  0.600000       30  0.692308     neg\n",
            "4   0.703704  0.633333  0.666667       30  0.707692     neg\n",
            "5   0.888889  0.800000  0.842105       30  0.861538     neg\n",
            "6   0.966667  0.966667  0.966667       30  0.969231     neg\n",
            "7   0.696970  0.766667  0.730159       30  0.738462     neg\n",
            "8   0.591837  0.966667  0.734177       30  0.676923     neg\n",
            "9   0.638889  0.766667  0.696970       30  0.687500     neg\n",
            "c =  0.25\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.27      0.39        30\n",
            "         1.0       0.59      0.91      0.72        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.66      0.59      0.55        65\n",
            "weighted avg       0.65      0.62      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.70      0.72        30\n",
            "         1.0       0.76      0.80      0.78        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.75      0.75      0.75        65\n",
            "weighted avg       0.75      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67        30\n",
            "         1.0       0.71      0.77      0.74        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.71        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.77      0.81        30\n",
            "         1.0       0.82      0.89      0.85        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.83      0.83      0.83        65\n",
            "weighted avg       0.83      0.83      0.83        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.97      0.97        30\n",
            "         1.0       0.97      0.97      0.97        35\n",
            "\n",
            "    accuracy                           0.97        65\n",
            "   macro avg       0.97      0.97      0.97        65\n",
            "weighted avg       0.97      0.97      0.97        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.77      0.74        30\n",
            "         1.0       0.79      0.74      0.76        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.75      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.97      0.74        30\n",
            "         1.0       0.94      0.46      0.62        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.77      0.71      0.68        65\n",
            "weighted avg       0.79      0.69      0.67        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.73      0.67        30\n",
            "         1.0       0.71      0.59      0.65        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.67      0.66      0.66        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.540984  0.970588  0.694737       34  0.553846     pos\n",
            "1   0.592593  0.914286  0.719101       35  0.615385     pos\n",
            "2   0.756757  0.800000  0.777778       35  0.753846     pos\n",
            "3   0.666667  0.857143  0.750000       35  0.692308     pos\n",
            "4   0.710526  0.771429  0.739726       35  0.707692     pos\n",
            "5   0.815789  0.885714  0.849315       35  0.830769     pos\n",
            "6   0.971429  0.971429  0.971429       35  0.969231     pos\n",
            "7   0.787879  0.742857  0.764706       35  0.753846     pos\n",
            "8   0.941176  0.457143  0.615385       35  0.692308     pos\n",
            "9   0.714286  0.588235  0.645161       34  0.656250     pos\n",
            "0   0.750000  0.096774  0.171429       31  0.553846     neg\n",
            "1   0.727273  0.266667  0.390244       30  0.615385     neg\n",
            "2   0.750000  0.700000  0.724138       30  0.753846     neg\n",
            "3   0.750000  0.500000  0.600000       30  0.692308     neg\n",
            "4   0.703704  0.633333  0.666667       30  0.707692     neg\n",
            "5   0.851852  0.766667  0.807018       30  0.830769     neg\n",
            "6   0.966667  0.966667  0.966667       30  0.969231     neg\n",
            "7   0.718750  0.766667  0.741935       30  0.753846     neg\n",
            "8   0.604167  0.966667  0.743590       30  0.692308     neg\n",
            "9   0.611111  0.733333  0.666667       30  0.656250     neg\n",
            "c =  0.5\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.27      0.39        30\n",
            "         1.0       0.59      0.91      0.72        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.66      0.59      0.55        65\n",
            "weighted avg       0.65      0.62      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.67      0.71        30\n",
            "         1.0       0.74      0.83      0.78        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.43      0.55        30\n",
            "         1.0       0.65      0.89      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.71      0.66      0.65        65\n",
            "weighted avg       0.70      0.68      0.66        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.57      0.63        30\n",
            "         1.0       0.68      0.80      0.74        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.70      0.68      0.68        65\n",
            "weighted avg       0.69      0.69      0.69        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.77      0.84        30\n",
            "         1.0       0.82      0.94      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.85      0.86        65\n",
            "weighted avg       0.87      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.93      0.97        30\n",
            "         1.0       0.95      1.00      0.97        35\n",
            "\n",
            "    accuracy                           0.97        65\n",
            "   macro avg       0.97      0.97      0.97        65\n",
            "weighted avg       0.97      0.97      0.97        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.80      0.77        30\n",
            "         1.0       0.82      0.77      0.79        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.78      0.79      0.78        65\n",
            "weighted avg       0.79      0.78      0.78        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.97      0.74        30\n",
            "         1.0       0.94      0.46      0.62        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.77      0.71      0.68        65\n",
            "weighted avg       0.79      0.69      0.67        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.73      0.68        30\n",
            "         1.0       0.72      0.62      0.67        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.68      0.68      0.67        64\n",
            "weighted avg       0.68      0.67      0.67        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.540984  0.970588  0.694737       34  0.553846     pos\n",
            "1   0.592593  0.914286  0.719101       35  0.615385     pos\n",
            "2   0.743590  0.828571  0.783784       35  0.753846     pos\n",
            "3   0.645833  0.885714  0.746988       35  0.676923     pos\n",
            "4   0.682927  0.800000  0.736842       35  0.692308     pos\n",
            "5   0.825000  0.942857  0.880000       35  0.861538     pos\n",
            "6   0.945946  1.000000  0.972222       35  0.969231     pos\n",
            "7   0.818182  0.771429  0.794118       35  0.784615     pos\n",
            "8   0.941176  0.457143  0.615385       35  0.692308     pos\n",
            "9   0.724138  0.617647  0.666667       34  0.671875     pos\n",
            "0   0.750000  0.096774  0.171429       31  0.553846     neg\n",
            "1   0.727273  0.266667  0.390244       30  0.615385     neg\n",
            "2   0.769231  0.666667  0.714286       30  0.753846     neg\n",
            "3   0.764706  0.433333  0.553191       30  0.676923     neg\n",
            "4   0.708333  0.566667  0.629630       30  0.692308     neg\n",
            "5   0.920000  0.766667  0.836364       30  0.861538     neg\n",
            "6   1.000000  0.933333  0.965517       30  0.969231     neg\n",
            "7   0.750000  0.800000  0.774194       30  0.784615     neg\n",
            "8   0.604167  0.966667  0.743590       30  0.692308     neg\n",
            "9   0.628571  0.733333  0.676923       30  0.671875     neg\n",
            "c =  0.75\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.27      0.40        30\n",
            "         1.0       0.60      0.94      0.73        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.70      0.60      0.57        65\n",
            "weighted avg       0.69      0.63      0.58        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.63      0.69        30\n",
            "         1.0       0.72      0.83      0.77        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.74      0.73      0.73        65\n",
            "weighted avg       0.74      0.74      0.74        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.43      0.55        30\n",
            "         1.0       0.65      0.89      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.71      0.66      0.65        65\n",
            "weighted avg       0.70      0.68      0.66        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.53      0.60        30\n",
            "         1.0       0.67      0.80      0.73        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.77      0.84        30\n",
            "         1.0       0.82      0.94      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.85      0.86        65\n",
            "weighted avg       0.87      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.90      0.95        30\n",
            "         1.0       0.92      1.00      0.96        35\n",
            "\n",
            "    accuracy                           0.95        65\n",
            "   macro avg       0.96      0.95      0.95        65\n",
            "weighted avg       0.96      0.95      0.95        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.77      0.77        30\n",
            "         1.0       0.80      0.80      0.80        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.78      0.78      0.78        65\n",
            "weighted avg       0.78      0.78      0.78        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.97      0.74        30\n",
            "         1.0       0.94      0.46      0.62        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.77      0.71      0.68        65\n",
            "weighted avg       0.79      0.69      0.67        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.540984  0.970588  0.694737       34  0.553846     pos\n",
            "1   0.600000  0.942857  0.733333       35  0.630769     pos\n",
            "2   0.725000  0.828571  0.773333       35  0.738462     pos\n",
            "3   0.645833  0.885714  0.746988       35  0.676923     pos\n",
            "4   0.666667  0.800000  0.727273       35  0.676923     pos\n",
            "5   0.825000  0.942857  0.880000       35  0.861538     pos\n",
            "6   0.921053  1.000000  0.958904       35  0.953846     pos\n",
            "7   0.800000  0.800000  0.800000       35  0.784615     pos\n",
            "8   0.941176  0.457143  0.615385       35  0.692308     pos\n",
            "9   0.700000  0.617647  0.656250       34  0.656250     pos\n",
            "0   0.750000  0.096774  0.171429       31  0.553846     neg\n",
            "1   0.800000  0.266667  0.400000       30  0.630769     neg\n",
            "2   0.760000  0.633333  0.690909       30  0.738462     neg\n",
            "3   0.764706  0.433333  0.553191       30  0.676923     neg\n",
            "4   0.695652  0.533333  0.603774       30  0.676923     neg\n",
            "5   0.920000  0.766667  0.836364       30  0.861538     neg\n",
            "6   1.000000  0.900000  0.947368       30  0.953846     neg\n",
            "7   0.766667  0.766667  0.766667       30  0.784615     neg\n",
            "8   0.604167  0.966667  0.743590       30  0.692308     neg\n",
            "9   0.617647  0.700000  0.656250       30  0.656250     neg\n",
            "c =  1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.27      0.40        30\n",
            "         1.0       0.60      0.94      0.73        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.70      0.60      0.57        65\n",
            "weighted avg       0.69      0.63      0.58        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.60      0.67        30\n",
            "         1.0       0.71      0.83      0.76        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.73      0.71      0.71        65\n",
            "weighted avg       0.73      0.72      0.72        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.37      0.49        30\n",
            "         1.0       0.62      0.89      0.73        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.68      0.63      0.61        65\n",
            "weighted avg       0.67      0.65      0.62        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.53      0.60        30\n",
            "         1.0       0.67      0.80      0.73        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.77      0.84        30\n",
            "         1.0       0.82      0.94      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.85      0.86        65\n",
            "weighted avg       0.87      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.90      0.95        30\n",
            "         1.0       0.92      1.00      0.96        35\n",
            "\n",
            "    accuracy                           0.95        65\n",
            "   macro avg       0.96      0.95      0.95        65\n",
            "weighted avg       0.96      0.95      0.95        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.73      0.76        30\n",
            "         1.0       0.78      0.83      0.81        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.78      0.78      0.78        65\n",
            "weighted avg       0.78      0.78      0.78        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.93      0.73        30\n",
            "         1.0       0.89      0.46      0.60        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.74      0.70      0.67        65\n",
            "weighted avg       0.75      0.68      0.66        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.540984  0.970588  0.694737       34  0.553846     pos\n",
            "1   0.600000  0.942857  0.733333       35  0.630769     pos\n",
            "2   0.707317  0.828571  0.763158       35  0.723077     pos\n",
            "3   0.620000  0.885714  0.729412       35  0.646154     pos\n",
            "4   0.666667  0.800000  0.727273       35  0.676923     pos\n",
            "5   0.825000  0.942857  0.880000       35  0.861538     pos\n",
            "6   0.921053  1.000000  0.958904       35  0.953846     pos\n",
            "7   0.783784  0.828571  0.805556       35  0.784615     pos\n",
            "8   0.888889  0.457143  0.603774       35  0.676923     pos\n",
            "9   0.700000  0.617647  0.656250       34  0.656250     pos\n",
            "0   0.750000  0.096774  0.171429       31  0.553846     neg\n",
            "1   0.800000  0.266667  0.400000       30  0.630769     neg\n",
            "2   0.750000  0.600000  0.666667       30  0.723077     neg\n",
            "3   0.733333  0.366667  0.488889       30  0.646154     neg\n",
            "4   0.695652  0.533333  0.603774       30  0.676923     neg\n",
            "5   0.920000  0.766667  0.836364       30  0.861538     neg\n",
            "6   1.000000  0.900000  0.947368       30  0.953846     neg\n",
            "7   0.785714  0.733333  0.758621       30  0.784615     neg\n",
            "8   0.595745  0.933333  0.727273       30  0.676923     neg\n",
            "9   0.617647  0.700000  0.656250       30  0.656250     neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xKkaHx0yVbE",
        "colab_type": "text"
      },
      "source": [
        "Resultado dos k-fold para cada valor de c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_d3uSAFyVbF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36a451e0-7bae-4e83-f137-c6f2df1dd8cd"
      },
      "source": [
        "result_chi"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001,\n",
              "     precision    recall  f1-score  support  accuracy pos/neg  Param(c) method\n",
              "  0   0.559322  0.970588  0.709677       34  0.584615     pos     0.001    CHI\n",
              "  1   0.647059  0.942857  0.767442       35  0.692308     pos     0.001    CHI\n",
              "  2   0.810811  0.857143  0.833333       35  0.815385     pos     0.001    CHI\n",
              "  3   0.688889  0.885714  0.775000       35  0.723077     pos     0.001    CHI\n",
              "  4   0.710526  0.771429  0.739726       35  0.707692     pos     0.001    CHI\n",
              "  5   0.842105  0.914286  0.876712       35  0.861538     pos     0.001    CHI\n",
              "  6   0.871795  0.971429  0.918919       35  0.907692     pos     0.001    CHI\n",
              "  7   0.742857  0.742857  0.742857       35  0.723077     pos     0.001    CHI\n",
              "  8   0.944444  0.485714  0.641509       35  0.707692     pos     0.001    CHI\n",
              "  9   0.750000  0.617647  0.677419       34  0.687500     pos     0.001    CHI\n",
              "  0   0.833333  0.161290  0.270270       31  0.584615     neg     0.001    CHI\n",
              "  1   0.857143  0.400000  0.545455       30  0.692308     neg     0.001    CHI\n",
              "  2   0.821429  0.766667  0.793103       30  0.815385     neg     0.001    CHI\n",
              "  3   0.800000  0.533333  0.640000       30  0.723077     neg     0.001    CHI\n",
              "  4   0.703704  0.633333  0.666667       30  0.707692     neg     0.001    CHI\n",
              "  5   0.888889  0.800000  0.842105       30  0.861538     neg     0.001    CHI\n",
              "  6   0.961538  0.833333  0.892857       30  0.907692     neg     0.001    CHI\n",
              "  7   0.700000  0.700000  0.700000       30  0.723077     neg     0.001    CHI\n",
              "  8   0.617021  0.966667  0.753247       30  0.707692     neg     0.001    CHI\n",
              "  9   0.638889  0.766667  0.696970       30  0.687500     neg     0.001    CHI],\n",
              " [0.1,\n",
              "     precision    recall  f1-score  support  accuracy pos/neg  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846     pos       0.1    CHI\n",
              "  1   0.615385  0.914286  0.735632       35  0.646154     pos       0.1    CHI\n",
              "  2   0.756757  0.800000  0.777778       35  0.753846     pos       0.1    CHI\n",
              "  3   0.666667  0.857143  0.750000       35  0.692308     pos       0.1    CHI\n",
              "  4   0.710526  0.771429  0.739726       35  0.707692     pos       0.1    CHI\n",
              "  5   0.842105  0.914286  0.876712       35  0.861538     pos       0.1    CHI\n",
              "  6   0.971429  0.971429  0.971429       35  0.969231     pos       0.1    CHI\n",
              "  7   0.781250  0.714286  0.746269       35  0.738462     pos       0.1    CHI\n",
              "  8   0.937500  0.428571  0.588235       35  0.676923     pos       0.1    CHI\n",
              "  9   0.750000  0.617647  0.677419       34  0.687500     pos       0.1    CHI\n",
              "  0   0.750000  0.096774  0.171429       31  0.553846     neg       0.1    CHI\n",
              "  1   0.769231  0.333333  0.465116       30  0.646154     neg       0.1    CHI\n",
              "  2   0.750000  0.700000  0.724138       30  0.753846     neg       0.1    CHI\n",
              "  3   0.750000  0.500000  0.600000       30  0.692308     neg       0.1    CHI\n",
              "  4   0.703704  0.633333  0.666667       30  0.707692     neg       0.1    CHI\n",
              "  5   0.888889  0.800000  0.842105       30  0.861538     neg       0.1    CHI\n",
              "  6   0.966667  0.966667  0.966667       30  0.969231     neg       0.1    CHI\n",
              "  7   0.696970  0.766667  0.730159       30  0.738462     neg       0.1    CHI\n",
              "  8   0.591837  0.966667  0.734177       30  0.676923     neg       0.1    CHI\n",
              "  9   0.638889  0.766667  0.696970       30  0.687500     neg       0.1    CHI],\n",
              " [0.25,\n",
              "     precision    recall  f1-score  support  accuracy pos/neg  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846     pos      0.25    CHI\n",
              "  1   0.592593  0.914286  0.719101       35  0.615385     pos      0.25    CHI\n",
              "  2   0.756757  0.800000  0.777778       35  0.753846     pos      0.25    CHI\n",
              "  3   0.666667  0.857143  0.750000       35  0.692308     pos      0.25    CHI\n",
              "  4   0.710526  0.771429  0.739726       35  0.707692     pos      0.25    CHI\n",
              "  5   0.815789  0.885714  0.849315       35  0.830769     pos      0.25    CHI\n",
              "  6   0.971429  0.971429  0.971429       35  0.969231     pos      0.25    CHI\n",
              "  7   0.787879  0.742857  0.764706       35  0.753846     pos      0.25    CHI\n",
              "  8   0.941176  0.457143  0.615385       35  0.692308     pos      0.25    CHI\n",
              "  9   0.714286  0.588235  0.645161       34  0.656250     pos      0.25    CHI\n",
              "  0   0.750000  0.096774  0.171429       31  0.553846     neg      0.25    CHI\n",
              "  1   0.727273  0.266667  0.390244       30  0.615385     neg      0.25    CHI\n",
              "  2   0.750000  0.700000  0.724138       30  0.753846     neg      0.25    CHI\n",
              "  3   0.750000  0.500000  0.600000       30  0.692308     neg      0.25    CHI\n",
              "  4   0.703704  0.633333  0.666667       30  0.707692     neg      0.25    CHI\n",
              "  5   0.851852  0.766667  0.807018       30  0.830769     neg      0.25    CHI\n",
              "  6   0.966667  0.966667  0.966667       30  0.969231     neg      0.25    CHI\n",
              "  7   0.718750  0.766667  0.741935       30  0.753846     neg      0.25    CHI\n",
              "  8   0.604167  0.966667  0.743590       30  0.692308     neg      0.25    CHI\n",
              "  9   0.611111  0.733333  0.666667       30  0.656250     neg      0.25    CHI],\n",
              " [0.5,\n",
              "     precision    recall  f1-score  support  accuracy pos/neg  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846     pos       0.5    CHI\n",
              "  1   0.592593  0.914286  0.719101       35  0.615385     pos       0.5    CHI\n",
              "  2   0.743590  0.828571  0.783784       35  0.753846     pos       0.5    CHI\n",
              "  3   0.645833  0.885714  0.746988       35  0.676923     pos       0.5    CHI\n",
              "  4   0.682927  0.800000  0.736842       35  0.692308     pos       0.5    CHI\n",
              "  5   0.825000  0.942857  0.880000       35  0.861538     pos       0.5    CHI\n",
              "  6   0.945946  1.000000  0.972222       35  0.969231     pos       0.5    CHI\n",
              "  7   0.818182  0.771429  0.794118       35  0.784615     pos       0.5    CHI\n",
              "  8   0.941176  0.457143  0.615385       35  0.692308     pos       0.5    CHI\n",
              "  9   0.724138  0.617647  0.666667       34  0.671875     pos       0.5    CHI\n",
              "  0   0.750000  0.096774  0.171429       31  0.553846     neg       0.5    CHI\n",
              "  1   0.727273  0.266667  0.390244       30  0.615385     neg       0.5    CHI\n",
              "  2   0.769231  0.666667  0.714286       30  0.753846     neg       0.5    CHI\n",
              "  3   0.764706  0.433333  0.553191       30  0.676923     neg       0.5    CHI\n",
              "  4   0.708333  0.566667  0.629630       30  0.692308     neg       0.5    CHI\n",
              "  5   0.920000  0.766667  0.836364       30  0.861538     neg       0.5    CHI\n",
              "  6   1.000000  0.933333  0.965517       30  0.969231     neg       0.5    CHI\n",
              "  7   0.750000  0.800000  0.774194       30  0.784615     neg       0.5    CHI\n",
              "  8   0.604167  0.966667  0.743590       30  0.692308     neg       0.5    CHI\n",
              "  9   0.628571  0.733333  0.676923       30  0.671875     neg       0.5    CHI],\n",
              " [0.75,\n",
              "     precision    recall  f1-score  support  accuracy pos/neg  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846     pos      0.75    CHI\n",
              "  1   0.600000  0.942857  0.733333       35  0.630769     pos      0.75    CHI\n",
              "  2   0.725000  0.828571  0.773333       35  0.738462     pos      0.75    CHI\n",
              "  3   0.645833  0.885714  0.746988       35  0.676923     pos      0.75    CHI\n",
              "  4   0.666667  0.800000  0.727273       35  0.676923     pos      0.75    CHI\n",
              "  5   0.825000  0.942857  0.880000       35  0.861538     pos      0.75    CHI\n",
              "  6   0.921053  1.000000  0.958904       35  0.953846     pos      0.75    CHI\n",
              "  7   0.800000  0.800000  0.800000       35  0.784615     pos      0.75    CHI\n",
              "  8   0.941176  0.457143  0.615385       35  0.692308     pos      0.75    CHI\n",
              "  9   0.700000  0.617647  0.656250       34  0.656250     pos      0.75    CHI\n",
              "  0   0.750000  0.096774  0.171429       31  0.553846     neg      0.75    CHI\n",
              "  1   0.800000  0.266667  0.400000       30  0.630769     neg      0.75    CHI\n",
              "  2   0.760000  0.633333  0.690909       30  0.738462     neg      0.75    CHI\n",
              "  3   0.764706  0.433333  0.553191       30  0.676923     neg      0.75    CHI\n",
              "  4   0.695652  0.533333  0.603774       30  0.676923     neg      0.75    CHI\n",
              "  5   0.920000  0.766667  0.836364       30  0.861538     neg      0.75    CHI\n",
              "  6   1.000000  0.900000  0.947368       30  0.953846     neg      0.75    CHI\n",
              "  7   0.766667  0.766667  0.766667       30  0.784615     neg      0.75    CHI\n",
              "  8   0.604167  0.966667  0.743590       30  0.692308     neg      0.75    CHI\n",
              "  9   0.617647  0.700000  0.656250       30  0.656250     neg      0.75    CHI],\n",
              " [1,\n",
              "     precision    recall  f1-score  support  accuracy pos/neg  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846     pos         1    CHI\n",
              "  1   0.600000  0.942857  0.733333       35  0.630769     pos         1    CHI\n",
              "  2   0.707317  0.828571  0.763158       35  0.723077     pos         1    CHI\n",
              "  3   0.620000  0.885714  0.729412       35  0.646154     pos         1    CHI\n",
              "  4   0.666667  0.800000  0.727273       35  0.676923     pos         1    CHI\n",
              "  5   0.825000  0.942857  0.880000       35  0.861538     pos         1    CHI\n",
              "  6   0.921053  1.000000  0.958904       35  0.953846     pos         1    CHI\n",
              "  7   0.783784  0.828571  0.805556       35  0.784615     pos         1    CHI\n",
              "  8   0.888889  0.457143  0.603774       35  0.676923     pos         1    CHI\n",
              "  9   0.700000  0.617647  0.656250       34  0.656250     pos         1    CHI\n",
              "  0   0.750000  0.096774  0.171429       31  0.553846     neg         1    CHI\n",
              "  1   0.800000  0.266667  0.400000       30  0.630769     neg         1    CHI\n",
              "  2   0.750000  0.600000  0.666667       30  0.723077     neg         1    CHI\n",
              "  3   0.733333  0.366667  0.488889       30  0.646154     neg         1    CHI\n",
              "  4   0.695652  0.533333  0.603774       30  0.676923     neg         1    CHI\n",
              "  5   0.920000  0.766667  0.836364       30  0.861538     neg         1    CHI\n",
              "  6   1.000000  0.900000  0.947368       30  0.953846     neg         1    CHI\n",
              "  7   0.785714  0.733333  0.758621       30  0.784615     neg         1    CHI\n",
              "  8   0.595745  0.933333  0.727273       30  0.676923     neg         1    CHI\n",
              "  9   0.617647  0.700000  0.656250       30  0.656250     neg         1    CHI]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxKFeV75yVbK",
        "colab_type": "text"
      },
      "source": [
        "Calcula a média das medidas de cada parâmetro c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiwgWhz2yVbL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "e3a246d6-080b-4739-fc39-3ceef575e2d4"
      },
      "source": [
        "result_chi_mean = calcula_media(result_chi)\n",
        "result_chi_mean"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.769488</td>\n",
              "      <td>0.736048</td>\n",
              "      <td>0.724163</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.741058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.753939</td>\n",
              "      <td>0.724489</td>\n",
              "      <td>0.707768</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.728750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250</td>\n",
              "      <td>0.746580</td>\n",
              "      <td>0.717780</td>\n",
              "      <td>0.700285</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.722548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.754132</td>\n",
              "      <td>0.720917</td>\n",
              "      <td>0.703260</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.727187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.752228</td>\n",
              "      <td>0.715441</td>\n",
              "      <td>0.697787</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.722548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.745089</td>\n",
              "      <td>0.708536</td>\n",
              "      <td>0.690451</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.716394</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "0  0.001        0.769488  ...         32.45       0.741058\n",
              "1  0.100        0.753939  ...         32.45       0.728750\n",
              "2  0.250        0.746580  ...         32.45       0.722548\n",
              "3  0.500        0.754132  ...         32.45       0.727187\n",
              "4  0.750        0.752228  ...         32.45       0.722548\n",
              "5  1.000        0.745089  ...         32.45       0.716394\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0PrGh7-yVbP",
        "colab_type": "text"
      },
      "source": [
        "Obtém o resultado da maior média de acurácia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZIuVq2JyVbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "17ff417a-9459-4a4f-aecd-0b427bf540b9"
      },
      "source": [
        "best_accuracy_chi = pd.Series(result_chi_mean.iloc[result_chi_mean['accuracy_mean'].idxmax()], \n",
        "                          name='Chi Squared')\n",
        "best_chi = pd.DataFrame(best_accuracy_chi)\n",
        "best_chi"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Chi Squared</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_mean</th>\n",
              "      <td>0.769488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_mean</th>\n",
              "      <td>0.736048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score_mean</th>\n",
              "      <td>0.724163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support_mean</th>\n",
              "      <td>32.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_mean</th>\n",
              "      <td>0.741058</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Chi Squared\n",
              "c                  0.001000\n",
              "precision_mean     0.769488\n",
              "recall_mean        0.736048\n",
              "f1_score_mean      0.724163\n",
              "support_mean      32.450000\n",
              "accuracy_mean      0.741058"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9NNSbziyVbU",
        "colab_type": "text"
      },
      "source": [
        "# Execução Base: recursive-feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1WD345tyVbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2bbeb091-98bc-481b-b627-93387ff26425"
      },
      "source": [
        "df = pd.read_csv('results/dataset-fs-recursive-feature.csv', header = 0)\n",
        "X = df.drop('is_approved', axis=1).to_numpy() # DATASET\n",
        "y = df['is_approved'].to_numpy() # target\n",
        "result_recursive , result_recursive_g  = stratified_k_fold(X, y, list_c, k=k_folds, name='Recursive')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 10, Dataset: 348 positivas, 301 negativas (53% x 46%)\n",
            "Fold 0: Pos: 34, Neg: 31, Total: 65, Proporção: 52% x 47%\n",
            "Fold 1: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 2: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 3: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 4: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 5: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 6: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 7: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 8: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 9: Pos: 34, Neg: 30, Total: 64, Proporção: 53% x 46%\n",
            "c =  0.001\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.13      0.23        31\n",
            "         1.0       0.56      1.00      0.72        34\n",
            "\n",
            "    accuracy                           0.58        65\n",
            "   macro avg       0.78      0.56      0.47        65\n",
            "weighted avg       0.77      0.58      0.48        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.40      0.53        30\n",
            "         1.0       0.64      0.91      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.72      0.66      0.64        65\n",
            "weighted avg       0.71      0.68      0.65        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.67      0.75        30\n",
            "         1.0       0.76      0.91      0.83        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.82      0.79      0.79        65\n",
            "weighted avg       0.81      0.80      0.80        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.47      0.55        30\n",
            "         1.0       0.64      0.80      0.71        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.65      0.63      0.63        65\n",
            "weighted avg       0.65      0.65      0.64        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.60      0.71        30\n",
            "         1.0       0.73      0.91      0.81        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.79      0.76      0.76        65\n",
            "weighted avg       0.79      0.77      0.76        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.67      0.75        30\n",
            "         1.0       0.76      0.91      0.83        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.82      0.79      0.79        65\n",
            "weighted avg       0.81      0.80      0.80        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.80      0.87        30\n",
            "         1.0       0.85      0.97      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.91      0.89      0.89        65\n",
            "weighted avg       0.90      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.47      0.56        30\n",
            "         1.0       0.64      0.83      0.73        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.67      0.65      0.64        65\n",
            "weighted avg       0.67      0.66      0.65        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.87      0.78        30\n",
            "         1.0       0.86      0.69      0.76        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.78      0.78      0.77        65\n",
            "weighted avg       0.79      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.73      0.71        30\n",
            "         1.0       0.75      0.71      0.73        34\n",
            "\n",
            "    accuracy                           0.72        64\n",
            "   macro avg       0.72      0.72      0.72        64\n",
            "weighted avg       0.72      0.72      0.72        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.557377  1.000000  0.715789       34  0.584615     pos\n",
            "1   0.640000  0.914286  0.752941       35  0.676923     pos\n",
            "2   0.761905  0.914286  0.831169       35  0.800000     pos\n",
            "3   0.636364  0.800000  0.708861       35  0.646154     pos\n",
            "4   0.727273  0.914286  0.810127       35  0.769231     pos\n",
            "5   0.761905  0.914286  0.831169       35  0.800000     pos\n",
            "6   0.850000  0.971429  0.906667       35  0.892308     pos\n",
            "7   0.644444  0.828571  0.725000       35  0.661538     pos\n",
            "8   0.857143  0.685714  0.761905       35  0.769231     pos\n",
            "9   0.750000  0.705882  0.727273       34  0.718750     pos\n",
            "0   1.000000  0.129032  0.228571       31  0.584615     neg\n",
            "1   0.800000  0.400000  0.533333       30  0.676923     neg\n",
            "2   0.869565  0.666667  0.754717       30  0.800000     neg\n",
            "3   0.666667  0.466667  0.549020       30  0.646154     neg\n",
            "4   0.857143  0.600000  0.705882       30  0.769231     neg\n",
            "5   0.869565  0.666667  0.754717       30  0.800000     neg\n",
            "6   0.960000  0.800000  0.872727       30  0.892308     neg\n",
            "7   0.700000  0.466667  0.560000       30  0.661538     neg\n",
            "8   0.702703  0.866667  0.776119       30  0.769231     neg\n",
            "9   0.687500  0.733333  0.709677       30  0.718750     neg\n",
            "c =  0.1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.06      0.12        31\n",
            "         1.0       0.53      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.54        65\n",
            "   macro avg       0.60      0.52      0.40        65\n",
            "weighted avg       0.60      0.54      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.33      0.49        30\n",
            "         1.0       0.63      0.97      0.76        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.77      0.65      0.63        65\n",
            "weighted avg       0.76      0.68      0.64        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.57      0.68        30\n",
            "         1.0       0.71      0.91      0.80        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.78      0.74      0.74        65\n",
            "weighted avg       0.78      0.75      0.74        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.40      0.53        30\n",
            "         1.0       0.64      0.91      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.72      0.66      0.64        65\n",
            "weighted avg       0.71      0.68      0.65        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.63      0.72        30\n",
            "         1.0       0.74      0.89      0.81        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.78      0.76      0.76        65\n",
            "weighted avg       0.78      0.77      0.76        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.67      0.78        30\n",
            "         1.0       0.77      0.97      0.86        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.86      0.82      0.82        65\n",
            "weighted avg       0.86      0.83      0.83        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.80      0.87        30\n",
            "         1.0       0.85      0.97      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.91      0.89      0.89        65\n",
            "weighted avg       0.90      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.47      0.57        30\n",
            "         1.0       0.65      0.86      0.74        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.69      0.66      0.66        65\n",
            "weighted avg       0.69      0.68      0.66        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.93      0.79        30\n",
            "         1.0       0.92      0.63      0.75        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.80      0.78      0.77        65\n",
            "weighted avg       0.81      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.70      0.69        30\n",
            "         1.0       0.73      0.71      0.72        34\n",
            "\n",
            "    accuracy                           0.70        64\n",
            "   macro avg       0.70      0.70      0.70        64\n",
            "weighted avg       0.70      0.70      0.70        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.532258  0.970588  0.687500       34  0.538462     pos\n",
            "1   0.629630  0.971429  0.764045       35  0.676923     pos\n",
            "2   0.711111  0.914286  0.800000       35  0.753846     pos\n",
            "3   0.640000  0.914286  0.752941       35  0.676923     pos\n",
            "4   0.738095  0.885714  0.805195       35  0.769231     pos\n",
            "5   0.772727  0.971429  0.860759       35  0.830769     pos\n",
            "6   0.850000  0.971429  0.906667       35  0.892308     pos\n",
            "7   0.652174  0.857143  0.740741       35  0.676923     pos\n",
            "8   0.916667  0.628571  0.745763       35  0.769231     pos\n",
            "9   0.727273  0.705882  0.716418       34  0.703125     pos\n",
            "0   0.666667  0.064516  0.117647       31  0.538462     neg\n",
            "1   0.909091  0.333333  0.487805       30  0.676923     neg\n",
            "2   0.850000  0.566667  0.680000       30  0.753846     neg\n",
            "3   0.800000  0.400000  0.533333       30  0.676923     neg\n",
            "4   0.826087  0.633333  0.716981       30  0.769231     neg\n",
            "5   0.952381  0.666667  0.784314       30  0.830769     neg\n",
            "6   0.960000  0.800000  0.872727       30  0.892308     neg\n",
            "7   0.736842  0.466667  0.571429       30  0.676923     neg\n",
            "8   0.682927  0.933333  0.788732       30  0.769231     neg\n",
            "9   0.677419  0.700000  0.688525       30  0.703125     neg\n",
            "c =  0.25\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.06      0.12        31\n",
            "         1.0       0.53      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.54        65\n",
            "   macro avg       0.60      0.52      0.40        65\n",
            "weighted avg       0.60      0.54      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.23      0.37        30\n",
            "         1.0       0.60      0.97      0.74        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.74      0.60      0.55        65\n",
            "weighted avg       0.73      0.63      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.57      0.68        30\n",
            "         1.0       0.71      0.91      0.80        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.78      0.74      0.74        65\n",
            "weighted avg       0.78      0.75      0.74        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.37      0.51        30\n",
            "         1.0       0.63      0.94      0.76        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.74      0.65      0.64        65\n",
            "weighted avg       0.73      0.68      0.64        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.57      0.68        30\n",
            "         1.0       0.71      0.91      0.80        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.78      0.74      0.74        65\n",
            "weighted avg       0.78      0.75      0.74        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.60      0.75        30\n",
            "         1.0       0.74      1.00      0.85        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.87      0.80      0.80        65\n",
            "weighted avg       0.86      0.82      0.81        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.77      0.85        30\n",
            "         1.0       0.83      0.97      0.89        35\n",
            "\n",
            "    accuracy                           0.88        65\n",
            "   macro avg       0.89      0.87      0.87        65\n",
            "weighted avg       0.89      0.88      0.87        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.47      0.60        30\n",
            "         1.0       0.67      0.91      0.77        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.75      0.69      0.68        65\n",
            "weighted avg       0.74      0.71      0.69        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.90      0.78        30\n",
            "         1.0       0.88      0.66      0.75        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.79      0.78      0.77        65\n",
            "weighted avg       0.80      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.73      0.71        30\n",
            "         1.0       0.75      0.71      0.73        34\n",
            "\n",
            "    accuracy                           0.72        64\n",
            "   macro avg       0.72      0.72      0.72        64\n",
            "weighted avg       0.72      0.72      0.72        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.532258  0.970588  0.687500       34  0.538462     pos\n",
            "1   0.596491  0.971429  0.739130       35  0.630769     pos\n",
            "2   0.711111  0.914286  0.800000       35  0.753846     pos\n",
            "3   0.634615  0.942857  0.758621       35  0.676923     pos\n",
            "4   0.711111  0.914286  0.800000       35  0.753846     pos\n",
            "5   0.744681  1.000000  0.853659       35  0.815385     pos\n",
            "6   0.829268  0.971429  0.894737       35  0.876923     pos\n",
            "7   0.666667  0.914286  0.771084       35  0.707692     pos\n",
            "8   0.884615  0.657143  0.754098       35  0.769231     pos\n",
            "9   0.750000  0.705882  0.727273       34  0.718750     pos\n",
            "0   0.666667  0.064516  0.117647       31  0.538462     neg\n",
            "1   0.875000  0.233333  0.368421       30  0.630769     neg\n",
            "2   0.850000  0.566667  0.680000       30  0.753846     neg\n",
            "3   0.846154  0.366667  0.511628       30  0.676923     neg\n",
            "4   0.850000  0.566667  0.680000       30  0.753846     neg\n",
            "5   1.000000  0.600000  0.750000       30  0.815385     neg\n",
            "6   0.958333  0.766667  0.851852       30  0.876923     neg\n",
            "7   0.823529  0.466667  0.595745       30  0.707692     neg\n",
            "8   0.692308  0.900000  0.782609       30  0.769231     neg\n",
            "9   0.687500  0.733333  0.709677       30  0.718750     neg\n",
            "c =  0.5\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.06      0.12        31\n",
            "         1.0       0.54      1.00      0.70        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.77      0.53      0.41        65\n",
            "weighted avg       0.76      0.55      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.17      0.28        30\n",
            "         1.0       0.58      0.97      0.72        35\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.70      0.57      0.50        65\n",
            "weighted avg       0.69      0.60      0.52        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.47      0.61        30\n",
            "         1.0       0.67      0.94      0.79        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.77      0.70      0.70        65\n",
            "weighted avg       0.77      0.72      0.70        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.30      0.44        30\n",
            "         1.0       0.61      0.94      0.74        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.71      0.62      0.59        65\n",
            "weighted avg       0.71      0.65      0.60        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.53      0.68        30\n",
            "         1.0       0.71      0.97      0.82        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.82      0.75      0.75        65\n",
            "weighted avg       0.82      0.77      0.76        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.60      0.75        30\n",
            "         1.0       0.74      1.00      0.85        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.87      0.80      0.80        65\n",
            "weighted avg       0.86      0.82      0.81        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.77      0.87        30\n",
            "         1.0       0.83      1.00      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.92      0.88      0.89        65\n",
            "weighted avg       0.91      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.47      0.60        30\n",
            "         1.0       0.67      0.91      0.77        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.75      0.69      0.68        65\n",
            "weighted avg       0.74      0.71      0.69        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.83      0.76        30\n",
            "         1.0       0.83      0.69      0.75        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.76      0.75        65\n",
            "weighted avg       0.77      0.75      0.75        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.70      0.69        30\n",
            "         1.0       0.73      0.71      0.72        34\n",
            "\n",
            "    accuracy                           0.70        64\n",
            "   macro avg       0.70      0.70      0.70        64\n",
            "weighted avg       0.70      0.70      0.70        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.539683  1.000000  0.701031       34  0.553846     pos\n",
            "1   0.576271  0.971429  0.723404       35  0.600000     pos\n",
            "2   0.673469  0.942857  0.785714       35  0.723077     pos\n",
            "3   0.611111  0.942857  0.741573       35  0.646154     pos\n",
            "4   0.708333  0.971429  0.819277       35  0.769231     pos\n",
            "5   0.744681  1.000000  0.853659       35  0.815385     pos\n",
            "6   0.833333  1.000000  0.909091       35  0.892308     pos\n",
            "7   0.666667  0.914286  0.771084       35  0.707692     pos\n",
            "8   0.827586  0.685714  0.750000       35  0.753846     pos\n",
            "9   0.727273  0.705882  0.716418       34  0.703125     pos\n",
            "0   1.000000  0.064516  0.121212       31  0.553846     neg\n",
            "1   0.833333  0.166667  0.277778       30  0.600000     neg\n",
            "2   0.875000  0.466667  0.608696       30  0.723077     neg\n",
            "3   0.818182  0.300000  0.439024       30  0.646154     neg\n",
            "4   0.941176  0.533333  0.680851       30  0.769231     neg\n",
            "5   1.000000  0.600000  0.750000       30  0.815385     neg\n",
            "6   1.000000  0.766667  0.867925       30  0.892308     neg\n",
            "7   0.823529  0.466667  0.595745       30  0.707692     neg\n",
            "8   0.694444  0.833333  0.757576       30  0.753846     neg\n",
            "9   0.677419  0.700000  0.688525       30  0.703125     neg\n",
            "c =  0.75\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.06      0.12        31\n",
            "         1.0       0.54      1.00      0.70        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.77      0.53      0.41        65\n",
            "weighted avg       0.76      0.55      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.17      0.28        30\n",
            "         1.0       0.58      0.97      0.72        35\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.70      0.57      0.50        65\n",
            "weighted avg       0.69      0.60      0.52        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.43      0.59        30\n",
            "         1.0       0.67      0.97      0.79        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.80      0.70      0.69        65\n",
            "weighted avg       0.79      0.72      0.70        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.30      0.44        30\n",
            "         1.0       0.61      0.94      0.74        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.71      0.62      0.59        65\n",
            "weighted avg       0.71      0.65      0.60        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.50      0.65        30\n",
            "         1.0       0.69      0.97      0.81        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.82      0.74      0.73        65\n",
            "weighted avg       0.81      0.75      0.74        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.57      0.72        30\n",
            "         1.0       0.73      1.00      0.84        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.86      0.78      0.78        65\n",
            "weighted avg       0.85      0.80      0.79        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.77      0.87        30\n",
            "         1.0       0.83      1.00      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.92      0.88      0.89        65\n",
            "weighted avg       0.91      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.43      0.57        30\n",
            "         1.0       0.65      0.91      0.76        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.73      0.67      0.66        65\n",
            "weighted avg       0.73      0.69      0.67        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.80      0.75        30\n",
            "         1.0       0.81      0.71      0.76        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.76      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.67      0.67        30\n",
            "         1.0       0.71      0.71      0.71        34\n",
            "\n",
            "    accuracy                           0.69        64\n",
            "   macro avg       0.69      0.69      0.69        64\n",
            "weighted avg       0.69      0.69      0.69        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.539683  1.000000  0.701031       34  0.553846     pos\n",
            "1   0.576271  0.971429  0.723404       35  0.600000     pos\n",
            "2   0.666667  0.971429  0.790698       35  0.723077     pos\n",
            "3   0.611111  0.942857  0.741573       35  0.646154     pos\n",
            "4   0.693878  0.971429  0.809524       35  0.753846     pos\n",
            "5   0.729167  1.000000  0.843373       35  0.800000     pos\n",
            "6   0.833333  1.000000  0.909091       35  0.892308     pos\n",
            "7   0.653061  0.914286  0.761905       35  0.692308     pos\n",
            "8   0.806452  0.714286  0.757576       35  0.753846     pos\n",
            "9   0.705882  0.705882  0.705882       34  0.687500     pos\n",
            "0   1.000000  0.064516  0.121212       31  0.553846     neg\n",
            "1   0.833333  0.166667  0.277778       30  0.600000     neg\n",
            "2   0.928571  0.433333  0.590909       30  0.723077     neg\n",
            "3   0.818182  0.300000  0.439024       30  0.646154     neg\n",
            "4   0.937500  0.500000  0.652174       30  0.753846     neg\n",
            "5   1.000000  0.566667  0.723404       30  0.800000     neg\n",
            "6   1.000000  0.766667  0.867925       30  0.892308     neg\n",
            "7   0.812500  0.433333  0.565217       30  0.692308     neg\n",
            "8   0.705882  0.800000  0.750000       30  0.753846     neg\n",
            "9   0.666667  0.666667  0.666667       30  0.687500     neg\n",
            "c =  1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.06      0.12        31\n",
            "         1.0       0.54      1.00      0.70        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.77      0.53      0.41        65\n",
            "weighted avg       0.76      0.55      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.17      0.28        30\n",
            "         1.0       0.58      0.97      0.72        35\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.70      0.57      0.50        65\n",
            "weighted avg       0.69      0.60      0.52        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.40      0.57        30\n",
            "         1.0       0.66      1.00      0.80        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.83      0.70      0.68        65\n",
            "weighted avg       0.82      0.72      0.69        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.27      0.40        30\n",
            "         1.0       0.60      0.94      0.73        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.70      0.60      0.57        65\n",
            "weighted avg       0.69      0.63      0.58        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.43      0.59        30\n",
            "         1.0       0.67      0.97      0.79        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.80      0.70      0.69        65\n",
            "weighted avg       0.79      0.72      0.70        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.57      0.72        30\n",
            "         1.0       0.73      1.00      0.84        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.86      0.78      0.78        65\n",
            "weighted avg       0.85      0.80      0.79        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.73      0.85        30\n",
            "         1.0       0.81      1.00      0.90        35\n",
            "\n",
            "    accuracy                           0.88        65\n",
            "   macro avg       0.91      0.87      0.87        65\n",
            "weighted avg       0.90      0.88      0.87        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.40      0.53        30\n",
            "         1.0       0.64      0.91      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.72      0.66      0.64        65\n",
            "weighted avg       0.71      0.68      0.65        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.77      0.75        30\n",
            "         1.0       0.79      0.77      0.78        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.77      0.77      0.77        65\n",
            "weighted avg       0.77      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.63      0.64        30\n",
            "         1.0       0.69      0.71      0.70        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.67      0.67      0.67        64\n",
            "weighted avg       0.67      0.67      0.67        64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "   precision    recall  f1-score  support  accuracy pos/neg\n",
            "0   0.539683  1.000000  0.701031       34  0.553846     pos\n",
            "1   0.576271  0.971429  0.723404       35  0.600000     pos\n",
            "2   0.660377  1.000000  0.795455       35  0.723077     pos\n",
            "3   0.600000  0.942857  0.733333       35  0.630769     pos\n",
            "4   0.666667  0.971429  0.790698       35  0.723077     pos\n",
            "5   0.729167  1.000000  0.843373       35  0.800000     pos\n",
            "6   0.813953  1.000000  0.897436       35  0.876923     pos\n",
            "7   0.640000  0.914286  0.752941       35  0.676923     pos\n",
            "8   0.794118  0.771429  0.782609       35  0.769231     pos\n",
            "9   0.685714  0.705882  0.695652       34  0.671875     pos\n",
            "0   1.000000  0.064516  0.121212       31  0.553846     neg\n",
            "1   0.833333  0.166667  0.277778       30  0.600000     neg\n",
            "2   1.000000  0.400000  0.571429       30  0.723077     neg\n",
            "3   0.800000  0.266667  0.400000       30  0.630769     neg\n",
            "4   0.928571  0.433333  0.590909       30  0.723077     neg\n",
            "5   1.000000  0.566667  0.723404       30  0.800000     neg\n",
            "6   1.000000  0.733333  0.846154       30  0.876923     neg\n",
            "7   0.800000  0.400000  0.533333       30  0.676923     neg\n",
            "8   0.741935  0.766667  0.754098       30  0.769231     neg\n",
            "9   0.655172  0.633333  0.644068       30  0.671875     neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Hx8mYEyVbZ",
        "colab_type": "text"
      },
      "source": [
        "Resultado dos k-fold para cada valor de c "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACFrA5kCyVba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6495892-d759-4903-e157-6c582dc91ddc"
      },
      "source": [
        "result_recursive"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001,    precision    recall  f1-score  ...  pos/neg  Param(c)     method\n",
              "  0   0.557377  1.000000  0.715789  ...      pos     0.001  Recursive\n",
              "  1   0.640000  0.914286  0.752941  ...      pos     0.001  Recursive\n",
              "  2   0.761905  0.914286  0.831169  ...      pos     0.001  Recursive\n",
              "  3   0.636364  0.800000  0.708861  ...      pos     0.001  Recursive\n",
              "  4   0.727273  0.914286  0.810127  ...      pos     0.001  Recursive\n",
              "  5   0.761905  0.914286  0.831169  ...      pos     0.001  Recursive\n",
              "  6   0.850000  0.971429  0.906667  ...      pos     0.001  Recursive\n",
              "  7   0.644444  0.828571  0.725000  ...      pos     0.001  Recursive\n",
              "  8   0.857143  0.685714  0.761905  ...      pos     0.001  Recursive\n",
              "  9   0.750000  0.705882  0.727273  ...      pos     0.001  Recursive\n",
              "  0   1.000000  0.129032  0.228571  ...      neg     0.001  Recursive\n",
              "  1   0.800000  0.400000  0.533333  ...      neg     0.001  Recursive\n",
              "  2   0.869565  0.666667  0.754717  ...      neg     0.001  Recursive\n",
              "  3   0.666667  0.466667  0.549020  ...      neg     0.001  Recursive\n",
              "  4   0.857143  0.600000  0.705882  ...      neg     0.001  Recursive\n",
              "  5   0.869565  0.666667  0.754717  ...      neg     0.001  Recursive\n",
              "  6   0.960000  0.800000  0.872727  ...      neg     0.001  Recursive\n",
              "  7   0.700000  0.466667  0.560000  ...      neg     0.001  Recursive\n",
              "  8   0.702703  0.866667  0.776119  ...      neg     0.001  Recursive\n",
              "  9   0.687500  0.733333  0.709677  ...      neg     0.001  Recursive\n",
              "  \n",
              "  [20 rows x 8 columns]],\n",
              " [0.1,    precision    recall  f1-score  ...  pos/neg  Param(c)     method\n",
              "  0   0.532258  0.970588  0.687500  ...      pos       0.1  Recursive\n",
              "  1   0.629630  0.971429  0.764045  ...      pos       0.1  Recursive\n",
              "  2   0.711111  0.914286  0.800000  ...      pos       0.1  Recursive\n",
              "  3   0.640000  0.914286  0.752941  ...      pos       0.1  Recursive\n",
              "  4   0.738095  0.885714  0.805195  ...      pos       0.1  Recursive\n",
              "  5   0.772727  0.971429  0.860759  ...      pos       0.1  Recursive\n",
              "  6   0.850000  0.971429  0.906667  ...      pos       0.1  Recursive\n",
              "  7   0.652174  0.857143  0.740741  ...      pos       0.1  Recursive\n",
              "  8   0.916667  0.628571  0.745763  ...      pos       0.1  Recursive\n",
              "  9   0.727273  0.705882  0.716418  ...      pos       0.1  Recursive\n",
              "  0   0.666667  0.064516  0.117647  ...      neg       0.1  Recursive\n",
              "  1   0.909091  0.333333  0.487805  ...      neg       0.1  Recursive\n",
              "  2   0.850000  0.566667  0.680000  ...      neg       0.1  Recursive\n",
              "  3   0.800000  0.400000  0.533333  ...      neg       0.1  Recursive\n",
              "  4   0.826087  0.633333  0.716981  ...      neg       0.1  Recursive\n",
              "  5   0.952381  0.666667  0.784314  ...      neg       0.1  Recursive\n",
              "  6   0.960000  0.800000  0.872727  ...      neg       0.1  Recursive\n",
              "  7   0.736842  0.466667  0.571429  ...      neg       0.1  Recursive\n",
              "  8   0.682927  0.933333  0.788732  ...      neg       0.1  Recursive\n",
              "  9   0.677419  0.700000  0.688525  ...      neg       0.1  Recursive\n",
              "  \n",
              "  [20 rows x 8 columns]],\n",
              " [0.25,    precision    recall  f1-score  ...  pos/neg  Param(c)     method\n",
              "  0   0.532258  0.970588  0.687500  ...      pos      0.25  Recursive\n",
              "  1   0.596491  0.971429  0.739130  ...      pos      0.25  Recursive\n",
              "  2   0.711111  0.914286  0.800000  ...      pos      0.25  Recursive\n",
              "  3   0.634615  0.942857  0.758621  ...      pos      0.25  Recursive\n",
              "  4   0.711111  0.914286  0.800000  ...      pos      0.25  Recursive\n",
              "  5   0.744681  1.000000  0.853659  ...      pos      0.25  Recursive\n",
              "  6   0.829268  0.971429  0.894737  ...      pos      0.25  Recursive\n",
              "  7   0.666667  0.914286  0.771084  ...      pos      0.25  Recursive\n",
              "  8   0.884615  0.657143  0.754098  ...      pos      0.25  Recursive\n",
              "  9   0.750000  0.705882  0.727273  ...      pos      0.25  Recursive\n",
              "  0   0.666667  0.064516  0.117647  ...      neg      0.25  Recursive\n",
              "  1   0.875000  0.233333  0.368421  ...      neg      0.25  Recursive\n",
              "  2   0.850000  0.566667  0.680000  ...      neg      0.25  Recursive\n",
              "  3   0.846154  0.366667  0.511628  ...      neg      0.25  Recursive\n",
              "  4   0.850000  0.566667  0.680000  ...      neg      0.25  Recursive\n",
              "  5   1.000000  0.600000  0.750000  ...      neg      0.25  Recursive\n",
              "  6   0.958333  0.766667  0.851852  ...      neg      0.25  Recursive\n",
              "  7   0.823529  0.466667  0.595745  ...      neg      0.25  Recursive\n",
              "  8   0.692308  0.900000  0.782609  ...      neg      0.25  Recursive\n",
              "  9   0.687500  0.733333  0.709677  ...      neg      0.25  Recursive\n",
              "  \n",
              "  [20 rows x 8 columns]],\n",
              " [0.5,    precision    recall  f1-score  ...  pos/neg  Param(c)     method\n",
              "  0   0.539683  1.000000  0.701031  ...      pos       0.5  Recursive\n",
              "  1   0.576271  0.971429  0.723404  ...      pos       0.5  Recursive\n",
              "  2   0.673469  0.942857  0.785714  ...      pos       0.5  Recursive\n",
              "  3   0.611111  0.942857  0.741573  ...      pos       0.5  Recursive\n",
              "  4   0.708333  0.971429  0.819277  ...      pos       0.5  Recursive\n",
              "  5   0.744681  1.000000  0.853659  ...      pos       0.5  Recursive\n",
              "  6   0.833333  1.000000  0.909091  ...      pos       0.5  Recursive\n",
              "  7   0.666667  0.914286  0.771084  ...      pos       0.5  Recursive\n",
              "  8   0.827586  0.685714  0.750000  ...      pos       0.5  Recursive\n",
              "  9   0.727273  0.705882  0.716418  ...      pos       0.5  Recursive\n",
              "  0   1.000000  0.064516  0.121212  ...      neg       0.5  Recursive\n",
              "  1   0.833333  0.166667  0.277778  ...      neg       0.5  Recursive\n",
              "  2   0.875000  0.466667  0.608696  ...      neg       0.5  Recursive\n",
              "  3   0.818182  0.300000  0.439024  ...      neg       0.5  Recursive\n",
              "  4   0.941176  0.533333  0.680851  ...      neg       0.5  Recursive\n",
              "  5   1.000000  0.600000  0.750000  ...      neg       0.5  Recursive\n",
              "  6   1.000000  0.766667  0.867925  ...      neg       0.5  Recursive\n",
              "  7   0.823529  0.466667  0.595745  ...      neg       0.5  Recursive\n",
              "  8   0.694444  0.833333  0.757576  ...      neg       0.5  Recursive\n",
              "  9   0.677419  0.700000  0.688525  ...      neg       0.5  Recursive\n",
              "  \n",
              "  [20 rows x 8 columns]],\n",
              " [0.75,    precision    recall  f1-score  ...  pos/neg  Param(c)     method\n",
              "  0   0.539683  1.000000  0.701031  ...      pos      0.75  Recursive\n",
              "  1   0.576271  0.971429  0.723404  ...      pos      0.75  Recursive\n",
              "  2   0.666667  0.971429  0.790698  ...      pos      0.75  Recursive\n",
              "  3   0.611111  0.942857  0.741573  ...      pos      0.75  Recursive\n",
              "  4   0.693878  0.971429  0.809524  ...      pos      0.75  Recursive\n",
              "  5   0.729167  1.000000  0.843373  ...      pos      0.75  Recursive\n",
              "  6   0.833333  1.000000  0.909091  ...      pos      0.75  Recursive\n",
              "  7   0.653061  0.914286  0.761905  ...      pos      0.75  Recursive\n",
              "  8   0.806452  0.714286  0.757576  ...      pos      0.75  Recursive\n",
              "  9   0.705882  0.705882  0.705882  ...      pos      0.75  Recursive\n",
              "  0   1.000000  0.064516  0.121212  ...      neg      0.75  Recursive\n",
              "  1   0.833333  0.166667  0.277778  ...      neg      0.75  Recursive\n",
              "  2   0.928571  0.433333  0.590909  ...      neg      0.75  Recursive\n",
              "  3   0.818182  0.300000  0.439024  ...      neg      0.75  Recursive\n",
              "  4   0.937500  0.500000  0.652174  ...      neg      0.75  Recursive\n",
              "  5   1.000000  0.566667  0.723404  ...      neg      0.75  Recursive\n",
              "  6   1.000000  0.766667  0.867925  ...      neg      0.75  Recursive\n",
              "  7   0.812500  0.433333  0.565217  ...      neg      0.75  Recursive\n",
              "  8   0.705882  0.800000  0.750000  ...      neg      0.75  Recursive\n",
              "  9   0.666667  0.666667  0.666667  ...      neg      0.75  Recursive\n",
              "  \n",
              "  [20 rows x 8 columns]],\n",
              " [1,    precision    recall  f1-score  ...  pos/neg  Param(c)     method\n",
              "  0   0.539683  1.000000  0.701031  ...      pos         1  Recursive\n",
              "  1   0.576271  0.971429  0.723404  ...      pos         1  Recursive\n",
              "  2   0.660377  1.000000  0.795455  ...      pos         1  Recursive\n",
              "  3   0.600000  0.942857  0.733333  ...      pos         1  Recursive\n",
              "  4   0.666667  0.971429  0.790698  ...      pos         1  Recursive\n",
              "  5   0.729167  1.000000  0.843373  ...      pos         1  Recursive\n",
              "  6   0.813953  1.000000  0.897436  ...      pos         1  Recursive\n",
              "  7   0.640000  0.914286  0.752941  ...      pos         1  Recursive\n",
              "  8   0.794118  0.771429  0.782609  ...      pos         1  Recursive\n",
              "  9   0.685714  0.705882  0.695652  ...      pos         1  Recursive\n",
              "  0   1.000000  0.064516  0.121212  ...      neg         1  Recursive\n",
              "  1   0.833333  0.166667  0.277778  ...      neg         1  Recursive\n",
              "  2   1.000000  0.400000  0.571429  ...      neg         1  Recursive\n",
              "  3   0.800000  0.266667  0.400000  ...      neg         1  Recursive\n",
              "  4   0.928571  0.433333  0.590909  ...      neg         1  Recursive\n",
              "  5   1.000000  0.566667  0.723404  ...      neg         1  Recursive\n",
              "  6   1.000000  0.733333  0.846154  ...      neg         1  Recursive\n",
              "  7   0.800000  0.400000  0.533333  ...      neg         1  Recursive\n",
              "  8   0.741935  0.766667  0.754098  ...      neg         1  Recursive\n",
              "  9   0.655172  0.633333  0.644068  ...      neg         1  Recursive\n",
              "  \n",
              "  [20 rows x 8 columns]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdJkXuJPyVbe",
        "colab_type": "text"
      },
      "source": [
        "Calcula a média das medidas de cada parâmetro c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acLGXhWCyVbf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "5cf09920-9cfc-4e2c-fa60-fa5e2f53b745"
      },
      "source": [
        "result_recursive_mean = calcula_media(result_recursive)\n",
        "result_recursive_mean"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.764978</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.710783</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.731875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.761567</td>\n",
              "      <td>0.717764</td>\n",
              "      <td>0.701076</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.728774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250</td>\n",
              "      <td>0.765515</td>\n",
              "      <td>0.711335</td>\n",
              "      <td>0.691684</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.724183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.778575</td>\n",
              "      <td>0.701615</td>\n",
              "      <td>0.677929</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.716466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.775907</td>\n",
              "      <td>0.694472</td>\n",
              "      <td>0.669918</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.710288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.773248</td>\n",
              "      <td>0.685425</td>\n",
              "      <td>0.658916</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.702572</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "0  0.001        0.764978  ...         32.45       0.731875\n",
              "1  0.100        0.761567  ...         32.45       0.728774\n",
              "2  0.250        0.765515  ...         32.45       0.724183\n",
              "3  0.500        0.778575  ...         32.45       0.716466\n",
              "4  0.750        0.775907  ...         32.45       0.710288\n",
              "5  1.000        0.773248  ...         32.45       0.702572\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68ZPmfziyVbk",
        "colab_type": "text"
      },
      "source": [
        "Obtém o resultado da maior média de acurácia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2q7ZKnsyVbl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "5c8fb9ca-1286-418d-9619-1aff7f46503b"
      },
      "source": [
        "best_accuracy_recursive = pd.Series(result_recursive_mean.iloc[result_recursive_mean['accuracy_mean'].idxmax()], \n",
        "                          name='Recursive Feature')\n",
        "best_recursive = pd.DataFrame(best_accuracy_recursive)\n",
        "best_recursive"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Recursive Feature</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_mean</th>\n",
              "      <td>0.764978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_mean</th>\n",
              "      <td>0.722222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score_mean</th>\n",
              "      <td>0.710783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support_mean</th>\n",
              "      <td>32.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_mean</th>\n",
              "      <td>0.731875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Recursive Feature\n",
              "c                        0.001000\n",
              "precision_mean           0.764978\n",
              "recall_mean              0.722222\n",
              "f1_score_mean            0.710783\n",
              "support_mean            32.450000\n",
              "accuracy_mean            0.731875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zd_HCB-yVbr",
        "colab_type": "text"
      },
      "source": [
        "# Junta todos os resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdvIGYH5yVbs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "29bf3491-53b2-41b7-83a7-9b5e23c2ff35"
      },
      "source": [
        "result = pd.concat([best_all_features, best_pca, best_chi, best_recursive], axis=1)\n",
        "\n",
        "print(\"Média das métricas geradas pelo processamento de cada dataset\")\n",
        "result.transpose()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Média das métricas geradas pelo processamento de cada dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>All Features</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.739483</td>\n",
              "      <td>0.710530</td>\n",
              "      <td>0.697445</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.714880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PCA</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.739416</td>\n",
              "      <td>0.673532</td>\n",
              "      <td>0.643785</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.687187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Chi Squared</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.769488</td>\n",
              "      <td>0.736048</td>\n",
              "      <td>0.724163</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.741058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Recursive Feature</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.764978</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.710783</td>\n",
              "      <td>32.45</td>\n",
              "      <td>0.731875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "All Features       0.100        0.739483  ...         32.45       0.714880\n",
              "PCA                0.500        0.739416  ...         32.45       0.687187\n",
              "Chi Squared        0.001        0.769488  ...         32.45       0.741058\n",
              "Recursive Feature  0.001        0.764978  ...         32.45       0.731875\n",
              "\n",
              "[4 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nPWiXY29cEu",
        "colab_type": "text"
      },
      "source": [
        "# Gerar Graficos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFhU-Fp2yVbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79e5c076-435d-444f-f6bc-6f70a4557f56"
      },
      "source": [
        "frames = result_all_features_g +  result_pca_g + result_chi_g + result_recursive_g\n",
        "\n",
        "tips = pd.concat(frames)\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "fig, ax = plt.subplots(3,1,figsize=[10,25])\n",
        "\n",
        "g = sns.barplot(ax=ax[0], x=\"method\", y=\"accuracy\", hue=\"Param(c)\", data=tips,capsize=.05)\n",
        "g.set(ylim=(0,1),yticks=np.arange(0,1.1,0.1).tolist())\n",
        "g.set_ylabel(\"Accuracy\",fontsize=30)\n",
        "g.set_title('Classificador Naive Bayes',fontsize=30)\n",
        "g = sns.barplot(ax=ax[1], x=\"method\", y=\"recall\", hue=\"Param(c)\", data=tips,capsize=.05);\n",
        "g.set(ylim=(0,1),yticks=np.arange(0,1.1,0.1).tolist())\n",
        "g.set_ylabel(\"Recall\",fontsize=30)\n",
        "g = sns.barplot(ax=ax[2], x=\"method\", y=\"precision\", hue=\"Param(c)\", data=tips,capsize=.05);\n",
        "g.set(ylim=(0,1),yticks=np.arange(0,1.1,0.1).tolist())\n",
        "g.set_ylabel(\"Precision\",fontsize=30)\n",
        "plt.close(2)\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x1800 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}