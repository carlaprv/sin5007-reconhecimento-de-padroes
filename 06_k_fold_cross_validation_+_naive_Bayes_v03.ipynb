{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of 06 - k-fold cross validation + naive Bayes v03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlaprv/sin5007-reconhecimento-de-padroes/blob/master/06_k_fold_cross_validation_%2B_naive_Bayes_v03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikayTX9-yVZk",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "-------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiZnnw89yVZm",
        "colab_type": "text"
      },
      "source": [
        "# Bibliotecas Necessárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xYCeWYlyVZn",
        "colab_type": "code",
        "outputId": "bd5b4b27-fbea-4bb0-9ab9-cf367c2c975d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np\n",
        "import seaborn as sns # visualize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "from numpy import mean"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRJncBBWyVZt",
        "colab_type": "text"
      },
      "source": [
        "# Funções Auxiliares\n",
        "\n",
        "describe_dataset() : realiza o cálculo das proporções de classes do dataset original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KutB_XTYyVZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def describe_dataset(X, y, k):\n",
        "    # get dataset rows: instances , columns: features\n",
        "    rows, columns = X.shape\n",
        "    # get proportion from target\n",
        "    (unique, counts) = np.unique(y, return_counts=True) \n",
        "    # calculate proportion\n",
        "    prop_neg = int(counts[0]/rows*100)\n",
        "    prop_pos = int(counts[1]/rows*100)\n",
        "\n",
        "    print(\"k = {}, Dataset: {} positivas, {} negativas ({}% x {}%)\".format(k, counts[1], counts[0], prop_pos, prop_neg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z4wMlsnyVZz",
        "colab_type": "text"
      },
      "source": [
        "get_classes_from_index() : realiza o cálculo das proporções de classes dos folds criados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEZ4jxJKyVZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classes_from_index(y, skf):\n",
        "    _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)\n",
        "    y_counts = np.bincount(y_inv)\n",
        "    _, class_perm = np.unique(y_idx, return_inverse=True)\n",
        "    y_encoded = class_perm[y_inv]\n",
        "    y_order = np.sort(y_encoded)\n",
        "    n_classes = len(y_idx)\n",
        "    allocation = np.asarray(\n",
        "            [np.bincount(y_order[i::skf.n_splits], minlength=n_classes)\n",
        "             for i in range(skf.n_splits)])\n",
        "\n",
        "    for idx, f in enumerate(allocation):\n",
        "        count_neg = int(f[0])\n",
        "        count_pos = int(f[1])\n",
        "        total = count_neg+count_pos\n",
        "        prop_temp_neg = int(count_neg/total*100)\n",
        "        prop_temp_pos = int(count_pos/total*100)\n",
        "        print(\"Fold {}: Pos: {}, Neg: {}, Total: {}, Proporção: {}% x {}%\".format(idx, count_pos, count_neg, total, prop_temp_pos, prop_temp_neg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RsPfCv4yVZ4",
        "colab_type": "text"
      },
      "source": [
        "# Função que aplica o Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YezZQsp7yVZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stratified_k_fold(X, y, list_c, k, name):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------    \n",
        "       X : array-like, shape (n_samples, n_features)\n",
        "           Training data, where n_samples is the number of samples\n",
        "           and n_features is the number of features.\n",
        "       y : array-like, of length n_samples\n",
        "           The target variable for supervised learning problems.\n",
        "       k : int\n",
        "           Determines the number of folds.\n",
        "    name : method selection (string)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### Estratifica o dataset em k folds\n",
        "    skf = StratifiedKFold(n_splits=k)\n",
        "    describe_dataset(X, y, k)\n",
        "    get_classes_from_index(y, skf)\n",
        "    \n",
        "    \n",
        "    ### Lista para armazenar os resultados de cada valor de c\n",
        "    ### Armazena um array bidimensional, onde terá o valor do c e uma lista dos resultados de c\n",
        "    result = []\n",
        "    reports_g = []\n",
        "    \n",
        "    \n",
        "    ### Executa o treino e teste para cada valor do parametro c\n",
        "    for c in list_c:\n",
        "        print(\"c =  {}\" .format(c))\n",
        "\n",
        "        ### create naive bayes classifier\n",
        "        clf = GaussianNB(var_smoothing = c)\n",
        "        \n",
        "        \n",
        "        ### Array para guardar os resultados dos testes para o parametro c\n",
        "        \"\"\"\n",
        "        Coluna 0 : Armazena o valor de c\n",
        "        Coluna 1 : Armazena o resultado \n",
        "        \"\"\"\n",
        "        result_c = []\n",
        "\n",
        "                \n",
        "        ### resultado do fold-k\n",
        "        result_k = []\n",
        "        ### Executa o treino e teste para k folds\n",
        "        fold_k = 1\n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            \n",
        "            print(\"fold_k: {}\" .format(fold_k))\n",
        "            print(\"\\nTRAIN: {}  TEST: {}\".format(len(train_index), len(test_index)))\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "            ### train classifier\n",
        "            clf.fit(X_train, y_train)\n",
        "            \n",
        "            ### calculate metrics\n",
        "            y_predicted = clf.predict(X_test)\n",
        "            report_dict = metrics.classification_report(y_test, y_predicted, output_dict=True)\n",
        "            report_str = metrics.classification_report(y_test, y_predicted)\n",
        "                \n",
        "            ### Armazena o resultado do test do fold-k          \n",
        "            result_k.append(report_dict)\n",
        "            print(report_str)\n",
        "            \n",
        "            fold_k = fold_k + 1\n",
        "            \n",
        "\n",
        "        ### Guarda os resultados dos k fold do parametro c\n",
        "        reports = pd.DataFrame(pd.DataFrame(result_k)['1.0'].to_list())\n",
        "        \n",
        "        accuracy_reports = pd.DataFrame(pd.DataFrame(result_k)['accuracy'])\n",
        "        reports['accuracy'] = accuracy_reports\n",
        "        print(reports)\n",
        "        reports_c = reports\n",
        "        reports_c['Param(c)'] = c\n",
        "        reports_c['method'] = str(name)\n",
        "        reports_g.append(reports_c)\n",
        "                \n",
        "        ### Guarda o resultado da execução para o parâmetro c\n",
        "        result_c = [c, reports]\n",
        "        result.append(result_c)\n",
        "        \n",
        "    \n",
        "    ### Retorna a lista com todos os resultado para cada c\n",
        "    return result , reports_g\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ14ict2yVZ-",
        "colab_type": "text"
      },
      "source": [
        "# Função para calcular a média das medidas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HpefPV4yVZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calcula a média das medidas de cada c\n",
        "def calcula_media(lista_result):\n",
        "    \n",
        "    mean_c = []\n",
        "    for result in lista_result:\n",
        "        \n",
        "        c = result[0]\n",
        "        result_c = result[1]\n",
        "        \n",
        "        # Calcula a média das medidas do parametro c\n",
        "        precision_mean = result_c['precision'].mean()\n",
        "        recall_mean = result_c['recall'].mean()\n",
        "        f1_score_mean = result_c['f1-score'].mean()\n",
        "        support_mean = result_c['support'].mean()\n",
        "        accuracy_mean = result_c['accuracy'].mean()\n",
        "        \n",
        "        # Armazena a média das medidas do parametro c\n",
        "        mean_c.append([c, precision_mean, recall_mean, f1_score_mean, support_mean, accuracy_mean])\n",
        "    \n",
        "    name_columns = ['c', 'precision_mean', 'recall_mean', 'f1_score_mean', 'support_mean', 'accuracy_mean']\n",
        "    mean_c = pd.DataFrame(mean_c, columns=name_columns)\n",
        "    return mean_c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OefDdR7WyVaD",
        "colab_type": "text"
      },
      "source": [
        "##### Parâmetros de execução do Naive Bayes\n",
        "list_c : valores do parâmetro de ajuste de probabilidade \n",
        "\n",
        "k_folds : número de folds para a estratificação do dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mss4hXMgyVaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_c = [0.001, 0.10, 0.25, 0.50, 0.75, 1]\n",
        "k_folds = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCMHkna8yVaJ",
        "colab_type": "text"
      },
      "source": [
        "# Execução base: Todas as características"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSyYoDy_yVaK",
        "colab_type": "code",
        "outputId": "2d55ee06-ee63-4bd8-bac6-aef79a14a069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df = pd.read_csv('results/dataset-normalizado.csv', header = 0)\n",
        "X = df.drop('is_approved', axis=1).to_numpy() # DATASET\n",
        "y = df['is_approved'].to_numpy() # target\n",
        "result_all_features , result_all_features_g = stratified_k_fold(X, y, list_c, k=k_folds, name='All Features')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 10, Dataset: 348 positivas, 301 negativas (53% x 46%)\n",
            "Fold 0: Pos: 34, Neg: 31, Total: 65, Proporção: 52% x 47%\n",
            "Fold 1: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 2: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 3: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 4: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 5: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 6: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 7: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 8: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 9: Pos: 34, Neg: 30, Total: 64, Proporção: 53% x 46%\n",
            "c =  0.001\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.13      0.22        31\n",
            "         1.0       0.55      0.97      0.70        34\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.68      0.55      0.46        65\n",
            "weighted avg       0.67      0.57      0.47        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.40      0.51        30\n",
            "         1.0       0.62      0.86      0.72        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.67      0.63      0.62        65\n",
            "weighted avg       0.66      0.65      0.62        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.70      0.74        30\n",
            "         1.0       0.76      0.83      0.79        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.77      0.76      0.77        65\n",
            "weighted avg       0.77      0.77      0.77        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.53      0.60        30\n",
            "         1.0       0.67      0.80      0.73        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.63      0.68        30\n",
            "         1.0       0.72      0.80      0.76        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.72      0.72      0.72        65\n",
            "weighted avg       0.72      0.72      0.72        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.70      0.76        30\n",
            "         1.0       0.78      0.89      0.83        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.81      0.79      0.80        65\n",
            "weighted avg       0.81      0.80      0.80        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.80      0.86        30\n",
            "         1.0       0.85      0.94      0.89        35\n",
            "\n",
            "    accuracy                           0.88        65\n",
            "   macro avg       0.88      0.87      0.87        65\n",
            "weighted avg       0.88      0.88      0.88        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.63      0.63        30\n",
            "         1.0       0.69      0.69      0.69        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.66      0.66      0.66        65\n",
            "weighted avg       0.66      0.66      0.66        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.93      0.73        30\n",
            "         1.0       0.89      0.46      0.60        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.74      0.70      0.67        65\n",
            "weighted avg       0.75      0.68      0.66        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.77      0.68        30\n",
            "         1.0       0.73      0.56      0.63        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.67      0.66      0.65        64\n",
            "weighted avg       0.67      0.66      0.65        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.550000  0.970588  0.702128       34  0.569231\n",
            "1   0.625000  0.857143  0.722892       35  0.646154\n",
            "2   0.763158  0.828571  0.794521       35  0.769231\n",
            "3   0.666667  0.800000  0.727273       35  0.676923\n",
            "4   0.717949  0.800000  0.756757       35  0.723077\n",
            "5   0.775000  0.885714  0.826667       35  0.800000\n",
            "6   0.846154  0.942857  0.891892       35  0.876923\n",
            "7   0.685714  0.685714  0.685714       35  0.661538\n",
            "8   0.888889  0.457143  0.603774       35  0.676923\n",
            "9   0.730769  0.558824  0.633333       34  0.656250\n",
            "c =  0.1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.16      0.27        31\n",
            "         1.0       0.56      0.97      0.71        34\n",
            "\n",
            "    accuracy                           0.58        65\n",
            "   macro avg       0.70      0.57      0.49        65\n",
            "weighted avg       0.69      0.58      0.50        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.40      0.53        30\n",
            "         1.0       0.64      0.91      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.72      0.66      0.64        65\n",
            "weighted avg       0.71      0.68      0.65        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.67      0.71        30\n",
            "         1.0       0.74      0.83      0.78        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67        30\n",
            "         1.0       0.71      0.77      0.74        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.71        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.80      0.81        30\n",
            "         1.0       0.83      0.86      0.85        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.83      0.83      0.83        65\n",
            "weighted avg       0.83      0.83      0.83        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.87      0.90        30\n",
            "         1.0       0.89      0.94      0.92        35\n",
            "\n",
            "    accuracy                           0.91        65\n",
            "   macro avg       0.91      0.90      0.91        65\n",
            "weighted avg       0.91      0.91      0.91        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.70      0.68        30\n",
            "         1.0       0.73      0.69      0.71        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.69      0.69      0.69        65\n",
            "weighted avg       0.69      0.69      0.69        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.90      0.69        30\n",
            "         1.0       0.82      0.40      0.54        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.69      0.65      0.62        65\n",
            "weighted avg       0.70      0.63      0.61        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.77      0.69        30\n",
            "         1.0       0.74      0.59      0.66        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.68      0.68      0.67        64\n",
            "weighted avg       0.68      0.67      0.67        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.559322  0.970588  0.709677       34  0.584615\n",
            "1   0.640000  0.914286  0.752941       35  0.676923\n",
            "2   0.743590  0.828571  0.783784       35  0.753846\n",
            "3   0.666667  0.857143  0.750000       35  0.692308\n",
            "4   0.710526  0.771429  0.739726       35  0.707692\n",
            "5   0.833333  0.857143  0.845070       35  0.830769\n",
            "6   0.891892  0.942857  0.916667       35  0.907692\n",
            "7   0.727273  0.685714  0.705882       35  0.692308\n",
            "8   0.823529  0.400000  0.538462       35  0.630769\n",
            "9   0.740741  0.588235  0.655738       34  0.671875\n",
            "c =  0.25\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.13      0.22        31\n",
            "         1.0       0.55      0.97      0.70        34\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.68      0.55      0.46        65\n",
            "weighted avg       0.67      0.57      0.47        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.37      0.51        30\n",
            "         1.0       0.63      0.94      0.76        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.74      0.65      0.64        65\n",
            "weighted avg       0.73      0.68      0.64        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.67      0.73        30\n",
            "         1.0       0.75      0.86      0.80        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.78      0.76      0.76        65\n",
            "weighted avg       0.77      0.77      0.77        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.60      0.63        30\n",
            "         1.0       0.68      0.74      0.71        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.68        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.80      0.83        30\n",
            "         1.0       0.84      0.89      0.86        35\n",
            "\n",
            "    accuracy                           0.85        65\n",
            "   macro avg       0.85      0.84      0.84        65\n",
            "weighted avg       0.85      0.85      0.85        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.83      0.88        30\n",
            "         1.0       0.87      0.94      0.90        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.90      0.89      0.89        65\n",
            "weighted avg       0.89      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.67      0.69        30\n",
            "         1.0       0.73      0.77      0.75        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.72      0.72      0.72        65\n",
            "weighted avg       0.72      0.72      0.72        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.55      0.90      0.68        30\n",
            "         1.0       0.81      0.37      0.51        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.68      0.64      0.60        65\n",
            "weighted avg       0.69      0.62      0.59        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.73      0.67        30\n",
            "         1.0       0.71      0.59      0.65        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.67      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.550000  0.970588  0.702128       34  0.569231\n",
            "1   0.634615  0.942857  0.758621       35  0.676923\n",
            "2   0.750000  0.857143  0.800000       35  0.769231\n",
            "3   0.666667  0.857143  0.750000       35  0.692308\n",
            "4   0.684211  0.742857  0.712329       35  0.676923\n",
            "5   0.837838  0.885714  0.861111       35  0.846154\n",
            "6   0.868421  0.942857  0.904110       35  0.892308\n",
            "7   0.729730  0.771429  0.750000       35  0.723077\n",
            "8   0.812500  0.371429  0.509804       35  0.615385\n",
            "9   0.714286  0.588235  0.645161       34  0.656250\n",
            "c =  0.5\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.13      0.22        31\n",
            "         1.0       0.55      0.97      0.70        34\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.68      0.55      0.46        65\n",
            "weighted avg       0.67      0.57      0.47        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.30      0.45        30\n",
            "         1.0       0.62      0.97      0.76        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.76      0.64      0.60        65\n",
            "weighted avg       0.75      0.66      0.61        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.63      0.70        30\n",
            "         1.0       0.73      0.86      0.79        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.43      0.54        30\n",
            "         1.0       0.64      0.86      0.73        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.68      0.65      0.64        65\n",
            "weighted avg       0.68      0.66      0.64        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.57      0.62        30\n",
            "         1.0       0.68      0.77      0.72        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.80      0.84        30\n",
            "         1.0       0.84      0.91      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.86      0.86        65\n",
            "weighted avg       0.86      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.87      0.90        30\n",
            "         1.0       0.89      0.94      0.92        35\n",
            "\n",
            "    accuracy                           0.91        65\n",
            "   macro avg       0.91      0.90      0.91        65\n",
            "weighted avg       0.91      0.91      0.91        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.60      0.64        30\n",
            "         1.0       0.69      0.77      0.73        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.69      0.69      0.69        65\n",
            "weighted avg       0.69      0.69      0.69        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.55      0.87      0.68        30\n",
            "         1.0       0.78      0.40      0.53        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.67      0.63      0.60        65\n",
            "weighted avg       0.67      0.62      0.60        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.550000  0.970588  0.702128       34  0.569231\n",
            "1   0.618182  0.971429  0.755556       35  0.661538\n",
            "2   0.731707  0.857143  0.789474       35  0.753846\n",
            "3   0.638298  0.857143  0.731707       35  0.661538\n",
            "4   0.675000  0.771429  0.720000       35  0.676923\n",
            "5   0.842105  0.914286  0.876712       35  0.861538\n",
            "6   0.891892  0.942857  0.916667       35  0.907692\n",
            "7   0.692308  0.771429  0.729730       35  0.692308\n",
            "8   0.777778  0.400000  0.528302       35  0.615385\n",
            "9   0.700000  0.617647  0.656250       34  0.656250\n",
            "c =  0.75\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.13      0.22        31\n",
            "         1.0       0.55      0.97      0.70        34\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.68      0.55      0.46        65\n",
            "weighted avg       0.67      0.57      0.47        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.23      0.37        30\n",
            "         1.0       0.60      0.97      0.74        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.74      0.60      0.55        65\n",
            "weighted avg       0.73      0.63      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.63      0.70        30\n",
            "         1.0       0.73      0.86      0.79        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.37      0.48        30\n",
            "         1.0       0.61      0.86      0.71        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.65      0.61      0.60        65\n",
            "weighted avg       0.65      0.63      0.61        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.57      0.62        30\n",
            "         1.0       0.68      0.77      0.72        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.80      0.87        30\n",
            "         1.0       0.85      0.97      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.91      0.89      0.89        65\n",
            "weighted avg       0.90      0.89      0.89        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.90      0.93        30\n",
            "         1.0       0.92      0.97      0.94        35\n",
            "\n",
            "    accuracy                           0.94        65\n",
            "   macro avg       0.94      0.94      0.94        65\n",
            "weighted avg       0.94      0.94      0.94        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.60      0.65        30\n",
            "         1.0       0.70      0.80      0.75        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.70        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.58      0.87      0.69        30\n",
            "         1.0       0.80      0.46      0.58        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.69      0.66      0.64        65\n",
            "weighted avg       0.70      0.65      0.63        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.550000  0.970588  0.702128       34  0.569231\n",
            "1   0.596491  0.971429  0.739130       35  0.630769\n",
            "2   0.731707  0.857143  0.789474       35  0.753846\n",
            "3   0.612245  0.857143  0.714286       35  0.630769\n",
            "4   0.675000  0.771429  0.720000       35  0.676923\n",
            "5   0.850000  0.971429  0.906667       35  0.892308\n",
            "6   0.918919  0.971429  0.944444       35  0.938462\n",
            "7   0.700000  0.800000  0.746667       35  0.707692\n",
            "8   0.800000  0.457143  0.581818       35  0.646154\n",
            "9   0.700000  0.617647  0.656250       34  0.656250\n",
            "c =  1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.23      0.37        30\n",
            "         1.0       0.60      0.97      0.74        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.74      0.60      0.55        65\n",
            "weighted avg       0.73      0.63      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.60      0.68        30\n",
            "         1.0       0.71      0.86      0.78        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.75      0.73      0.73        65\n",
            "weighted avg       0.75      0.74      0.73        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.33      0.44        30\n",
            "         1.0       0.60      0.86      0.71        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.63      0.60      0.58        65\n",
            "weighted avg       0.63      0.62      0.59        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.53      0.60        30\n",
            "         1.0       0.67      0.80      0.73        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.77      0.85        30\n",
            "         1.0       0.83      0.97      0.89        35\n",
            "\n",
            "    accuracy                           0.88        65\n",
            "   macro avg       0.89      0.87      0.87        65\n",
            "weighted avg       0.89      0.88      0.87        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.87      0.91        30\n",
            "         1.0       0.89      0.97      0.93        35\n",
            "\n",
            "    accuracy                           0.92        65\n",
            "   macro avg       0.93      0.92      0.92        65\n",
            "weighted avg       0.93      0.92      0.92        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.53      0.62        30\n",
            "         1.0       0.67      0.83      0.74        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.70      0.68      0.68        65\n",
            "weighted avg       0.70      0.69      0.68        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.87      0.72        30\n",
            "         1.0       0.83      0.54      0.66        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.72      0.70      0.69        65\n",
            "weighted avg       0.73      0.69      0.69        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.540984  0.970588  0.694737       34  0.553846\n",
            "1   0.596491  0.971429  0.739130       35  0.630769\n",
            "2   0.714286  0.857143  0.779221       35  0.738462\n",
            "3   0.600000  0.857143  0.705882       35  0.615385\n",
            "4   0.666667  0.800000  0.727273       35  0.676923\n",
            "5   0.829268  0.971429  0.894737       35  0.876923\n",
            "6   0.894737  0.971429  0.931507       35  0.923077\n",
            "7   0.674419  0.828571  0.743590       35  0.692308\n",
            "8   0.826087  0.542857  0.655172       35  0.692308\n",
            "9   0.700000  0.617647  0.656250       34  0.656250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AymSdHvyyVaQ",
        "colab_type": "text"
      },
      "source": [
        "Resultado dos k-fold para cada valor de c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-NvZLvfyVaR",
        "colab_type": "code",
        "outputId": "f3e6617a-2a50-4970-ea78-7f87d24e76af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "result_all_features"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)        method\n",
              "  0   0.550000  0.970588  0.702128       34  0.569231     0.001  All Features\n",
              "  1   0.625000  0.857143  0.722892       35  0.646154     0.001  All Features\n",
              "  2   0.763158  0.828571  0.794521       35  0.769231     0.001  All Features\n",
              "  3   0.666667  0.800000  0.727273       35  0.676923     0.001  All Features\n",
              "  4   0.717949  0.800000  0.756757       35  0.723077     0.001  All Features\n",
              "  5   0.775000  0.885714  0.826667       35  0.800000     0.001  All Features\n",
              "  6   0.846154  0.942857  0.891892       35  0.876923     0.001  All Features\n",
              "  7   0.685714  0.685714  0.685714       35  0.661538     0.001  All Features\n",
              "  8   0.888889  0.457143  0.603774       35  0.676923     0.001  All Features\n",
              "  9   0.730769  0.558824  0.633333       34  0.656250     0.001  All Features],\n",
              " [0.1,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)        method\n",
              "  0   0.559322  0.970588  0.709677       34  0.584615       0.1  All Features\n",
              "  1   0.640000  0.914286  0.752941       35  0.676923       0.1  All Features\n",
              "  2   0.743590  0.828571  0.783784       35  0.753846       0.1  All Features\n",
              "  3   0.666667  0.857143  0.750000       35  0.692308       0.1  All Features\n",
              "  4   0.710526  0.771429  0.739726       35  0.707692       0.1  All Features\n",
              "  5   0.833333  0.857143  0.845070       35  0.830769       0.1  All Features\n",
              "  6   0.891892  0.942857  0.916667       35  0.907692       0.1  All Features\n",
              "  7   0.727273  0.685714  0.705882       35  0.692308       0.1  All Features\n",
              "  8   0.823529  0.400000  0.538462       35  0.630769       0.1  All Features\n",
              "  9   0.740741  0.588235  0.655738       34  0.671875       0.1  All Features],\n",
              " [0.25,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)        method\n",
              "  0   0.550000  0.970588  0.702128       34  0.569231      0.25  All Features\n",
              "  1   0.634615  0.942857  0.758621       35  0.676923      0.25  All Features\n",
              "  2   0.750000  0.857143  0.800000       35  0.769231      0.25  All Features\n",
              "  3   0.666667  0.857143  0.750000       35  0.692308      0.25  All Features\n",
              "  4   0.684211  0.742857  0.712329       35  0.676923      0.25  All Features\n",
              "  5   0.837838  0.885714  0.861111       35  0.846154      0.25  All Features\n",
              "  6   0.868421  0.942857  0.904110       35  0.892308      0.25  All Features\n",
              "  7   0.729730  0.771429  0.750000       35  0.723077      0.25  All Features\n",
              "  8   0.812500  0.371429  0.509804       35  0.615385      0.25  All Features\n",
              "  9   0.714286  0.588235  0.645161       34  0.656250      0.25  All Features],\n",
              " [0.5,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)        method\n",
              "  0   0.550000  0.970588  0.702128       34  0.569231       0.5  All Features\n",
              "  1   0.618182  0.971429  0.755556       35  0.661538       0.5  All Features\n",
              "  2   0.731707  0.857143  0.789474       35  0.753846       0.5  All Features\n",
              "  3   0.638298  0.857143  0.731707       35  0.661538       0.5  All Features\n",
              "  4   0.675000  0.771429  0.720000       35  0.676923       0.5  All Features\n",
              "  5   0.842105  0.914286  0.876712       35  0.861538       0.5  All Features\n",
              "  6   0.891892  0.942857  0.916667       35  0.907692       0.5  All Features\n",
              "  7   0.692308  0.771429  0.729730       35  0.692308       0.5  All Features\n",
              "  8   0.777778  0.400000  0.528302       35  0.615385       0.5  All Features\n",
              "  9   0.700000  0.617647  0.656250       34  0.656250       0.5  All Features],\n",
              " [0.75,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)        method\n",
              "  0   0.550000  0.970588  0.702128       34  0.569231      0.75  All Features\n",
              "  1   0.596491  0.971429  0.739130       35  0.630769      0.75  All Features\n",
              "  2   0.731707  0.857143  0.789474       35  0.753846      0.75  All Features\n",
              "  3   0.612245  0.857143  0.714286       35  0.630769      0.75  All Features\n",
              "  4   0.675000  0.771429  0.720000       35  0.676923      0.75  All Features\n",
              "  5   0.850000  0.971429  0.906667       35  0.892308      0.75  All Features\n",
              "  6   0.918919  0.971429  0.944444       35  0.938462      0.75  All Features\n",
              "  7   0.700000  0.800000  0.746667       35  0.707692      0.75  All Features\n",
              "  8   0.800000  0.457143  0.581818       35  0.646154      0.75  All Features\n",
              "  9   0.700000  0.617647  0.656250       34  0.656250      0.75  All Features],\n",
              " [1,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)        method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846         1  All Features\n",
              "  1   0.596491  0.971429  0.739130       35  0.630769         1  All Features\n",
              "  2   0.714286  0.857143  0.779221       35  0.738462         1  All Features\n",
              "  3   0.600000  0.857143  0.705882       35  0.615385         1  All Features\n",
              "  4   0.666667  0.800000  0.727273       35  0.676923         1  All Features\n",
              "  5   0.829268  0.971429  0.894737       35  0.876923         1  All Features\n",
              "  6   0.894737  0.971429  0.931507       35  0.923077         1  All Features\n",
              "  7   0.674419  0.828571  0.743590       35  0.692308         1  All Features\n",
              "  8   0.826087  0.542857  0.655172       35  0.692308         1  All Features\n",
              "  9   0.700000  0.617647  0.656250       34  0.656250         1  All Features]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEf_UxQYyVaX",
        "colab_type": "text"
      },
      "source": [
        "Calcula a média das medidas de cada parâmetro c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GPD7fz5yVaY",
        "colab_type": "code",
        "outputId": "7f886696-bc4d-4e5b-d326-8a37c2f46e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "result_all_features_mean = calcula_media(result_all_features)\n",
        "result_all_features_mean"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.724930</td>\n",
              "      <td>0.778655</td>\n",
              "      <td>0.734495</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.705625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.733687</td>\n",
              "      <td>0.781597</td>\n",
              "      <td>0.739795</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.714880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250</td>\n",
              "      <td>0.724827</td>\n",
              "      <td>0.793025</td>\n",
              "      <td>0.739326</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.711779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.711727</td>\n",
              "      <td>0.807395</td>\n",
              "      <td>0.740652</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.705625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.713436</td>\n",
              "      <td>0.824538</td>\n",
              "      <td>0.750086</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.710240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.704294</td>\n",
              "      <td>0.838824</td>\n",
              "      <td>0.752750</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.705625</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "0  0.001        0.724930  ...          34.8       0.705625\n",
              "1  0.100        0.733687  ...          34.8       0.714880\n",
              "2  0.250        0.724827  ...          34.8       0.711779\n",
              "3  0.500        0.711727  ...          34.8       0.705625\n",
              "4  0.750        0.713436  ...          34.8       0.710240\n",
              "5  1.000        0.704294  ...          34.8       0.705625\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elz06yqgyVad",
        "colab_type": "text"
      },
      "source": [
        "Obtém as medidas da maior média de acurácia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPHbgy6ByVae",
        "colab_type": "code",
        "outputId": "a7196248-0b0c-4ea2-d7f5-76d03785f4b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "best_accuracy_all_features = pd.Series(result_all_features_mean.iloc[result_all_features_mean['accuracy_mean'].idxmax()], \n",
        "                          name='All Features')\n",
        "best_all_features = pd.DataFrame(best_accuracy_all_features)\n",
        "best_all_features"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>All Features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_mean</th>\n",
              "      <td>0.733687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_mean</th>\n",
              "      <td>0.781597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score_mean</th>\n",
              "      <td>0.739795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support_mean</th>\n",
              "      <td>34.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_mean</th>\n",
              "      <td>0.714880</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                All Features\n",
              "c                   0.100000\n",
              "precision_mean      0.733687\n",
              "recall_mean         0.781597\n",
              "f1_score_mean       0.739795\n",
              "support_mean       34.800000\n",
              "accuracy_mean       0.714880"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXEKLi39yVaj",
        "colab_type": "text"
      },
      "source": [
        "# Execução Base: PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiTGHSiMyVak",
        "colab_type": "code",
        "outputId": "e3d9c7ec-7e47-43b3-fed5-1b5bb9414652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df = pd.read_csv('results/dataset-pca.csv', header = 0)\n",
        "X = df.drop('is_approved', axis=1).to_numpy() # DATASET\n",
        "y = df['is_approved'].to_numpy() # target\n",
        "result_pca , result_pca_g = stratified_k_fold(X, y, list_c, k=k_folds, name='PCA')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 10, Dataset: 348 positivas, 301 negativas (53% x 46%)\n",
            "Fold 0: Pos: 34, Neg: 31, Total: 65, Proporção: 52% x 47%\n",
            "Fold 1: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 2: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 3: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 4: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 5: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 6: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 7: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 8: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 9: Pos: 34, Neg: 30, Total: 64, Proporção: 53% x 46%\n",
            "c =  0.001\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.16      0.28        31\n",
            "         1.0       0.57      1.00      0.72        34\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.78      0.58      0.50        65\n",
            "weighted avg       0.77      0.60      0.51        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.33      0.47        30\n",
            "         1.0       0.62      0.91      0.74        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.69      0.62      0.60        65\n",
            "weighted avg       0.69      0.65      0.61        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.77      0.79        30\n",
            "         1.0       0.81      0.86      0.83        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.82      0.81      0.81        65\n",
            "weighted avg       0.82      0.82      0.81        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.40      0.50        30\n",
            "         1.0       0.62      0.83      0.71        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.64      0.61      0.60        65\n",
            "weighted avg       0.64      0.63      0.61        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.60      0.61        30\n",
            "         1.0       0.67      0.69      0.68        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.64      0.64      0.64        65\n",
            "weighted avg       0.65      0.65      0.65        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.80      0.79        30\n",
            "         1.0       0.82      0.80      0.81        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.80      0.80      0.80        65\n",
            "weighted avg       0.80      0.80      0.80        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.80      0.80        30\n",
            "         1.0       0.83      0.83      0.83        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.81      0.81      0.81        65\n",
            "weighted avg       0.82      0.82      0.82        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.73      0.64        30\n",
            "         1.0       0.69      0.51      0.59        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.63      0.62      0.61        65\n",
            "weighted avg       0.63      0.62      0.61        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.57      0.97      0.72        30\n",
            "         1.0       0.93      0.37      0.53        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.75      0.67      0.62        65\n",
            "weighted avg       0.76      0.65      0.62        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.83      0.67        30\n",
            "         1.0       0.74      0.41      0.53        34\n",
            "\n",
            "    accuracy                           0.61        64\n",
            "   macro avg       0.65      0.62      0.60        64\n",
            "weighted avg       0.65      0.61      0.59        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.566667  1.000000  0.723404       34  0.600000\n",
            "1   0.615385  0.914286  0.735632       35  0.646154\n",
            "2   0.810811  0.857143  0.833333       35  0.815385\n",
            "3   0.617021  0.828571  0.707317       35  0.630769\n",
            "4   0.666667  0.685714  0.676056       35  0.646154\n",
            "5   0.823529  0.800000  0.811594       35  0.800000\n",
            "6   0.828571  0.828571  0.828571       35  0.815385\n",
            "7   0.692308  0.514286  0.590164       35  0.615385\n",
            "8   0.928571  0.371429  0.530612       35  0.646154\n",
            "9   0.736842  0.411765  0.528302       34  0.609375\n",
            "c =  0.1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.06      0.12        31\n",
            "         1.0       0.54      1.00      0.70        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.77      0.53      0.41        65\n",
            "weighted avg       0.76      0.55      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.23      0.36        30\n",
            "         1.0       0.59      0.94      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.68      0.59      0.54        65\n",
            "weighted avg       0.68      0.62      0.56        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.67      0.75        30\n",
            "         1.0       0.76      0.91      0.83        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.82      0.79      0.79        65\n",
            "weighted avg       0.81      0.80      0.80        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.30      0.42        30\n",
            "         1.0       0.60      0.89      0.71        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.64      0.59      0.57        65\n",
            "weighted avg       0.64      0.62      0.58        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      0.43      0.50        30\n",
            "         1.0       0.60      0.74      0.67        35\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.60      0.59      0.58        65\n",
            "weighted avg       0.60      0.60      0.59        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.77      0.77        30\n",
            "         1.0       0.80      0.80      0.80        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.78      0.78      0.78        65\n",
            "weighted avg       0.78      0.78      0.78        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.80      0.81        30\n",
            "         1.0       0.83      0.86      0.85        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.83      0.83      0.83        65\n",
            "weighted avg       0.83      0.83      0.83        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.67      0.65        30\n",
            "         1.0       0.70      0.66      0.68        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.66      0.66      0.66        65\n",
            "weighted avg       0.66      0.66      0.66        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      0.90      0.71        30\n",
            "         1.0       0.84      0.46      0.59        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.71      0.68      0.65        65\n",
            "weighted avg       0.72      0.66      0.65        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.57      0.77      0.66        30\n",
            "         1.0       0.71      0.50      0.59        34\n",
            "\n",
            "    accuracy                           0.62        64\n",
            "   macro avg       0.64      0.63      0.62        64\n",
            "weighted avg       0.65      0.62      0.62        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.539683  1.000000  0.701031       34  0.553846\n",
            "1   0.589286  0.942857  0.725275       35  0.615385\n",
            "2   0.761905  0.914286  0.831169       35  0.800000\n",
            "3   0.596154  0.885714  0.712644       35  0.615385\n",
            "4   0.604651  0.742857  0.666667       35  0.600000\n",
            "5   0.800000  0.800000  0.800000       35  0.784615\n",
            "6   0.833333  0.857143  0.845070       35  0.830769\n",
            "7   0.696970  0.657143  0.676471       35  0.661538\n",
            "8   0.842105  0.457143  0.592593       35  0.661538\n",
            "9   0.708333  0.500000  0.586207       34  0.625000\n",
            "c =  0.25\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.03      0.06        31\n",
            "         1.0       0.53      1.00      0.69        34\n",
            "\n",
            "    accuracy                           0.54        65\n",
            "   macro avg       0.77      0.52      0.38        65\n",
            "weighted avg       0.75      0.54      0.39        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.20      0.32        30\n",
            "         1.0       0.59      0.97      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.72      0.59      0.53        65\n",
            "weighted avg       0.71      0.62      0.54        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.63      0.73        30\n",
            "         1.0       0.74      0.91      0.82        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.80      0.77      0.78        65\n",
            "weighted avg       0.80      0.78      0.78        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.20      0.31        30\n",
            "         1.0       0.57      0.91      0.70        35\n",
            "\n",
            "    accuracy                           0.58        65\n",
            "   macro avg       0.62      0.56      0.51        65\n",
            "weighted avg       0.62      0.58      0.52        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.40      0.49        30\n",
            "         1.0       0.61      0.80      0.69        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.62      0.60      0.59        65\n",
            "weighted avg       0.62      0.62      0.60        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.67      0.74        30\n",
            "         1.0       0.76      0.89      0.82        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.79      0.78      0.78        65\n",
            "weighted avg       0.79      0.78      0.78        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.73      0.80        30\n",
            "         1.0       0.80      0.91      0.85        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.84      0.82      0.83        65\n",
            "weighted avg       0.84      0.83      0.83        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.63      0.68        30\n",
            "         1.0       0.72      0.80      0.76        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.72      0.72      0.72        65\n",
            "weighted avg       0.72      0.72      0.72        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.90      0.73        30\n",
            "         1.0       0.86      0.51      0.64        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.74      0.71      0.69        65\n",
            "weighted avg       0.74      0.69      0.68        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.67      0.65        30\n",
            "         1.0       0.69      0.65      0.67        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.531250  1.000000  0.693878       34  0.538462\n",
            "1   0.586207  0.971429  0.731183       35  0.615385\n",
            "2   0.744186  0.914286  0.820513       35  0.784615\n",
            "3   0.571429  0.914286  0.703297       35  0.584615\n",
            "4   0.608696  0.800000  0.691358       35  0.615385\n",
            "5   0.756098  0.885714  0.815789       35  0.784615\n",
            "6   0.800000  0.914286  0.853333       35  0.830769\n",
            "7   0.717949  0.800000  0.756757       35  0.723077\n",
            "8   0.857143  0.514286  0.642857       35  0.692308\n",
            "9   0.687500  0.647059  0.666667       34  0.656250\n",
            "c =  0.5\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.03      0.06        31\n",
            "         1.0       0.53      1.00      0.69        34\n",
            "\n",
            "    accuracy                           0.54        65\n",
            "   macro avg       0.77      0.52      0.38        65\n",
            "weighted avg       0.75      0.54      0.39        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.20      0.32        30\n",
            "         1.0       0.59      0.97      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.72      0.59      0.53        65\n",
            "weighted avg       0.71      0.62      0.54        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.53      0.65        30\n",
            "         1.0       0.70      0.91      0.79        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.77      0.72      0.72        65\n",
            "weighted avg       0.76      0.74      0.73        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.13      0.22        30\n",
            "         1.0       0.56      0.94      0.70        35\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.61      0.54      0.46        65\n",
            "weighted avg       0.61      0.57      0.48        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.40      0.51        30\n",
            "         1.0       0.62      0.86      0.72        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.67      0.63      0.62        65\n",
            "weighted avg       0.66      0.65      0.62        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.63      0.75        30\n",
            "         1.0       0.75      0.94      0.84        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.83      0.79      0.79        65\n",
            "weighted avg       0.82      0.80      0.79        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.73      0.83        30\n",
            "         1.0       0.81      0.97      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.88      0.85      0.86        65\n",
            "weighted avg       0.88      0.86      0.86        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.53      0.63        30\n",
            "         1.0       0.68      0.86      0.76        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.72      0.70      0.69        65\n",
            "weighted avg       0.72      0.71      0.70        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.64      0.90      0.75        30\n",
            "         1.0       0.87      0.57      0.69        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.76      0.74      0.72        65\n",
            "weighted avg       0.76      0.72      0.72        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.65      0.67      0.66        30\n",
            "         1.0       0.70      0.68      0.69        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.67      0.67      0.67        64\n",
            "weighted avg       0.67      0.67      0.67        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.531250  1.000000  0.693878       34  0.538462\n",
            "1   0.586207  0.971429  0.731183       35  0.615385\n",
            "2   0.695652  0.914286  0.790123       35  0.738462\n",
            "3   0.559322  0.942857  0.702128       35  0.569231\n",
            "4   0.625000  0.857143  0.722892       35  0.646154\n",
            "5   0.750000  0.942857  0.835443       35  0.800000\n",
            "6   0.809524  0.971429  0.883117       35  0.861538\n",
            "7   0.681818  0.857143  0.759494       35  0.707692\n",
            "8   0.869565  0.571429  0.689655       35  0.723077\n",
            "9   0.696970  0.676471  0.686567       34  0.671875\n",
            "c =  0.75\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        31\n",
            "         1.0       0.52      1.00      0.69        34\n",
            "\n",
            "    accuracy                           0.52        65\n",
            "   macro avg       0.26      0.50      0.34        65\n",
            "weighted avg       0.27      0.52      0.36        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.20      0.32        30\n",
            "         1.0       0.59      0.97      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.72      0.59      0.53        65\n",
            "weighted avg       0.71      0.62      0.54        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.50      0.62        30\n",
            "         1.0       0.68      0.91      0.78        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.76      0.71      0.70        65\n",
            "weighted avg       0.75      0.72      0.71        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.13      0.22        30\n",
            "         1.0       0.56      0.94      0.70        35\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.61      0.54      0.46        65\n",
            "weighted avg       0.61      0.57      0.48        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.37      0.49        30\n",
            "         1.0       0.62      0.89      0.73        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.68      0.63      0.61        65\n",
            "weighted avg       0.67      0.65      0.62        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.60      0.72        30\n",
            "         1.0       0.73      0.94      0.83        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.82      0.77      0.77        65\n",
            "weighted avg       0.81      0.78      0.78        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.70      0.82        30\n",
            "         1.0       0.80      1.00      0.89        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.90      0.85      0.85        65\n",
            "weighted avg       0.89      0.86      0.86        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.87      0.75        30\n",
            "         1.0       0.85      0.63      0.72        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.76      0.75      0.74        65\n",
            "weighted avg       0.76      0.74      0.74        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.63      0.63        30\n",
            "         1.0       0.68      0.68      0.68        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.65      0.65      0.65        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.523077  1.000000  0.686869       34  0.523077\n",
            "1   0.586207  0.971429  0.731183       35  0.615385\n",
            "2   0.680851  0.914286  0.780488       35  0.723077\n",
            "3   0.559322  0.942857  0.702128       35  0.569231\n",
            "4   0.620000  0.885714  0.729412       35  0.646154\n",
            "5   0.733333  0.942857  0.825000       35  0.784615\n",
            "6   0.795455  1.000000  0.886076       35  0.861538\n",
            "7   0.666667  0.857143  0.750000       35  0.692308\n",
            "8   0.846154  0.628571  0.721311       35  0.738462\n",
            "9   0.676471  0.676471  0.676471       34  0.656250\n",
            "c =  1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        31\n",
            "         1.0       0.52      1.00      0.69        34\n",
            "\n",
            "    accuracy                           0.52        65\n",
            "   macro avg       0.26      0.50      0.34        65\n",
            "weighted avg       0.27      0.52      0.36        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.20      0.32        30\n",
            "         1.0       0.59      0.97      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.72      0.59      0.53        65\n",
            "weighted avg       0.71      0.62      0.54        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.47      0.60        30\n",
            "         1.0       0.67      0.91      0.77        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.75      0.69      0.68        65\n",
            "weighted avg       0.74      0.71      0.69        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.10      0.17        30\n",
            "         1.0       0.55      0.94      0.69        35\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.57      0.52      0.43        65\n",
            "weighted avg       0.57      0.55      0.45        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.37      0.50        30\n",
            "         1.0       0.63      0.91      0.74        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.71      0.64      0.62        65\n",
            "weighted avg       0.70      0.66      0.63        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.53      0.67        30\n",
            "         1.0       0.70      0.94      0.80        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.80      0.74      0.74        65\n",
            "weighted avg       0.79      0.75      0.74        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.70      0.82        30\n",
            "         1.0       0.80      1.00      0.89        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.90      0.85      0.85        65\n",
            "weighted avg       0.89      0.86      0.86        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.43      0.54        30\n",
            "         1.0       0.64      0.86      0.73        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.68      0.65      0.64        65\n",
            "weighted avg       0.68      0.66      0.64        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.83      0.77        30\n",
            "         1.0       0.83      0.71      0.77        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.77      0.77      0.77        65\n",
            "weighted avg       0.78      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.63      0.64        30\n",
            "         1.0       0.69      0.71      0.70        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.67      0.67      0.67        64\n",
            "weighted avg       0.67      0.67      0.67        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.523077  1.000000  0.686869       34  0.523077\n",
            "1   0.586207  0.971429  0.731183       35  0.615385\n",
            "2   0.666667  0.914286  0.771084       35  0.707692\n",
            "3   0.550000  0.942857  0.694737       35  0.553846\n",
            "4   0.627451  0.914286  0.744186       35  0.661538\n",
            "5   0.702128  0.942857  0.804878       35  0.753846\n",
            "6   0.795455  1.000000  0.886076       35  0.861538\n",
            "7   0.638298  0.857143  0.731707       35  0.661538\n",
            "8   0.833333  0.714286  0.769231       35  0.769231\n",
            "9   0.685714  0.705882  0.695652       34  0.671875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFhxcmvRyVap",
        "colab_type": "text"
      },
      "source": [
        "Resultado dos k-fold para cada valor de c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "5BhEkmnkyVaq",
        "colab_type": "code",
        "outputId": "88be87e8-83e8-4f46-cb8c-ca8ae0279ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "result_pca"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.566667  1.000000  0.723404       34  0.600000     0.001    PCA\n",
              "  1   0.615385  0.914286  0.735632       35  0.646154     0.001    PCA\n",
              "  2   0.810811  0.857143  0.833333       35  0.815385     0.001    PCA\n",
              "  3   0.617021  0.828571  0.707317       35  0.630769     0.001    PCA\n",
              "  4   0.666667  0.685714  0.676056       35  0.646154     0.001    PCA\n",
              "  5   0.823529  0.800000  0.811594       35  0.800000     0.001    PCA\n",
              "  6   0.828571  0.828571  0.828571       35  0.815385     0.001    PCA\n",
              "  7   0.692308  0.514286  0.590164       35  0.615385     0.001    PCA\n",
              "  8   0.928571  0.371429  0.530612       35  0.646154     0.001    PCA\n",
              "  9   0.736842  0.411765  0.528302       34  0.609375     0.001    PCA],\n",
              " [0.1,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.539683  1.000000  0.701031       34  0.553846       0.1    PCA\n",
              "  1   0.589286  0.942857  0.725275       35  0.615385       0.1    PCA\n",
              "  2   0.761905  0.914286  0.831169       35  0.800000       0.1    PCA\n",
              "  3   0.596154  0.885714  0.712644       35  0.615385       0.1    PCA\n",
              "  4   0.604651  0.742857  0.666667       35  0.600000       0.1    PCA\n",
              "  5   0.800000  0.800000  0.800000       35  0.784615       0.1    PCA\n",
              "  6   0.833333  0.857143  0.845070       35  0.830769       0.1    PCA\n",
              "  7   0.696970  0.657143  0.676471       35  0.661538       0.1    PCA\n",
              "  8   0.842105  0.457143  0.592593       35  0.661538       0.1    PCA\n",
              "  9   0.708333  0.500000  0.586207       34  0.625000       0.1    PCA],\n",
              " [0.25,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.531250  1.000000  0.693878       34  0.538462      0.25    PCA\n",
              "  1   0.586207  0.971429  0.731183       35  0.615385      0.25    PCA\n",
              "  2   0.744186  0.914286  0.820513       35  0.784615      0.25    PCA\n",
              "  3   0.571429  0.914286  0.703297       35  0.584615      0.25    PCA\n",
              "  4   0.608696  0.800000  0.691358       35  0.615385      0.25    PCA\n",
              "  5   0.756098  0.885714  0.815789       35  0.784615      0.25    PCA\n",
              "  6   0.800000  0.914286  0.853333       35  0.830769      0.25    PCA\n",
              "  7   0.717949  0.800000  0.756757       35  0.723077      0.25    PCA\n",
              "  8   0.857143  0.514286  0.642857       35  0.692308      0.25    PCA\n",
              "  9   0.687500  0.647059  0.666667       34  0.656250      0.25    PCA],\n",
              " [0.5,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.531250  1.000000  0.693878       34  0.538462       0.5    PCA\n",
              "  1   0.586207  0.971429  0.731183       35  0.615385       0.5    PCA\n",
              "  2   0.695652  0.914286  0.790123       35  0.738462       0.5    PCA\n",
              "  3   0.559322  0.942857  0.702128       35  0.569231       0.5    PCA\n",
              "  4   0.625000  0.857143  0.722892       35  0.646154       0.5    PCA\n",
              "  5   0.750000  0.942857  0.835443       35  0.800000       0.5    PCA\n",
              "  6   0.809524  0.971429  0.883117       35  0.861538       0.5    PCA\n",
              "  7   0.681818  0.857143  0.759494       35  0.707692       0.5    PCA\n",
              "  8   0.869565  0.571429  0.689655       35  0.723077       0.5    PCA\n",
              "  9   0.696970  0.676471  0.686567       34  0.671875       0.5    PCA],\n",
              " [0.75,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.523077  1.000000  0.686869       34  0.523077      0.75    PCA\n",
              "  1   0.586207  0.971429  0.731183       35  0.615385      0.75    PCA\n",
              "  2   0.680851  0.914286  0.780488       35  0.723077      0.75    PCA\n",
              "  3   0.559322  0.942857  0.702128       35  0.569231      0.75    PCA\n",
              "  4   0.620000  0.885714  0.729412       35  0.646154      0.75    PCA\n",
              "  5   0.733333  0.942857  0.825000       35  0.784615      0.75    PCA\n",
              "  6   0.795455  1.000000  0.886076       35  0.861538      0.75    PCA\n",
              "  7   0.666667  0.857143  0.750000       35  0.692308      0.75    PCA\n",
              "  8   0.846154  0.628571  0.721311       35  0.738462      0.75    PCA\n",
              "  9   0.676471  0.676471  0.676471       34  0.656250      0.75    PCA],\n",
              " [1,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.523077  1.000000  0.686869       34  0.523077         1    PCA\n",
              "  1   0.586207  0.971429  0.731183       35  0.615385         1    PCA\n",
              "  2   0.666667  0.914286  0.771084       35  0.707692         1    PCA\n",
              "  3   0.550000  0.942857  0.694737       35  0.553846         1    PCA\n",
              "  4   0.627451  0.914286  0.744186       35  0.661538         1    PCA\n",
              "  5   0.702128  0.942857  0.804878       35  0.753846         1    PCA\n",
              "  6   0.795455  1.000000  0.886076       35  0.861538         1    PCA\n",
              "  7   0.638298  0.857143  0.731707       35  0.661538         1    PCA\n",
              "  8   0.833333  0.714286  0.769231       35  0.769231         1    PCA\n",
              "  9   0.685714  0.705882  0.695652       34  0.671875         1    PCA]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPSXY9-tyVau",
        "colab_type": "text"
      },
      "source": [
        "Calcula a média das medidas de cada parâmetro c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tTeRvhucyVaw",
        "colab_type": "code",
        "outputId": "39f53e17-c69d-497a-e34c-63e512c7b386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "result_pca_mean = calcula_media(result_pca)\n",
        "result_pca_mean"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.728637</td>\n",
              "      <td>0.721176</td>\n",
              "      <td>0.696499</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.682476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.697242</td>\n",
              "      <td>0.775714</td>\n",
              "      <td>0.713713</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.674808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250</td>\n",
              "      <td>0.686046</td>\n",
              "      <td>0.836134</td>\n",
              "      <td>0.737563</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.682548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.680531</td>\n",
              "      <td>0.870504</td>\n",
              "      <td>0.749448</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.687188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.668754</td>\n",
              "      <td>0.881933</td>\n",
              "      <td>0.748894</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.681010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.660833</td>\n",
              "      <td>0.896303</td>\n",
              "      <td>0.751560</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.677957</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "0  0.001        0.728637  ...          34.8       0.682476\n",
              "1  0.100        0.697242  ...          34.8       0.674808\n",
              "2  0.250        0.686046  ...          34.8       0.682548\n",
              "3  0.500        0.680531  ...          34.8       0.687188\n",
              "4  0.750        0.668754  ...          34.8       0.681010\n",
              "5  1.000        0.660833  ...          34.8       0.677957\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n21hC2IuyVa2",
        "colab_type": "text"
      },
      "source": [
        "Obtém o resultado da maior média de acurácia "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7slQvTAkyVa3",
        "colab_type": "code",
        "outputId": "81b60bd4-196f-4a20-e7b0-22b6c68195b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "best_accuracy_pca = pd.Series(result_pca_mean.iloc[result_pca_mean['accuracy_mean'].idxmax()], \n",
        "                          name='PCA')\n",
        "best_pca = pd.DataFrame(best_accuracy_pca)\n",
        "best_pca"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PCA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_mean</th>\n",
              "      <td>0.680531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_mean</th>\n",
              "      <td>0.870504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score_mean</th>\n",
              "      <td>0.749448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support_mean</th>\n",
              "      <td>34.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_mean</th>\n",
              "      <td>0.687188</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      PCA\n",
              "c                0.500000\n",
              "precision_mean   0.680531\n",
              "recall_mean      0.870504\n",
              "f1_score_mean    0.749448\n",
              "support_mean    34.800000\n",
              "accuracy_mean    0.687188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ale_cv-gyVa8",
        "colab_type": "text"
      },
      "source": [
        "# Execução Base: Chi Squared (K-Best)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9dnzjgTyVa9",
        "colab_type": "code",
        "outputId": "40dc7dde-5835-49fc-ee2f-007c7ff1c3e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df = pd.read_csv('results/dataset-fs-chi-squared.csv', header = 0)\n",
        "X = df.drop('is_approved', axis=1).to_numpy() # DATASET\n",
        "y = df['is_approved'].to_numpy() # target\n",
        "result_chi , result_chi_g  = stratified_k_fold(X, y, list_c, k=k_folds, name='CHI')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 10, Dataset: 348 positivas, 301 negativas (53% x 46%)\n",
            "Fold 0: Pos: 34, Neg: 31, Total: 65, Proporção: 52% x 47%\n",
            "Fold 1: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 2: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 3: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 4: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 5: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 6: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 7: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 8: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 9: Pos: 34, Neg: 30, Total: 64, Proporção: 53% x 46%\n",
            "c =  0.001\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.16      0.27        31\n",
            "         1.0       0.56      0.97      0.71        34\n",
            "\n",
            "    accuracy                           0.58        65\n",
            "   macro avg       0.70      0.57      0.49        65\n",
            "weighted avg       0.69      0.58      0.50        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.40      0.55        30\n",
            "         1.0       0.65      0.94      0.77        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.75      0.67      0.66        65\n",
            "weighted avg       0.74      0.69      0.66        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.77      0.79        30\n",
            "         1.0       0.81      0.86      0.83        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.82      0.81      0.81        65\n",
            "weighted avg       0.82      0.82      0.81        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.53      0.64        30\n",
            "         1.0       0.69      0.89      0.78        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.74      0.71      0.71        65\n",
            "weighted avg       0.74      0.72      0.71        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67        30\n",
            "         1.0       0.71      0.77      0.74        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.71        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.80      0.84        30\n",
            "         1.0       0.84      0.91      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.86      0.86        65\n",
            "weighted avg       0.86      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.83      0.89        30\n",
            "         1.0       0.87      0.97      0.92        35\n",
            "\n",
            "    accuracy                           0.91        65\n",
            "   macro avg       0.92      0.90      0.91        65\n",
            "weighted avg       0.91      0.91      0.91        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.70      0.70        30\n",
            "         1.0       0.74      0.74      0.74        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.72      0.72      0.72        65\n",
            "weighted avg       0.72      0.72      0.72        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.97      0.75        30\n",
            "         1.0       0.94      0.49      0.64        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.78      0.73      0.70        65\n",
            "weighted avg       0.79      0.71      0.69        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.64      0.77      0.70        30\n",
            "         1.0       0.75      0.62      0.68        34\n",
            "\n",
            "    accuracy                           0.69        64\n",
            "   macro avg       0.69      0.69      0.69        64\n",
            "weighted avg       0.70      0.69      0.69        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.559322  0.970588  0.709677       34  0.584615\n",
            "1   0.647059  0.942857  0.767442       35  0.692308\n",
            "2   0.810811  0.857143  0.833333       35  0.815385\n",
            "3   0.688889  0.885714  0.775000       35  0.723077\n",
            "4   0.710526  0.771429  0.739726       35  0.707692\n",
            "5   0.842105  0.914286  0.876712       35  0.861538\n",
            "6   0.871795  0.971429  0.918919       35  0.907692\n",
            "7   0.742857  0.742857  0.742857       35  0.723077\n",
            "8   0.944444  0.485714  0.641509       35  0.707692\n",
            "9   0.750000  0.617647  0.677419       34  0.687500\n",
            "c =  0.1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.33      0.47        30\n",
            "         1.0       0.62      0.91      0.74        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.69      0.62      0.60        65\n",
            "weighted avg       0.69      0.65      0.61        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.70      0.72        30\n",
            "         1.0       0.76      0.80      0.78        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.75      0.75      0.75        65\n",
            "weighted avg       0.75      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67        30\n",
            "         1.0       0.71      0.77      0.74        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.71        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.80      0.84        30\n",
            "         1.0       0.84      0.91      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.86      0.86        65\n",
            "weighted avg       0.86      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.97      0.97        30\n",
            "         1.0       0.97      0.97      0.97        35\n",
            "\n",
            "    accuracy                           0.97        65\n",
            "   macro avg       0.97      0.97      0.97        65\n",
            "weighted avg       0.97      0.97      0.97        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.77      0.73        30\n",
            "         1.0       0.78      0.71      0.75        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.74      0.74      0.74        65\n",
            "weighted avg       0.74      0.74      0.74        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      0.97      0.73        30\n",
            "         1.0       0.94      0.43      0.59        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.76      0.70      0.66        65\n",
            "weighted avg       0.78      0.68      0.66        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.64      0.77      0.70        30\n",
            "         1.0       0.75      0.62      0.68        34\n",
            "\n",
            "    accuracy                           0.69        64\n",
            "   macro avg       0.69      0.69      0.69        64\n",
            "weighted avg       0.70      0.69      0.69        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.540984  0.970588  0.694737       34  0.553846\n",
            "1   0.615385  0.914286  0.735632       35  0.646154\n",
            "2   0.756757  0.800000  0.777778       35  0.753846\n",
            "3   0.666667  0.857143  0.750000       35  0.692308\n",
            "4   0.710526  0.771429  0.739726       35  0.707692\n",
            "5   0.842105  0.914286  0.876712       35  0.861538\n",
            "6   0.971429  0.971429  0.971429       35  0.969231\n",
            "7   0.781250  0.714286  0.746269       35  0.738462\n",
            "8   0.937500  0.428571  0.588235       35  0.676923\n",
            "9   0.750000  0.617647  0.677419       34  0.687500\n",
            "c =  0.25\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.27      0.39        30\n",
            "         1.0       0.59      0.91      0.72        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.66      0.59      0.55        65\n",
            "weighted avg       0.65      0.62      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.70      0.72        30\n",
            "         1.0       0.76      0.80      0.78        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.75      0.75      0.75        65\n",
            "weighted avg       0.75      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67        30\n",
            "         1.0       0.71      0.77      0.74        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.71        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.77      0.81        30\n",
            "         1.0       0.82      0.89      0.85        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.83      0.83      0.83        65\n",
            "weighted avg       0.83      0.83      0.83        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.97      0.97        30\n",
            "         1.0       0.97      0.97      0.97        35\n",
            "\n",
            "    accuracy                           0.97        65\n",
            "   macro avg       0.97      0.97      0.97        65\n",
            "weighted avg       0.97      0.97      0.97        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.77      0.74        30\n",
            "         1.0       0.79      0.74      0.76        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.75      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.97      0.74        30\n",
            "         1.0       0.94      0.46      0.62        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.77      0.71      0.68        65\n",
            "weighted avg       0.79      0.69      0.67        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.73      0.67        30\n",
            "         1.0       0.71      0.59      0.65        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.67      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.540984  0.970588  0.694737       34  0.553846\n",
            "1   0.592593  0.914286  0.719101       35  0.615385\n",
            "2   0.756757  0.800000  0.777778       35  0.753846\n",
            "3   0.666667  0.857143  0.750000       35  0.692308\n",
            "4   0.710526  0.771429  0.739726       35  0.707692\n",
            "5   0.815789  0.885714  0.849315       35  0.830769\n",
            "6   0.971429  0.971429  0.971429       35  0.969231\n",
            "7   0.787879  0.742857  0.764706       35  0.753846\n",
            "8   0.941176  0.457143  0.615385       35  0.692308\n",
            "9   0.714286  0.588235  0.645161       34  0.656250\n",
            "c =  0.5\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.27      0.39        30\n",
            "         1.0       0.59      0.91      0.72        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.66      0.59      0.55        65\n",
            "weighted avg       0.65      0.62      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.67      0.71        30\n",
            "         1.0       0.74      0.83      0.78        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.43      0.55        30\n",
            "         1.0       0.65      0.89      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.71      0.66      0.65        65\n",
            "weighted avg       0.70      0.68      0.66        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.57      0.63        30\n",
            "         1.0       0.68      0.80      0.74        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.70      0.68      0.68        65\n",
            "weighted avg       0.69      0.69      0.69        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.77      0.84        30\n",
            "         1.0       0.82      0.94      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.85      0.86        65\n",
            "weighted avg       0.87      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.93      0.97        30\n",
            "         1.0       0.95      1.00      0.97        35\n",
            "\n",
            "    accuracy                           0.97        65\n",
            "   macro avg       0.97      0.97      0.97        65\n",
            "weighted avg       0.97      0.97      0.97        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.80      0.77        30\n",
            "         1.0       0.82      0.77      0.79        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.78      0.79      0.78        65\n",
            "weighted avg       0.79      0.78      0.78        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.97      0.74        30\n",
            "         1.0       0.94      0.46      0.62        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.77      0.71      0.68        65\n",
            "weighted avg       0.79      0.69      0.67        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.73      0.68        30\n",
            "         1.0       0.72      0.62      0.67        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.68      0.68      0.67        64\n",
            "weighted avg       0.68      0.67      0.67        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.540984  0.970588  0.694737       34  0.553846\n",
            "1   0.592593  0.914286  0.719101       35  0.615385\n",
            "2   0.743590  0.828571  0.783784       35  0.753846\n",
            "3   0.645833  0.885714  0.746988       35  0.676923\n",
            "4   0.682927  0.800000  0.736842       35  0.692308\n",
            "5   0.825000  0.942857  0.880000       35  0.861538\n",
            "6   0.945946  1.000000  0.972222       35  0.969231\n",
            "7   0.818182  0.771429  0.794118       35  0.784615\n",
            "8   0.941176  0.457143  0.615385       35  0.692308\n",
            "9   0.724138  0.617647  0.666667       34  0.671875\n",
            "c =  0.75\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.27      0.40        30\n",
            "         1.0       0.60      0.94      0.73        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.70      0.60      0.57        65\n",
            "weighted avg       0.69      0.63      0.58        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.63      0.69        30\n",
            "         1.0       0.72      0.83      0.77        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.74      0.73      0.73        65\n",
            "weighted avg       0.74      0.74      0.74        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.43      0.55        30\n",
            "         1.0       0.65      0.89      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.71      0.66      0.65        65\n",
            "weighted avg       0.70      0.68      0.66        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.53      0.60        30\n",
            "         1.0       0.67      0.80      0.73        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.77      0.84        30\n",
            "         1.0       0.82      0.94      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.85      0.86        65\n",
            "weighted avg       0.87      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.90      0.95        30\n",
            "         1.0       0.92      1.00      0.96        35\n",
            "\n",
            "    accuracy                           0.95        65\n",
            "   macro avg       0.96      0.95      0.95        65\n",
            "weighted avg       0.96      0.95      0.95        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.77      0.77        30\n",
            "         1.0       0.80      0.80      0.80        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.78      0.78      0.78        65\n",
            "weighted avg       0.78      0.78      0.78        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.97      0.74        30\n",
            "         1.0       0.94      0.46      0.62        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.77      0.71      0.68        65\n",
            "weighted avg       0.79      0.69      0.67        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.540984  0.970588  0.694737       34  0.553846\n",
            "1   0.600000  0.942857  0.733333       35  0.630769\n",
            "2   0.725000  0.828571  0.773333       35  0.738462\n",
            "3   0.645833  0.885714  0.746988       35  0.676923\n",
            "4   0.666667  0.800000  0.727273       35  0.676923\n",
            "5   0.825000  0.942857  0.880000       35  0.861538\n",
            "6   0.921053  1.000000  0.958904       35  0.953846\n",
            "7   0.800000  0.800000  0.800000       35  0.784615\n",
            "8   0.941176  0.457143  0.615385       35  0.692308\n",
            "9   0.700000  0.617647  0.656250       34  0.656250\n",
            "c =  1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.27      0.40        30\n",
            "         1.0       0.60      0.94      0.73        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.70      0.60      0.57        65\n",
            "weighted avg       0.69      0.63      0.58        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.60      0.67        30\n",
            "         1.0       0.71      0.83      0.76        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.73      0.71      0.71        65\n",
            "weighted avg       0.73      0.72      0.72        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.37      0.49        30\n",
            "         1.0       0.62      0.89      0.73        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.68      0.63      0.61        65\n",
            "weighted avg       0.67      0.65      0.62        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.53      0.60        30\n",
            "         1.0       0.67      0.80      0.73        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.77      0.84        30\n",
            "         1.0       0.82      0.94      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.85      0.86        65\n",
            "weighted avg       0.87      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.90      0.95        30\n",
            "         1.0       0.92      1.00      0.96        35\n",
            "\n",
            "    accuracy                           0.95        65\n",
            "   macro avg       0.96      0.95      0.95        65\n",
            "weighted avg       0.96      0.95      0.95        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.73      0.76        30\n",
            "         1.0       0.78      0.83      0.81        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.78      0.78      0.78        65\n",
            "weighted avg       0.78      0.78      0.78        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.93      0.73        30\n",
            "         1.0       0.89      0.46      0.60        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.74      0.70      0.67        65\n",
            "weighted avg       0.75      0.68      0.66        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.540984  0.970588  0.694737       34  0.553846\n",
            "1   0.600000  0.942857  0.733333       35  0.630769\n",
            "2   0.707317  0.828571  0.763158       35  0.723077\n",
            "3   0.620000  0.885714  0.729412       35  0.646154\n",
            "4   0.666667  0.800000  0.727273       35  0.676923\n",
            "5   0.825000  0.942857  0.880000       35  0.861538\n",
            "6   0.921053  1.000000  0.958904       35  0.953846\n",
            "7   0.783784  0.828571  0.805556       35  0.784615\n",
            "8   0.888889  0.457143  0.603774       35  0.676923\n",
            "9   0.700000  0.617647  0.656250       34  0.656250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xKkaHx0yVbE",
        "colab_type": "text"
      },
      "source": [
        "Resultado dos k-fold para cada valor de c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_d3uSAFyVbF",
        "colab_type": "code",
        "outputId": "39ddff48-6d74-408e-f64c-36178345d498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "result_chi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.559322  0.970588  0.709677       34  0.584615     0.001    CHI\n",
              "  1   0.647059  0.942857  0.767442       35  0.692308     0.001    CHI\n",
              "  2   0.810811  0.857143  0.833333       35  0.815385     0.001    CHI\n",
              "  3   0.688889  0.885714  0.775000       35  0.723077     0.001    CHI\n",
              "  4   0.710526  0.771429  0.739726       35  0.707692     0.001    CHI\n",
              "  5   0.842105  0.914286  0.876712       35  0.861538     0.001    CHI\n",
              "  6   0.871795  0.971429  0.918919       35  0.907692     0.001    CHI\n",
              "  7   0.742857  0.742857  0.742857       35  0.723077     0.001    CHI\n",
              "  8   0.944444  0.485714  0.641509       35  0.707692     0.001    CHI\n",
              "  9   0.750000  0.617647  0.677419       34  0.687500     0.001    CHI],\n",
              " [0.1,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846       0.1    CHI\n",
              "  1   0.615385  0.914286  0.735632       35  0.646154       0.1    CHI\n",
              "  2   0.756757  0.800000  0.777778       35  0.753846       0.1    CHI\n",
              "  3   0.666667  0.857143  0.750000       35  0.692308       0.1    CHI\n",
              "  4   0.710526  0.771429  0.739726       35  0.707692       0.1    CHI\n",
              "  5   0.842105  0.914286  0.876712       35  0.861538       0.1    CHI\n",
              "  6   0.971429  0.971429  0.971429       35  0.969231       0.1    CHI\n",
              "  7   0.781250  0.714286  0.746269       35  0.738462       0.1    CHI\n",
              "  8   0.937500  0.428571  0.588235       35  0.676923       0.1    CHI\n",
              "  9   0.750000  0.617647  0.677419       34  0.687500       0.1    CHI],\n",
              " [0.25,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846      0.25    CHI\n",
              "  1   0.592593  0.914286  0.719101       35  0.615385      0.25    CHI\n",
              "  2   0.756757  0.800000  0.777778       35  0.753846      0.25    CHI\n",
              "  3   0.666667  0.857143  0.750000       35  0.692308      0.25    CHI\n",
              "  4   0.710526  0.771429  0.739726       35  0.707692      0.25    CHI\n",
              "  5   0.815789  0.885714  0.849315       35  0.830769      0.25    CHI\n",
              "  6   0.971429  0.971429  0.971429       35  0.969231      0.25    CHI\n",
              "  7   0.787879  0.742857  0.764706       35  0.753846      0.25    CHI\n",
              "  8   0.941176  0.457143  0.615385       35  0.692308      0.25    CHI\n",
              "  9   0.714286  0.588235  0.645161       34  0.656250      0.25    CHI],\n",
              " [0.5,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846       0.5    CHI\n",
              "  1   0.592593  0.914286  0.719101       35  0.615385       0.5    CHI\n",
              "  2   0.743590  0.828571  0.783784       35  0.753846       0.5    CHI\n",
              "  3   0.645833  0.885714  0.746988       35  0.676923       0.5    CHI\n",
              "  4   0.682927  0.800000  0.736842       35  0.692308       0.5    CHI\n",
              "  5   0.825000  0.942857  0.880000       35  0.861538       0.5    CHI\n",
              "  6   0.945946  1.000000  0.972222       35  0.969231       0.5    CHI\n",
              "  7   0.818182  0.771429  0.794118       35  0.784615       0.5    CHI\n",
              "  8   0.941176  0.457143  0.615385       35  0.692308       0.5    CHI\n",
              "  9   0.724138  0.617647  0.666667       34  0.671875       0.5    CHI],\n",
              " [0.75,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846      0.75    CHI\n",
              "  1   0.600000  0.942857  0.733333       35  0.630769      0.75    CHI\n",
              "  2   0.725000  0.828571  0.773333       35  0.738462      0.75    CHI\n",
              "  3   0.645833  0.885714  0.746988       35  0.676923      0.75    CHI\n",
              "  4   0.666667  0.800000  0.727273       35  0.676923      0.75    CHI\n",
              "  5   0.825000  0.942857  0.880000       35  0.861538      0.75    CHI\n",
              "  6   0.921053  1.000000  0.958904       35  0.953846      0.75    CHI\n",
              "  7   0.800000  0.800000  0.800000       35  0.784615      0.75    CHI\n",
              "  8   0.941176  0.457143  0.615385       35  0.692308      0.75    CHI\n",
              "  9   0.700000  0.617647  0.656250       34  0.656250      0.75    CHI],\n",
              " [1,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846         1    CHI\n",
              "  1   0.600000  0.942857  0.733333       35  0.630769         1    CHI\n",
              "  2   0.707317  0.828571  0.763158       35  0.723077         1    CHI\n",
              "  3   0.620000  0.885714  0.729412       35  0.646154         1    CHI\n",
              "  4   0.666667  0.800000  0.727273       35  0.676923         1    CHI\n",
              "  5   0.825000  0.942857  0.880000       35  0.861538         1    CHI\n",
              "  6   0.921053  1.000000  0.958904       35  0.953846         1    CHI\n",
              "  7   0.783784  0.828571  0.805556       35  0.784615         1    CHI\n",
              "  8   0.888889  0.457143  0.603774       35  0.676923         1    CHI\n",
              "  9   0.700000  0.617647  0.656250       34  0.656250         1    CHI]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxKFeV75yVbK",
        "colab_type": "text"
      },
      "source": [
        "Calcula a média das medidas de cada parâmetro c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiwgWhz2yVbL",
        "colab_type": "code",
        "outputId": "b34d512d-eb55-4dfd-df22-00a2226c9eb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "result_chi_mean = calcula_media(result_chi)\n",
        "result_chi_mean"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.756781</td>\n",
              "      <td>0.815966</td>\n",
              "      <td>0.768260</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.741058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.757260</td>\n",
              "      <td>0.795966</td>\n",
              "      <td>0.755794</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.728750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250</td>\n",
              "      <td>0.749808</td>\n",
              "      <td>0.795882</td>\n",
              "      <td>0.752734</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.722548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.746037</td>\n",
              "      <td>0.818824</td>\n",
              "      <td>0.760984</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.727187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.736571</td>\n",
              "      <td>0.824538</td>\n",
              "      <td>0.758620</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.722548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.725369</td>\n",
              "      <td>0.827395</td>\n",
              "      <td>0.755240</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.716394</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "0  0.001        0.756781  ...          34.8       0.741058\n",
              "1  0.100        0.757260  ...          34.8       0.728750\n",
              "2  0.250        0.749808  ...          34.8       0.722548\n",
              "3  0.500        0.746037  ...          34.8       0.727187\n",
              "4  0.750        0.736571  ...          34.8       0.722548\n",
              "5  1.000        0.725369  ...          34.8       0.716394\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0PrGh7-yVbP",
        "colab_type": "text"
      },
      "source": [
        "Obtém o resultado da maior média de acurácia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZIuVq2JyVbP",
        "colab_type": "code",
        "outputId": "c38fc24a-0086-424c-f9d9-43c3878f10c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "best_accuracy_chi = pd.Series(result_chi_mean.iloc[result_chi_mean['accuracy_mean'].idxmax()], \n",
        "                          name='Chi Squared')\n",
        "best_chi = pd.DataFrame(best_accuracy_chi)\n",
        "best_chi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Chi Squared</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_mean</th>\n",
              "      <td>0.756781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_mean</th>\n",
              "      <td>0.815966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score_mean</th>\n",
              "      <td>0.768260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support_mean</th>\n",
              "      <td>34.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_mean</th>\n",
              "      <td>0.741058</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Chi Squared\n",
              "c                  0.001000\n",
              "precision_mean     0.756781\n",
              "recall_mean        0.815966\n",
              "f1_score_mean      0.768260\n",
              "support_mean      34.800000\n",
              "accuracy_mean      0.741058"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9NNSbziyVbU",
        "colab_type": "text"
      },
      "source": [
        "# Execução Base: recursive-feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1WD345tyVbV",
        "colab_type": "code",
        "outputId": "6dde0ec8-637f-4531-b5de-33ec0891f504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df = pd.read_csv('results/dataset-fs-recursive-feature.csv', header = 0)\n",
        "X = df.drop('is_approved', axis=1).to_numpy() # DATASET\n",
        "y = df['is_approved'].to_numpy() # target\n",
        "result_recursive , result_recursive_g  = stratified_k_fold(X, y, list_c, k=k_folds, name='Recursive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 10, Dataset: 348 positivas, 301 negativas (53% x 46%)\n",
            "Fold 0: Pos: 34, Neg: 31, Total: 65, Proporção: 52% x 47%\n",
            "Fold 1: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 2: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 3: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 4: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 5: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 6: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 7: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 8: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 9: Pos: 34, Neg: 30, Total: 64, Proporção: 53% x 46%\n",
            "c =  0.001\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.13      0.23        31\n",
            "         1.0       0.56      1.00      0.72        34\n",
            "\n",
            "    accuracy                           0.58        65\n",
            "   macro avg       0.78      0.56      0.47        65\n",
            "weighted avg       0.77      0.58      0.48        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.40      0.53        30\n",
            "         1.0       0.64      0.91      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.72      0.66      0.64        65\n",
            "weighted avg       0.71      0.68      0.65        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.67      0.75        30\n",
            "         1.0       0.76      0.91      0.83        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.82      0.79      0.79        65\n",
            "weighted avg       0.81      0.80      0.80        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.47      0.55        30\n",
            "         1.0       0.64      0.80      0.71        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.65      0.63      0.63        65\n",
            "weighted avg       0.65      0.65      0.64        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.60      0.71        30\n",
            "         1.0       0.73      0.91      0.81        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.79      0.76      0.76        65\n",
            "weighted avg       0.79      0.77      0.76        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.67      0.75        30\n",
            "         1.0       0.76      0.91      0.83        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.82      0.79      0.79        65\n",
            "weighted avg       0.81      0.80      0.80        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.80      0.87        30\n",
            "         1.0       0.85      0.97      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.91      0.89      0.89        65\n",
            "weighted avg       0.90      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.47      0.56        30\n",
            "         1.0       0.64      0.83      0.73        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.67      0.65      0.64        65\n",
            "weighted avg       0.67      0.66      0.65        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.87      0.78        30\n",
            "         1.0       0.86      0.69      0.76        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.78      0.78      0.77        65\n",
            "weighted avg       0.79      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.73      0.71        30\n",
            "         1.0       0.75      0.71      0.73        34\n",
            "\n",
            "    accuracy                           0.72        64\n",
            "   macro avg       0.72      0.72      0.72        64\n",
            "weighted avg       0.72      0.72      0.72        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.557377  1.000000  0.715789       34  0.584615\n",
            "1   0.640000  0.914286  0.752941       35  0.676923\n",
            "2   0.761905  0.914286  0.831169       35  0.800000\n",
            "3   0.636364  0.800000  0.708861       35  0.646154\n",
            "4   0.727273  0.914286  0.810127       35  0.769231\n",
            "5   0.761905  0.914286  0.831169       35  0.800000\n",
            "6   0.850000  0.971429  0.906667       35  0.892308\n",
            "7   0.644444  0.828571  0.725000       35  0.661538\n",
            "8   0.857143  0.685714  0.761905       35  0.769231\n",
            "9   0.750000  0.705882  0.727273       34  0.718750\n",
            "c =  0.1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.06      0.12        31\n",
            "         1.0       0.53      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.54        65\n",
            "   macro avg       0.60      0.52      0.40        65\n",
            "weighted avg       0.60      0.54      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.33      0.49        30\n",
            "         1.0       0.63      0.97      0.76        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.77      0.65      0.63        65\n",
            "weighted avg       0.76      0.68      0.64        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.57      0.68        30\n",
            "         1.0       0.71      0.91      0.80        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.78      0.74      0.74        65\n",
            "weighted avg       0.78      0.75      0.74        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.40      0.53        30\n",
            "         1.0       0.64      0.91      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.72      0.66      0.64        65\n",
            "weighted avg       0.71      0.68      0.65        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.63      0.72        30\n",
            "         1.0       0.74      0.89      0.81        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.78      0.76      0.76        65\n",
            "weighted avg       0.78      0.77      0.76        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.67      0.78        30\n",
            "         1.0       0.77      0.97      0.86        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.86      0.82      0.82        65\n",
            "weighted avg       0.86      0.83      0.83        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.80      0.87        30\n",
            "         1.0       0.85      0.97      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.91      0.89      0.89        65\n",
            "weighted avg       0.90      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.47      0.57        30\n",
            "         1.0       0.65      0.86      0.74        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.69      0.66      0.66        65\n",
            "weighted avg       0.69      0.68      0.66        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.93      0.79        30\n",
            "         1.0       0.92      0.63      0.75        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.80      0.78      0.77        65\n",
            "weighted avg       0.81      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.70      0.69        30\n",
            "         1.0       0.73      0.71      0.72        34\n",
            "\n",
            "    accuracy                           0.70        64\n",
            "   macro avg       0.70      0.70      0.70        64\n",
            "weighted avg       0.70      0.70      0.70        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.532258  0.970588  0.687500       34  0.538462\n",
            "1   0.629630  0.971429  0.764045       35  0.676923\n",
            "2   0.711111  0.914286  0.800000       35  0.753846\n",
            "3   0.640000  0.914286  0.752941       35  0.676923\n",
            "4   0.738095  0.885714  0.805195       35  0.769231\n",
            "5   0.772727  0.971429  0.860759       35  0.830769\n",
            "6   0.850000  0.971429  0.906667       35  0.892308\n",
            "7   0.652174  0.857143  0.740741       35  0.676923\n",
            "8   0.916667  0.628571  0.745763       35  0.769231\n",
            "9   0.727273  0.705882  0.716418       34  0.703125\n",
            "c =  0.25\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.06      0.12        31\n",
            "         1.0       0.53      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.54        65\n",
            "   macro avg       0.60      0.52      0.40        65\n",
            "weighted avg       0.60      0.54      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.23      0.37        30\n",
            "         1.0       0.60      0.97      0.74        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.74      0.60      0.55        65\n",
            "weighted avg       0.73      0.63      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.57      0.68        30\n",
            "         1.0       0.71      0.91      0.80        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.78      0.74      0.74        65\n",
            "weighted avg       0.78      0.75      0.74        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.37      0.51        30\n",
            "         1.0       0.63      0.94      0.76        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.74      0.65      0.64        65\n",
            "weighted avg       0.73      0.68      0.64        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.57      0.68        30\n",
            "         1.0       0.71      0.91      0.80        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.78      0.74      0.74        65\n",
            "weighted avg       0.78      0.75      0.74        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.60      0.75        30\n",
            "         1.0       0.74      1.00      0.85        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.87      0.80      0.80        65\n",
            "weighted avg       0.86      0.82      0.81        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.77      0.85        30\n",
            "         1.0       0.83      0.97      0.89        35\n",
            "\n",
            "    accuracy                           0.88        65\n",
            "   macro avg       0.89      0.87      0.87        65\n",
            "weighted avg       0.89      0.88      0.87        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.47      0.60        30\n",
            "         1.0       0.67      0.91      0.77        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.75      0.69      0.68        65\n",
            "weighted avg       0.74      0.71      0.69        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.90      0.78        30\n",
            "         1.0       0.88      0.66      0.75        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.79      0.78      0.77        65\n",
            "weighted avg       0.80      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.73      0.71        30\n",
            "         1.0       0.75      0.71      0.73        34\n",
            "\n",
            "    accuracy                           0.72        64\n",
            "   macro avg       0.72      0.72      0.72        64\n",
            "weighted avg       0.72      0.72      0.72        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.532258  0.970588  0.687500       34  0.538462\n",
            "1   0.596491  0.971429  0.739130       35  0.630769\n",
            "2   0.711111  0.914286  0.800000       35  0.753846\n",
            "3   0.634615  0.942857  0.758621       35  0.676923\n",
            "4   0.711111  0.914286  0.800000       35  0.753846\n",
            "5   0.744681  1.000000  0.853659       35  0.815385\n",
            "6   0.829268  0.971429  0.894737       35  0.876923\n",
            "7   0.666667  0.914286  0.771084       35  0.707692\n",
            "8   0.884615  0.657143  0.754098       35  0.769231\n",
            "9   0.750000  0.705882  0.727273       34  0.718750\n",
            "c =  0.5\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.06      0.12        31\n",
            "         1.0       0.54      1.00      0.70        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.77      0.53      0.41        65\n",
            "weighted avg       0.76      0.55      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.17      0.28        30\n",
            "         1.0       0.58      0.97      0.72        35\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.70      0.57      0.50        65\n",
            "weighted avg       0.69      0.60      0.52        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.47      0.61        30\n",
            "         1.0       0.67      0.94      0.79        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.77      0.70      0.70        65\n",
            "weighted avg       0.77      0.72      0.70        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.30      0.44        30\n",
            "         1.0       0.61      0.94      0.74        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.71      0.62      0.59        65\n",
            "weighted avg       0.71      0.65      0.60        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.53      0.68        30\n",
            "         1.0       0.71      0.97      0.82        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.82      0.75      0.75        65\n",
            "weighted avg       0.82      0.77      0.76        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.60      0.75        30\n",
            "         1.0       0.74      1.00      0.85        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.87      0.80      0.80        65\n",
            "weighted avg       0.86      0.82      0.81        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.77      0.87        30\n",
            "         1.0       0.83      1.00      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.92      0.88      0.89        65\n",
            "weighted avg       0.91      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.47      0.60        30\n",
            "         1.0       0.67      0.91      0.77        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.75      0.69      0.68        65\n",
            "weighted avg       0.74      0.71      0.69        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.83      0.76        30\n",
            "         1.0       0.83      0.69      0.75        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.76      0.75        65\n",
            "weighted avg       0.77      0.75      0.75        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.70      0.69        30\n",
            "         1.0       0.73      0.71      0.72        34\n",
            "\n",
            "    accuracy                           0.70        64\n",
            "   macro avg       0.70      0.70      0.70        64\n",
            "weighted avg       0.70      0.70      0.70        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.539683  1.000000  0.701031       34  0.553846\n",
            "1   0.576271  0.971429  0.723404       35  0.600000\n",
            "2   0.673469  0.942857  0.785714       35  0.723077\n",
            "3   0.611111  0.942857  0.741573       35  0.646154\n",
            "4   0.708333  0.971429  0.819277       35  0.769231\n",
            "5   0.744681  1.000000  0.853659       35  0.815385\n",
            "6   0.833333  1.000000  0.909091       35  0.892308\n",
            "7   0.666667  0.914286  0.771084       35  0.707692\n",
            "8   0.827586  0.685714  0.750000       35  0.753846\n",
            "9   0.727273  0.705882  0.716418       34  0.703125\n",
            "c =  0.75\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.06      0.12        31\n",
            "         1.0       0.54      1.00      0.70        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.77      0.53      0.41        65\n",
            "weighted avg       0.76      0.55      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.17      0.28        30\n",
            "         1.0       0.58      0.97      0.72        35\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.70      0.57      0.50        65\n",
            "weighted avg       0.69      0.60      0.52        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.43      0.59        30\n",
            "         1.0       0.67      0.97      0.79        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.80      0.70      0.69        65\n",
            "weighted avg       0.79      0.72      0.70        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.30      0.44        30\n",
            "         1.0       0.61      0.94      0.74        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.71      0.62      0.59        65\n",
            "weighted avg       0.71      0.65      0.60        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.50      0.65        30\n",
            "         1.0       0.69      0.97      0.81        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.82      0.74      0.73        65\n",
            "weighted avg       0.81      0.75      0.74        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.57      0.72        30\n",
            "         1.0       0.73      1.00      0.84        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.86      0.78      0.78        65\n",
            "weighted avg       0.85      0.80      0.79        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.77      0.87        30\n",
            "         1.0       0.83      1.00      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.92      0.88      0.89        65\n",
            "weighted avg       0.91      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.43      0.57        30\n",
            "         1.0       0.65      0.91      0.76        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.73      0.67      0.66        65\n",
            "weighted avg       0.73      0.69      0.67        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.80      0.75        30\n",
            "         1.0       0.81      0.71      0.76        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.76      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.67      0.67        30\n",
            "         1.0       0.71      0.71      0.71        34\n",
            "\n",
            "    accuracy                           0.69        64\n",
            "   macro avg       0.69      0.69      0.69        64\n",
            "weighted avg       0.69      0.69      0.69        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.539683  1.000000  0.701031       34  0.553846\n",
            "1   0.576271  0.971429  0.723404       35  0.600000\n",
            "2   0.666667  0.971429  0.790698       35  0.723077\n",
            "3   0.611111  0.942857  0.741573       35  0.646154\n",
            "4   0.693878  0.971429  0.809524       35  0.753846\n",
            "5   0.729167  1.000000  0.843373       35  0.800000\n",
            "6   0.833333  1.000000  0.909091       35  0.892308\n",
            "7   0.653061  0.914286  0.761905       35  0.692308\n",
            "8   0.806452  0.714286  0.757576       35  0.753846\n",
            "9   0.705882  0.705882  0.705882       34  0.687500\n",
            "c =  1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.06      0.12        31\n",
            "         1.0       0.54      1.00      0.70        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.77      0.53      0.41        65\n",
            "weighted avg       0.76      0.55      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.17      0.28        30\n",
            "         1.0       0.58      0.97      0.72        35\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.70      0.57      0.50        65\n",
            "weighted avg       0.69      0.60      0.52        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.40      0.57        30\n",
            "         1.0       0.66      1.00      0.80        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.83      0.70      0.68        65\n",
            "weighted avg       0.82      0.72      0.69        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.27      0.40        30\n",
            "         1.0       0.60      0.94      0.73        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.70      0.60      0.57        65\n",
            "weighted avg       0.69      0.63      0.58        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.43      0.59        30\n",
            "         1.0       0.67      0.97      0.79        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.80      0.70      0.69        65\n",
            "weighted avg       0.79      0.72      0.70        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.57      0.72        30\n",
            "         1.0       0.73      1.00      0.84        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.86      0.78      0.78        65\n",
            "weighted avg       0.85      0.80      0.79        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.73      0.85        30\n",
            "         1.0       0.81      1.00      0.90        35\n",
            "\n",
            "    accuracy                           0.88        65\n",
            "   macro avg       0.91      0.87      0.87        65\n",
            "weighted avg       0.90      0.88      0.87        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.40      0.53        30\n",
            "         1.0       0.64      0.91      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.72      0.66      0.64        65\n",
            "weighted avg       0.71      0.68      0.65        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.77      0.75        30\n",
            "         1.0       0.79      0.77      0.78        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.77      0.77      0.77        65\n",
            "weighted avg       0.77      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.63      0.64        30\n",
            "         1.0       0.69      0.71      0.70        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.67      0.67      0.67        64\n",
            "weighted avg       0.67      0.67      0.67        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.539683  1.000000  0.701031       34  0.553846\n",
            "1   0.576271  0.971429  0.723404       35  0.600000\n",
            "2   0.660377  1.000000  0.795455       35  0.723077\n",
            "3   0.600000  0.942857  0.733333       35  0.630769\n",
            "4   0.666667  0.971429  0.790698       35  0.723077\n",
            "5   0.729167  1.000000  0.843373       35  0.800000\n",
            "6   0.813953  1.000000  0.897436       35  0.876923\n",
            "7   0.640000  0.914286  0.752941       35  0.676923\n",
            "8   0.794118  0.771429  0.782609       35  0.769231\n",
            "9   0.685714  0.705882  0.695652       34  0.671875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Hx8mYEyVbZ",
        "colab_type": "text"
      },
      "source": [
        "Resultado dos k-fold para cada valor de c "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACFrA5kCyVba",
        "colab_type": "code",
        "outputId": "9bd0efb9-91e9-48a5-9836-8e29d0ee1deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "result_recursive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)     method\n",
              "  0   0.557377  1.000000  0.715789       34  0.584615     0.001  Recursive\n",
              "  1   0.640000  0.914286  0.752941       35  0.676923     0.001  Recursive\n",
              "  2   0.761905  0.914286  0.831169       35  0.800000     0.001  Recursive\n",
              "  3   0.636364  0.800000  0.708861       35  0.646154     0.001  Recursive\n",
              "  4   0.727273  0.914286  0.810127       35  0.769231     0.001  Recursive\n",
              "  5   0.761905  0.914286  0.831169       35  0.800000     0.001  Recursive\n",
              "  6   0.850000  0.971429  0.906667       35  0.892308     0.001  Recursive\n",
              "  7   0.644444  0.828571  0.725000       35  0.661538     0.001  Recursive\n",
              "  8   0.857143  0.685714  0.761905       35  0.769231     0.001  Recursive\n",
              "  9   0.750000  0.705882  0.727273       34  0.718750     0.001  Recursive],\n",
              " [0.1,    precision    recall  f1-score  support  accuracy  Param(c)     method\n",
              "  0   0.532258  0.970588  0.687500       34  0.538462       0.1  Recursive\n",
              "  1   0.629630  0.971429  0.764045       35  0.676923       0.1  Recursive\n",
              "  2   0.711111  0.914286  0.800000       35  0.753846       0.1  Recursive\n",
              "  3   0.640000  0.914286  0.752941       35  0.676923       0.1  Recursive\n",
              "  4   0.738095  0.885714  0.805195       35  0.769231       0.1  Recursive\n",
              "  5   0.772727  0.971429  0.860759       35  0.830769       0.1  Recursive\n",
              "  6   0.850000  0.971429  0.906667       35  0.892308       0.1  Recursive\n",
              "  7   0.652174  0.857143  0.740741       35  0.676923       0.1  Recursive\n",
              "  8   0.916667  0.628571  0.745763       35  0.769231       0.1  Recursive\n",
              "  9   0.727273  0.705882  0.716418       34  0.703125       0.1  Recursive],\n",
              " [0.25,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)     method\n",
              "  0   0.532258  0.970588  0.687500       34  0.538462      0.25  Recursive\n",
              "  1   0.596491  0.971429  0.739130       35  0.630769      0.25  Recursive\n",
              "  2   0.711111  0.914286  0.800000       35  0.753846      0.25  Recursive\n",
              "  3   0.634615  0.942857  0.758621       35  0.676923      0.25  Recursive\n",
              "  4   0.711111  0.914286  0.800000       35  0.753846      0.25  Recursive\n",
              "  5   0.744681  1.000000  0.853659       35  0.815385      0.25  Recursive\n",
              "  6   0.829268  0.971429  0.894737       35  0.876923      0.25  Recursive\n",
              "  7   0.666667  0.914286  0.771084       35  0.707692      0.25  Recursive\n",
              "  8   0.884615  0.657143  0.754098       35  0.769231      0.25  Recursive\n",
              "  9   0.750000  0.705882  0.727273       34  0.718750      0.25  Recursive],\n",
              " [0.5,    precision    recall  f1-score  support  accuracy  Param(c)     method\n",
              "  0   0.539683  1.000000  0.701031       34  0.553846       0.5  Recursive\n",
              "  1   0.576271  0.971429  0.723404       35  0.600000       0.5  Recursive\n",
              "  2   0.673469  0.942857  0.785714       35  0.723077       0.5  Recursive\n",
              "  3   0.611111  0.942857  0.741573       35  0.646154       0.5  Recursive\n",
              "  4   0.708333  0.971429  0.819277       35  0.769231       0.5  Recursive\n",
              "  5   0.744681  1.000000  0.853659       35  0.815385       0.5  Recursive\n",
              "  6   0.833333  1.000000  0.909091       35  0.892308       0.5  Recursive\n",
              "  7   0.666667  0.914286  0.771084       35  0.707692       0.5  Recursive\n",
              "  8   0.827586  0.685714  0.750000       35  0.753846       0.5  Recursive\n",
              "  9   0.727273  0.705882  0.716418       34  0.703125       0.5  Recursive],\n",
              " [0.75,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)     method\n",
              "  0   0.539683  1.000000  0.701031       34  0.553846      0.75  Recursive\n",
              "  1   0.576271  0.971429  0.723404       35  0.600000      0.75  Recursive\n",
              "  2   0.666667  0.971429  0.790698       35  0.723077      0.75  Recursive\n",
              "  3   0.611111  0.942857  0.741573       35  0.646154      0.75  Recursive\n",
              "  4   0.693878  0.971429  0.809524       35  0.753846      0.75  Recursive\n",
              "  5   0.729167  1.000000  0.843373       35  0.800000      0.75  Recursive\n",
              "  6   0.833333  1.000000  0.909091       35  0.892308      0.75  Recursive\n",
              "  7   0.653061  0.914286  0.761905       35  0.692308      0.75  Recursive\n",
              "  8   0.806452  0.714286  0.757576       35  0.753846      0.75  Recursive\n",
              "  9   0.705882  0.705882  0.705882       34  0.687500      0.75  Recursive],\n",
              " [1,    precision    recall  f1-score  support  accuracy  Param(c)     method\n",
              "  0   0.539683  1.000000  0.701031       34  0.553846         1  Recursive\n",
              "  1   0.576271  0.971429  0.723404       35  0.600000         1  Recursive\n",
              "  2   0.660377  1.000000  0.795455       35  0.723077         1  Recursive\n",
              "  3   0.600000  0.942857  0.733333       35  0.630769         1  Recursive\n",
              "  4   0.666667  0.971429  0.790698       35  0.723077         1  Recursive\n",
              "  5   0.729167  1.000000  0.843373       35  0.800000         1  Recursive\n",
              "  6   0.813953  1.000000  0.897436       35  0.876923         1  Recursive\n",
              "  7   0.640000  0.914286  0.752941       35  0.676923         1  Recursive\n",
              "  8   0.794118  0.771429  0.782609       35  0.769231         1  Recursive\n",
              "  9   0.685714  0.705882  0.695652       34  0.671875         1  Recursive]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdJkXuJPyVbe",
        "colab_type": "text"
      },
      "source": [
        "Calcula a média das medidas de cada parâmetro c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acLGXhWCyVbf",
        "colab_type": "code",
        "outputId": "6893a514-7509-4ecf-ddec-ab481aa227f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "result_recursive_mean = calcula_media(result_recursive)\n",
        "result_recursive_mean"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.718641</td>\n",
              "      <td>0.864874</td>\n",
              "      <td>0.777090</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.731875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.716993</td>\n",
              "      <td>0.879076</td>\n",
              "      <td>0.778003</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.728774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250</td>\n",
              "      <td>0.706082</td>\n",
              "      <td>0.896218</td>\n",
              "      <td>0.778610</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.724183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.690841</td>\n",
              "      <td>0.913445</td>\n",
              "      <td>0.777125</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.716466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.681550</td>\n",
              "      <td>0.919160</td>\n",
              "      <td>0.774406</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.710288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.670595</td>\n",
              "      <td>0.927731</td>\n",
              "      <td>0.771593</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.702572</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "0  0.001        0.718641  ...          34.8       0.731875\n",
              "1  0.100        0.716993  ...          34.8       0.728774\n",
              "2  0.250        0.706082  ...          34.8       0.724183\n",
              "3  0.500        0.690841  ...          34.8       0.716466\n",
              "4  0.750        0.681550  ...          34.8       0.710288\n",
              "5  1.000        0.670595  ...          34.8       0.702572\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68ZPmfziyVbk",
        "colab_type": "text"
      },
      "source": [
        "Obtém o resultado da maior média de acurácia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2q7ZKnsyVbl",
        "colab_type": "code",
        "outputId": "fd76d5db-0616-4cc6-c54d-02b8a576ddcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "best_accuracy_recursive = pd.Series(result_recursive_mean.iloc[result_recursive_mean['accuracy_mean'].idxmax()], \n",
        "                          name='Recursive Feature')\n",
        "best_recursive = pd.DataFrame(best_accuracy_recursive)\n",
        "best_recursive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Recursive Feature</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_mean</th>\n",
              "      <td>0.718641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_mean</th>\n",
              "      <td>0.864874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score_mean</th>\n",
              "      <td>0.777090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support_mean</th>\n",
              "      <td>34.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_mean</th>\n",
              "      <td>0.731875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Recursive Feature\n",
              "c                        0.001000\n",
              "precision_mean           0.718641\n",
              "recall_mean              0.864874\n",
              "f1_score_mean            0.777090\n",
              "support_mean            34.800000\n",
              "accuracy_mean            0.731875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zd_HCB-yVbr",
        "colab_type": "text"
      },
      "source": [
        "# Junta todos os resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdvIGYH5yVbs",
        "colab_type": "code",
        "outputId": "40e56508-bffd-41c8-f82b-e36d3263922f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "result = pd.concat([best_all_features, best_pca, best_chi, best_recursive], axis=1)\n",
        "\n",
        "print(\"Média das métricas geradas pelo processamento de cada dataset\")\n",
        "result.transpose()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Média das métricas geradas pelo processamento de cada dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>All Features</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.733687</td>\n",
              "      <td>0.781597</td>\n",
              "      <td>0.739795</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.714880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PCA</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.680531</td>\n",
              "      <td>0.870504</td>\n",
              "      <td>0.749448</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.687188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Chi Squared</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.756781</td>\n",
              "      <td>0.815966</td>\n",
              "      <td>0.768260</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.741058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Recursive Feature</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.718641</td>\n",
              "      <td>0.864874</td>\n",
              "      <td>0.777090</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.731875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "All Features       0.100        0.733687  ...          34.8       0.714880\n",
              "PCA                0.500        0.680531  ...          34.8       0.687188\n",
              "Chi Squared        0.001        0.756781  ...          34.8       0.741058\n",
              "Recursive Feature  0.001        0.718641  ...          34.8       0.731875\n",
              "\n",
              "[4 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nPWiXY29cEu",
        "colab_type": "text"
      },
      "source": [
        "# Gerar Graficos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFhU-Fp2yVbw",
        "colab_type": "code",
        "outputId": "00f7a24a-a643-422f-bb18-15d720146840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        }
      },
      "source": [
        "frames = result_all_features_g +  result_pca_g + result_chi_g + result_recursive_g\n",
        "\n",
        "tips = pd.concat(frames)\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "fig, ax = plt.subplots(3,1,figsize=[50,25])\n",
        "\n",
        "g = sns.barplot(ax=ax[0], x=\"method\", y=\"accuracy\", hue=\"Param(c)\", data=tips,capsize=.05)\n",
        "g.set(ylim=(0,1),yticks=np.arange(0,1.1,0.1).tolist())\n",
        "g.set_ylabel(\"Accuracy\",fontsize=30)\n",
        "g.set_title('Classificador Naive Bayes',fontsize=30)\n",
        "g = sns.barplot(ax=ax[1], x=\"method\", y=\"recall\", hue=\"Param(c)\", data=tips,capsize=.05);\n",
        "g.set(ylim=(0,1),yticks=np.arange(0,1.1,0.1).tolist())\n",
        "g.set_ylabel(\"Recall\",fontsize=30)\n",
        "g = sns.barplot(ax=ax[2], x=\"method\", y=\"precision\", hue=\"Param(c)\", data=tips,capsize=.05);\n",
        "g.set(ylim=(0,1),yticks=np.arange(0,1.1,0.1).tolist())\n",
        "g.set_ylabel(\"Precision\",fontsize=30)\n",
        "plt.close(2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 3600x1800 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}