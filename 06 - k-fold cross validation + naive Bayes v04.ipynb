{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of 06 - k-fold cross validation + naive Bayes v03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlaprv/sin5007-reconhecimento-de-padroes/blob/master/06%20-%20k-fold%20cross%20validation%20%2B%20naive%20Bayes%20v04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikayTX9-yVZk",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "-------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiZnnw89yVZm",
        "colab_type": "text"
      },
      "source": [
        "# Bibliotecas Necessárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xYCeWYlyVZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np\n",
        "import seaborn as sns # visualize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRJncBBWyVZt",
        "colab_type": "text"
      },
      "source": [
        "# Funções Auxiliares\n",
        "\n",
        "describe_dataset() : realiza o cálculo das proporções de classes do dataset original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KutB_XTYyVZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def describe_dataset(X, y, k):\n",
        "    # get dataset rows: instances , columns: features\n",
        "    rows, columns = X.shape\n",
        "    # get proportion from target\n",
        "    (unique, counts) = np.unique(y, return_counts=True) \n",
        "    # calculate proportion\n",
        "    prop_neg = int(counts[0]/rows*100)\n",
        "    prop_pos = int(counts[1]/rows*100)\n",
        "\n",
        "    print(\"k = {}, Dataset: {} positivas, {} negativas ({}% x {}%)\".format(k, counts[1], counts[0], prop_pos, prop_neg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z4wMlsnyVZz",
        "colab_type": "text"
      },
      "source": [
        "get_classes_from_index() : realiza o cálculo das proporções de classes dos folds criados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEZ4jxJKyVZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classes_from_index(y, skf):\n",
        "    _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)\n",
        "    y_counts = np.bincount(y_inv)\n",
        "    _, class_perm = np.unique(y_idx, return_inverse=True)\n",
        "    y_encoded = class_perm[y_inv]\n",
        "    y_order = np.sort(y_encoded)\n",
        "    n_classes = len(y_idx)\n",
        "    allocation = np.asarray(\n",
        "            [np.bincount(y_order[i::skf.n_splits], minlength=n_classes)\n",
        "             for i in range(skf.n_splits)])\n",
        "\n",
        "    for idx, f in enumerate(allocation):\n",
        "        count_neg = int(f[0])\n",
        "        count_pos = int(f[1])\n",
        "        total = count_neg+count_pos\n",
        "        prop_temp_neg = int(count_neg/total*100)\n",
        "        prop_temp_pos = int(count_pos/total*100)\n",
        "        print(\"Fold {}: Pos: {}, Neg: {}, Total: {}, Proporção: {}% x {}%\".format(idx, count_pos, count_neg, total, prop_temp_pos, prop_temp_neg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RsPfCv4yVZ4",
        "colab_type": "text"
      },
      "source": [
        "# Função que aplica o Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YezZQsp7yVZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stratified_k_fold(X, y, list_c, k, name):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------    \n",
        "       X : array-like, shape (n_samples, n_features)\n",
        "           Training data, where n_samples is the number of samples\n",
        "           and n_features is the number of features.\n",
        "       y : array-like, of length n_samples\n",
        "           The target variable for supervised learning problems.\n",
        "       k : int\n",
        "           Determines the number of folds.\n",
        "    name : method selection (string)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### Estratifica o dataset em k folds\n",
        "    skf = StratifiedKFold(n_splits=k)\n",
        "    describe_dataset(X, y, k)\n",
        "    get_classes_from_index(y, skf)\n",
        "    \n",
        "    \n",
        "    ### Lista para armazenar os resultados de cada valor de c\n",
        "    ### Armazena um array bidimensional, onde terá o valor do c e uma lista dos resultados de c\n",
        "    result = []\n",
        "    reports_g = []\n",
        "    \n",
        "    \n",
        "    ### Executa o treino e teste para cada valor do parametro c\n",
        "    for c in list_c:\n",
        "        print(\"c =  {}\" .format(c))\n",
        "\n",
        "        ### create naive bayes classifier\n",
        "        clf = GaussianNB(var_smoothing = c)\n",
        "        \n",
        "        \n",
        "        ### Array para guardar os resultados dos testes para o parametro c\n",
        "        \"\"\"\n",
        "        Coluna 0 : Armazena o valor de c\n",
        "        Coluna 1 : Armazena o resultado \n",
        "        \"\"\"\n",
        "        result_c = []\n",
        "\n",
        "                \n",
        "        ### resultado do fold-k\n",
        "        result_k = []\n",
        "        ### Executa o treino e teste para k folds\n",
        "        fold_k = 1\n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            \n",
        "            print(\"fold_k: {}\" .format(fold_k))\n",
        "            print(\"\\nTRAIN: {}  TEST: {}\".format(len(train_index), len(test_index)))\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "            ### train classifier\n",
        "            clf.fit(X_train, y_train)\n",
        "            \n",
        "            ### calculate metrics\n",
        "            y_predicted = clf.predict(X_test)\n",
        "            report_dict = metrics.classification_report(y_test, y_predicted, output_dict=True)\n",
        "            report_str = metrics.classification_report(y_test, y_predicted)\n",
        "                \n",
        "            ### Armazena o resultado do test do fold-k          \n",
        "            result_k.append(report_dict)\n",
        "            print(report_str)\n",
        "            \n",
        "            fold_k = fold_k + 1\n",
        "            \n",
        "\n",
        "        ### Guarda os resultados dos k fold do parametro c\n",
        "        reports = pd.DataFrame(pd.DataFrame(result_k)['1.0'].to_list())\n",
        "        \n",
        "        accuracy_reports = pd.DataFrame(pd.DataFrame(result_k)['accuracy'])\n",
        "        reports['accuracy'] = accuracy_reports\n",
        "        print(reports)\n",
        "\n",
        "        ### Guarda o resultado para impressao grafica: reports_g\n",
        "        reports_c = reports\n",
        "        reports_c['Param(c)'] = c\n",
        "        reports_c['method'] = str(name)\n",
        "        reports_g.append(reports_c)\n",
        "                \n",
        "        ### Guarda o resultado da execução para o parâmetro c\n",
        "        result_c = [c, reports]\n",
        "        result.append(result_c)\n",
        "        \n",
        "    \n",
        "    ### Retorna a lista com todos os resultado para cada c\n",
        "    return result , reports_g\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ14ict2yVZ-",
        "colab_type": "text"
      },
      "source": [
        "# Função para calcular a média das medidas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HpefPV4yVZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calcula a média das medidas de cada c\n",
        "def calcula_media(lista_result):\n",
        "    \n",
        "    mean_c = []\n",
        "    for result in lista_result:\n",
        "        \n",
        "        c = result[0]\n",
        "        result_c = result[1]\n",
        "        \n",
        "        # Calcula a média das medidas do parametro c\n",
        "        precision_mean = result_c['precision'].mean()\n",
        "        recall_mean = result_c['recall'].mean()\n",
        "        f1_score_mean = result_c['f1-score'].mean()\n",
        "        support_mean = result_c['support'].mean()\n",
        "        accuracy_mean = result_c['accuracy'].mean()\n",
        "        \n",
        "        # Armazena a média das medidas do parametro c\n",
        "        mean_c.append([c, precision_mean, recall_mean, f1_score_mean, support_mean, accuracy_mean])\n",
        "    \n",
        "    name_columns = ['c', 'precision_mean', 'recall_mean', 'f1_score_mean', 'support_mean', 'accuracy_mean']\n",
        "    mean_c = pd.DataFrame(mean_c, columns=name_columns)\n",
        "    return mean_c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OefDdR7WyVaD",
        "colab_type": "text"
      },
      "source": [
        "##### Parâmetros de execução do Naive Bayes\n",
        "list_c : valores do parâmetro de ajuste de probabilidade \n",
        "\n",
        "k_folds : número de folds para a estratificação do dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mss4hXMgyVaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_c = [0.001, 0.10, 0.25, 0.50, 0.75, 1]\n",
        "k_folds = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCMHkna8yVaJ",
        "colab_type": "text"
      },
      "source": [
        "# Execução base: Todas as características"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSyYoDy_yVaK",
        "colab_type": "code",
        "outputId": "2d55ee06-ee63-4bd8-bac6-aef79a14a069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df = pd.read_csv('results/dataset-normalizado.csv', header = 0)\n",
        "X = df.drop('is_approved', axis=1).to_numpy() # DATASET\n",
        "y = df['is_approved'].to_numpy() # target\n",
        "result_all_features , result_all_features_g = stratified_k_fold(X, y, list_c, k=k_folds, name='All Features')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 10, Dataset: 348 positivas, 301 negativas (53% x 46%)\n",
            "Fold 0: Pos: 34, Neg: 31, Total: 65, Proporção: 52% x 47%\n",
            "Fold 1: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 2: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 3: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 4: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 5: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 6: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 7: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 8: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 9: Pos: 34, Neg: 30, Total: 64, Proporção: 53% x 46%\n",
            "c =  0.001\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.13      0.22        31\n",
            "         1.0       0.55      0.97      0.70        34\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.68      0.55      0.46        65\n",
            "weighted avg       0.67      0.57      0.47        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.40      0.51        30\n",
            "         1.0       0.62      0.86      0.72        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.67      0.63      0.62        65\n",
            "weighted avg       0.66      0.65      0.62        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.70      0.74        30\n",
            "         1.0       0.76      0.83      0.79        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.77      0.76      0.77        65\n",
            "weighted avg       0.77      0.77      0.77        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.53      0.60        30\n",
            "         1.0       0.67      0.80      0.73        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.63      0.68        30\n",
            "         1.0       0.72      0.80      0.76        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.72      0.72      0.72        65\n",
            "weighted avg       0.72      0.72      0.72        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.70      0.76        30\n",
            "         1.0       0.78      0.89      0.83        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.81      0.79      0.80        65\n",
            "weighted avg       0.81      0.80      0.80        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.80      0.86        30\n",
            "         1.0       0.85      0.94      0.89        35\n",
            "\n",
            "    accuracy                           0.88        65\n",
            "   macro avg       0.88      0.87      0.87        65\n",
            "weighted avg       0.88      0.88      0.88        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.63      0.63        30\n",
            "         1.0       0.69      0.69      0.69        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.66      0.66      0.66        65\n",
            "weighted avg       0.66      0.66      0.66        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.93      0.73        30\n",
            "         1.0       0.89      0.46      0.60        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.74      0.70      0.67        65\n",
            "weighted avg       0.75      0.68      0.66        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.77      0.68        30\n",
            "         1.0       0.73      0.56      0.63        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.67      0.66      0.65        64\n",
            "weighted avg       0.67      0.66      0.65        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.550000  0.970588  0.702128       34  0.569231\n",
            "1   0.625000  0.857143  0.722892       35  0.646154\n",
            "2   0.763158  0.828571  0.794521       35  0.769231\n",
            "3   0.666667  0.800000  0.727273       35  0.676923\n",
            "4   0.717949  0.800000  0.756757       35  0.723077\n",
            "5   0.775000  0.885714  0.826667       35  0.800000\n",
            "6   0.846154  0.942857  0.891892       35  0.876923\n",
            "7   0.685714  0.685714  0.685714       35  0.661538\n",
            "8   0.888889  0.457143  0.603774       35  0.676923\n",
            "9   0.730769  0.558824  0.633333       34  0.656250\n",
            "c =  0.1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.16      0.27        31\n",
            "         1.0       0.56      0.97      0.71        34\n",
            "\n",
            "    accuracy                           0.58        65\n",
            "   macro avg       0.70      0.57      0.49        65\n",
            "weighted avg       0.69      0.58      0.50        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.40      0.53        30\n",
            "         1.0       0.64      0.91      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.72      0.66      0.64        65\n",
            "weighted avg       0.71      0.68      0.65        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.67      0.71        30\n",
            "         1.0       0.74      0.83      0.78        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67        30\n",
            "         1.0       0.71      0.77      0.74        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.71        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.80      0.81        30\n",
            "         1.0       0.83      0.86      0.85        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.83      0.83      0.83        65\n",
            "weighted avg       0.83      0.83      0.83        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.87      0.90        30\n",
            "         1.0       0.89      0.94      0.92        35\n",
            "\n",
            "    accuracy                           0.91        65\n",
            "   macro avg       0.91      0.90      0.91        65\n",
            "weighted avg       0.91      0.91      0.91        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.70      0.68        30\n",
            "         1.0       0.73      0.69      0.71        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.69      0.69      0.69        65\n",
            "weighted avg       0.69      0.69      0.69        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.90      0.69        30\n",
            "         1.0       0.82      0.40      0.54        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.69      0.65      0.62        65\n",
            "weighted avg       0.70      0.63      0.61        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.77      0.69        30\n",
            "         1.0       0.74      0.59      0.66        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.68      0.68      0.67        64\n",
            "weighted avg       0.68      0.67      0.67        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.559322  0.970588  0.709677       34  0.584615\n",
            "1   0.640000  0.914286  0.752941       35  0.676923\n",
            "2   0.743590  0.828571  0.783784       35  0.753846\n",
            "3   0.666667  0.857143  0.750000       35  0.692308\n",
            "4   0.710526  0.771429  0.739726       35  0.707692\n",
            "5   0.833333  0.857143  0.845070       35  0.830769\n",
            "6   0.891892  0.942857  0.916667       35  0.907692\n",
            "7   0.727273  0.685714  0.705882       35  0.692308\n",
            "8   0.823529  0.400000  0.538462       35  0.630769\n",
            "9   0.740741  0.588235  0.655738       34  0.671875\n",
            "c =  0.25\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.13      0.22        31\n",
            "         1.0       0.55      0.97      0.70        34\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.68      0.55      0.46        65\n",
            "weighted avg       0.67      0.57      0.47        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.37      0.51        30\n",
            "         1.0       0.63      0.94      0.76        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.74      0.65      0.64        65\n",
            "weighted avg       0.73      0.68      0.64        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.67      0.73        30\n",
            "         1.0       0.75      0.86      0.80        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.78      0.76      0.76        65\n",
            "weighted avg       0.77      0.77      0.77        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.60      0.63        30\n",
            "         1.0       0.68      0.74      0.71        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.68        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.80      0.83        30\n",
            "         1.0       0.84      0.89      0.86        35\n",
            "\n",
            "    accuracy                           0.85        65\n",
            "   macro avg       0.85      0.84      0.84        65\n",
            "weighted avg       0.85      0.85      0.85        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.83      0.88        30\n",
            "         1.0       0.87      0.94      0.90        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.90      0.89      0.89        65\n",
            "weighted avg       0.89      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.67      0.69        30\n",
            "         1.0       0.73      0.77      0.75        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.72      0.72      0.72        65\n",
            "weighted avg       0.72      0.72      0.72        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.55      0.90      0.68        30\n",
            "         1.0       0.81      0.37      0.51        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.68      0.64      0.60        65\n",
            "weighted avg       0.69      0.62      0.59        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.73      0.67        30\n",
            "         1.0       0.71      0.59      0.65        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.67      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.550000  0.970588  0.702128       34  0.569231\n",
            "1   0.634615  0.942857  0.758621       35  0.676923\n",
            "2   0.750000  0.857143  0.800000       35  0.769231\n",
            "3   0.666667  0.857143  0.750000       35  0.692308\n",
            "4   0.684211  0.742857  0.712329       35  0.676923\n",
            "5   0.837838  0.885714  0.861111       35  0.846154\n",
            "6   0.868421  0.942857  0.904110       35  0.892308\n",
            "7   0.729730  0.771429  0.750000       35  0.723077\n",
            "8   0.812500  0.371429  0.509804       35  0.615385\n",
            "9   0.714286  0.588235  0.645161       34  0.656250\n",
            "c =  0.5\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.13      0.22        31\n",
            "         1.0       0.55      0.97      0.70        34\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.68      0.55      0.46        65\n",
            "weighted avg       0.67      0.57      0.47        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.30      0.45        30\n",
            "         1.0       0.62      0.97      0.76        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.76      0.64      0.60        65\n",
            "weighted avg       0.75      0.66      0.61        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.63      0.70        30\n",
            "         1.0       0.73      0.86      0.79        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.43      0.54        30\n",
            "         1.0       0.64      0.86      0.73        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.68      0.65      0.64        65\n",
            "weighted avg       0.68      0.66      0.64        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.57      0.62        30\n",
            "         1.0       0.68      0.77      0.72        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.80      0.84        30\n",
            "         1.0       0.84      0.91      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.86      0.86        65\n",
            "weighted avg       0.86      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.87      0.90        30\n",
            "         1.0       0.89      0.94      0.92        35\n",
            "\n",
            "    accuracy                           0.91        65\n",
            "   macro avg       0.91      0.90      0.91        65\n",
            "weighted avg       0.91      0.91      0.91        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.60      0.64        30\n",
            "         1.0       0.69      0.77      0.73        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.69      0.69      0.69        65\n",
            "weighted avg       0.69      0.69      0.69        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.55      0.87      0.68        30\n",
            "         1.0       0.78      0.40      0.53        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.67      0.63      0.60        65\n",
            "weighted avg       0.67      0.62      0.60        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.550000  0.970588  0.702128       34  0.569231\n",
            "1   0.618182  0.971429  0.755556       35  0.661538\n",
            "2   0.731707  0.857143  0.789474       35  0.753846\n",
            "3   0.638298  0.857143  0.731707       35  0.661538\n",
            "4   0.675000  0.771429  0.720000       35  0.676923\n",
            "5   0.842105  0.914286  0.876712       35  0.861538\n",
            "6   0.891892  0.942857  0.916667       35  0.907692\n",
            "7   0.692308  0.771429  0.729730       35  0.692308\n",
            "8   0.777778  0.400000  0.528302       35  0.615385\n",
            "9   0.700000  0.617647  0.656250       34  0.656250\n",
            "c =  0.75\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.13      0.22        31\n",
            "         1.0       0.55      0.97      0.70        34\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.68      0.55      0.46        65\n",
            "weighted avg       0.67      0.57      0.47        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.23      0.37        30\n",
            "         1.0       0.60      0.97      0.74        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.74      0.60      0.55        65\n",
            "weighted avg       0.73      0.63      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.63      0.70        30\n",
            "         1.0       0.73      0.86      0.79        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.37      0.48        30\n",
            "         1.0       0.61      0.86      0.71        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.65      0.61      0.60        65\n",
            "weighted avg       0.65      0.63      0.61        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.57      0.62        30\n",
            "         1.0       0.68      0.77      0.72        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.80      0.87        30\n",
            "         1.0       0.85      0.97      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.91      0.89      0.89        65\n",
            "weighted avg       0.90      0.89      0.89        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.90      0.93        30\n",
            "         1.0       0.92      0.97      0.94        35\n",
            "\n",
            "    accuracy                           0.94        65\n",
            "   macro avg       0.94      0.94      0.94        65\n",
            "weighted avg       0.94      0.94      0.94        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.60      0.65        30\n",
            "         1.0       0.70      0.80      0.75        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.70        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.58      0.87      0.69        30\n",
            "         1.0       0.80      0.46      0.58        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.69      0.66      0.64        65\n",
            "weighted avg       0.70      0.65      0.63        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.550000  0.970588  0.702128       34  0.569231\n",
            "1   0.596491  0.971429  0.739130       35  0.630769\n",
            "2   0.731707  0.857143  0.789474       35  0.753846\n",
            "3   0.612245  0.857143  0.714286       35  0.630769\n",
            "4   0.675000  0.771429  0.720000       35  0.676923\n",
            "5   0.850000  0.971429  0.906667       35  0.892308\n",
            "6   0.918919  0.971429  0.944444       35  0.938462\n",
            "7   0.700000  0.800000  0.746667       35  0.707692\n",
            "8   0.800000  0.457143  0.581818       35  0.646154\n",
            "9   0.700000  0.617647  0.656250       34  0.656250\n",
            "c =  1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.23      0.37        30\n",
            "         1.0       0.60      0.97      0.74        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.74      0.60      0.55        65\n",
            "weighted avg       0.73      0.63      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.60      0.68        30\n",
            "         1.0       0.71      0.86      0.78        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.75      0.73      0.73        65\n",
            "weighted avg       0.75      0.74      0.73        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.33      0.44        30\n",
            "         1.0       0.60      0.86      0.71        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.63      0.60      0.58        65\n",
            "weighted avg       0.63      0.62      0.59        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.53      0.60        30\n",
            "         1.0       0.67      0.80      0.73        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.77      0.85        30\n",
            "         1.0       0.83      0.97      0.89        35\n",
            "\n",
            "    accuracy                           0.88        65\n",
            "   macro avg       0.89      0.87      0.87        65\n",
            "weighted avg       0.89      0.88      0.87        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.87      0.91        30\n",
            "         1.0       0.89      0.97      0.93        35\n",
            "\n",
            "    accuracy                           0.92        65\n",
            "   macro avg       0.93      0.92      0.92        65\n",
            "weighted avg       0.93      0.92      0.92        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.53      0.62        30\n",
            "         1.0       0.67      0.83      0.74        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.70      0.68      0.68        65\n",
            "weighted avg       0.70      0.69      0.68        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.87      0.72        30\n",
            "         1.0       0.83      0.54      0.66        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.72      0.70      0.69        65\n",
            "weighted avg       0.73      0.69      0.69        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.540984  0.970588  0.694737       34  0.553846\n",
            "1   0.596491  0.971429  0.739130       35  0.630769\n",
            "2   0.714286  0.857143  0.779221       35  0.738462\n",
            "3   0.600000  0.857143  0.705882       35  0.615385\n",
            "4   0.666667  0.800000  0.727273       35  0.676923\n",
            "5   0.829268  0.971429  0.894737       35  0.876923\n",
            "6   0.894737  0.971429  0.931507       35  0.923077\n",
            "7   0.674419  0.828571  0.743590       35  0.692308\n",
            "8   0.826087  0.542857  0.655172       35  0.692308\n",
            "9   0.700000  0.617647  0.656250       34  0.656250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AymSdHvyyVaQ",
        "colab_type": "text"
      },
      "source": [
        "Resultado dos k-fold para cada valor de c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-NvZLvfyVaR",
        "colab_type": "code",
        "outputId": "f3e6617a-2a50-4970-ea78-7f87d24e76af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "result_all_features"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)        method\n",
              "  0   0.550000  0.970588  0.702128       34  0.569231     0.001  All Features\n",
              "  1   0.625000  0.857143  0.722892       35  0.646154     0.001  All Features\n",
              "  2   0.763158  0.828571  0.794521       35  0.769231     0.001  All Features\n",
              "  3   0.666667  0.800000  0.727273       35  0.676923     0.001  All Features\n",
              "  4   0.717949  0.800000  0.756757       35  0.723077     0.001  All Features\n",
              "  5   0.775000  0.885714  0.826667       35  0.800000     0.001  All Features\n",
              "  6   0.846154  0.942857  0.891892       35  0.876923     0.001  All Features\n",
              "  7   0.685714  0.685714  0.685714       35  0.661538     0.001  All Features\n",
              "  8   0.888889  0.457143  0.603774       35  0.676923     0.001  All Features\n",
              "  9   0.730769  0.558824  0.633333       34  0.656250     0.001  All Features],\n",
              " [0.1,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)        method\n",
              "  0   0.559322  0.970588  0.709677       34  0.584615       0.1  All Features\n",
              "  1   0.640000  0.914286  0.752941       35  0.676923       0.1  All Features\n",
              "  2   0.743590  0.828571  0.783784       35  0.753846       0.1  All Features\n",
              "  3   0.666667  0.857143  0.750000       35  0.692308       0.1  All Features\n",
              "  4   0.710526  0.771429  0.739726       35  0.707692       0.1  All Features\n",
              "  5   0.833333  0.857143  0.845070       35  0.830769       0.1  All Features\n",
              "  6   0.891892  0.942857  0.916667       35  0.907692       0.1  All Features\n",
              "  7   0.727273  0.685714  0.705882       35  0.692308       0.1  All Features\n",
              "  8   0.823529  0.400000  0.538462       35  0.630769       0.1  All Features\n",
              "  9   0.740741  0.588235  0.655738       34  0.671875       0.1  All Features],\n",
              " [0.25,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)        method\n",
              "  0   0.550000  0.970588  0.702128       34  0.569231      0.25  All Features\n",
              "  1   0.634615  0.942857  0.758621       35  0.676923      0.25  All Features\n",
              "  2   0.750000  0.857143  0.800000       35  0.769231      0.25  All Features\n",
              "  3   0.666667  0.857143  0.750000       35  0.692308      0.25  All Features\n",
              "  4   0.684211  0.742857  0.712329       35  0.676923      0.25  All Features\n",
              "  5   0.837838  0.885714  0.861111       35  0.846154      0.25  All Features\n",
              "  6   0.868421  0.942857  0.904110       35  0.892308      0.25  All Features\n",
              "  7   0.729730  0.771429  0.750000       35  0.723077      0.25  All Features\n",
              "  8   0.812500  0.371429  0.509804       35  0.615385      0.25  All Features\n",
              "  9   0.714286  0.588235  0.645161       34  0.656250      0.25  All Features],\n",
              " [0.5,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)        method\n",
              "  0   0.550000  0.970588  0.702128       34  0.569231       0.5  All Features\n",
              "  1   0.618182  0.971429  0.755556       35  0.661538       0.5  All Features\n",
              "  2   0.731707  0.857143  0.789474       35  0.753846       0.5  All Features\n",
              "  3   0.638298  0.857143  0.731707       35  0.661538       0.5  All Features\n",
              "  4   0.675000  0.771429  0.720000       35  0.676923       0.5  All Features\n",
              "  5   0.842105  0.914286  0.876712       35  0.861538       0.5  All Features\n",
              "  6   0.891892  0.942857  0.916667       35  0.907692       0.5  All Features\n",
              "  7   0.692308  0.771429  0.729730       35  0.692308       0.5  All Features\n",
              "  8   0.777778  0.400000  0.528302       35  0.615385       0.5  All Features\n",
              "  9   0.700000  0.617647  0.656250       34  0.656250       0.5  All Features],\n",
              " [0.75,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)        method\n",
              "  0   0.550000  0.970588  0.702128       34  0.569231      0.75  All Features\n",
              "  1   0.596491  0.971429  0.739130       35  0.630769      0.75  All Features\n",
              "  2   0.731707  0.857143  0.789474       35  0.753846      0.75  All Features\n",
              "  3   0.612245  0.857143  0.714286       35  0.630769      0.75  All Features\n",
              "  4   0.675000  0.771429  0.720000       35  0.676923      0.75  All Features\n",
              "  5   0.850000  0.971429  0.906667       35  0.892308      0.75  All Features\n",
              "  6   0.918919  0.971429  0.944444       35  0.938462      0.75  All Features\n",
              "  7   0.700000  0.800000  0.746667       35  0.707692      0.75  All Features\n",
              "  8   0.800000  0.457143  0.581818       35  0.646154      0.75  All Features\n",
              "  9   0.700000  0.617647  0.656250       34  0.656250      0.75  All Features],\n",
              " [1,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)        method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846         1  All Features\n",
              "  1   0.596491  0.971429  0.739130       35  0.630769         1  All Features\n",
              "  2   0.714286  0.857143  0.779221       35  0.738462         1  All Features\n",
              "  3   0.600000  0.857143  0.705882       35  0.615385         1  All Features\n",
              "  4   0.666667  0.800000  0.727273       35  0.676923         1  All Features\n",
              "  5   0.829268  0.971429  0.894737       35  0.876923         1  All Features\n",
              "  6   0.894737  0.971429  0.931507       35  0.923077         1  All Features\n",
              "  7   0.674419  0.828571  0.743590       35  0.692308         1  All Features\n",
              "  8   0.826087  0.542857  0.655172       35  0.692308         1  All Features\n",
              "  9   0.700000  0.617647  0.656250       34  0.656250         1  All Features]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEf_UxQYyVaX",
        "colab_type": "text"
      },
      "source": [
        "Calcula a média das medidas de cada parâmetro c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GPD7fz5yVaY",
        "colab_type": "code",
        "outputId": "7f886696-bc4d-4e5b-d326-8a37c2f46e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "result_all_features_mean = calcula_media(result_all_features)\n",
        "result_all_features_mean"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.724930</td>\n",
              "      <td>0.778655</td>\n",
              "      <td>0.734495</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.705625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.733687</td>\n",
              "      <td>0.781597</td>\n",
              "      <td>0.739795</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.714880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250</td>\n",
              "      <td>0.724827</td>\n",
              "      <td>0.793025</td>\n",
              "      <td>0.739326</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.711779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.711727</td>\n",
              "      <td>0.807395</td>\n",
              "      <td>0.740652</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.705625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.713436</td>\n",
              "      <td>0.824538</td>\n",
              "      <td>0.750086</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.710240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.704294</td>\n",
              "      <td>0.838824</td>\n",
              "      <td>0.752750</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.705625</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "0  0.001        0.724930  ...          34.8       0.705625\n",
              "1  0.100        0.733687  ...          34.8       0.714880\n",
              "2  0.250        0.724827  ...          34.8       0.711779\n",
              "3  0.500        0.711727  ...          34.8       0.705625\n",
              "4  0.750        0.713436  ...          34.8       0.710240\n",
              "5  1.000        0.704294  ...          34.8       0.705625\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elz06yqgyVad",
        "colab_type": "text"
      },
      "source": [
        "Obtém as medidas da maior média de acurácia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPHbgy6ByVae",
        "colab_type": "code",
        "outputId": "a7196248-0b0c-4ea2-d7f5-76d03785f4b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "best_accuracy_all_features = pd.Series(result_all_features_mean.iloc[result_all_features_mean['accuracy_mean'].idxmax()], \n",
        "                          name='All Features')\n",
        "best_all_features = pd.DataFrame(best_accuracy_all_features)\n",
        "best_all_features"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>All Features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_mean</th>\n",
              "      <td>0.733687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_mean</th>\n",
              "      <td>0.781597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score_mean</th>\n",
              "      <td>0.739795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support_mean</th>\n",
              "      <td>34.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_mean</th>\n",
              "      <td>0.714880</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                All Features\n",
              "c                   0.100000\n",
              "precision_mean      0.733687\n",
              "recall_mean         0.781597\n",
              "f1_score_mean       0.739795\n",
              "support_mean       34.800000\n",
              "accuracy_mean       0.714880"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXEKLi39yVaj",
        "colab_type": "text"
      },
      "source": [
        "# Execução Base: PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiTGHSiMyVak",
        "colab_type": "code",
        "outputId": "e3d9c7ec-7e47-43b3-fed5-1b5bb9414652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df = pd.read_csv('results/dataset-pca.csv', header = 0)\n",
        "X = df.drop('is_approved', axis=1).to_numpy() # DATASET\n",
        "y = df['is_approved'].to_numpy() # target\n",
        "result_pca , result_pca_g = stratified_k_fold(X, y, list_c, k=k_folds, name='PCA')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 10, Dataset: 348 positivas, 301 negativas (53% x 46%)\n",
            "Fold 0: Pos: 34, Neg: 31, Total: 65, Proporção: 52% x 47%\n",
            "Fold 1: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 2: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 3: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 4: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 5: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 6: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 7: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 8: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 9: Pos: 34, Neg: 30, Total: 64, Proporção: 53% x 46%\n",
            "c =  0.001\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.16      0.28        31\n",
            "         1.0       0.57      1.00      0.72        34\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.78      0.58      0.50        65\n",
            "weighted avg       0.77      0.60      0.51        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.33      0.47        30\n",
            "         1.0       0.62      0.91      0.74        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.69      0.62      0.60        65\n",
            "weighted avg       0.69      0.65      0.61        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.77      0.79        30\n",
            "         1.0       0.81      0.86      0.83        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.82      0.81      0.81        65\n",
            "weighted avg       0.82      0.82      0.81        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.40      0.50        30\n",
            "         1.0       0.62      0.83      0.71        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.64      0.61      0.60        65\n",
            "weighted avg       0.64      0.63      0.61        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.60      0.61        30\n",
            "         1.0       0.67      0.69      0.68        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.64      0.64      0.64        65\n",
            "weighted avg       0.65      0.65      0.65        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.80      0.79        30\n",
            "         1.0       0.82      0.80      0.81        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.80      0.80      0.80        65\n",
            "weighted avg       0.80      0.80      0.80        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.80      0.80        30\n",
            "         1.0       0.83      0.83      0.83        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.81      0.81      0.81        65\n",
            "weighted avg       0.82      0.82      0.82        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.73      0.64        30\n",
            "         1.0       0.69      0.51      0.59        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.63      0.62      0.61        65\n",
            "weighted avg       0.63      0.62      0.61        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.57      0.97      0.72        30\n",
            "         1.0       0.93      0.37      0.53        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.75      0.67      0.62        65\n",
            "weighted avg       0.76      0.65      0.62        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.83      0.67        30\n",
            "         1.0       0.74      0.41      0.53        34\n",
            "\n",
            "    accuracy                           0.61        64\n",
            "   macro avg       0.65      0.62      0.60        64\n",
            "weighted avg       0.65      0.61      0.59        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.566667  1.000000  0.723404       34  0.600000\n",
            "1   0.615385  0.914286  0.735632       35  0.646154\n",
            "2   0.810811  0.857143  0.833333       35  0.815385\n",
            "3   0.617021  0.828571  0.707317       35  0.630769\n",
            "4   0.666667  0.685714  0.676056       35  0.646154\n",
            "5   0.823529  0.800000  0.811594       35  0.800000\n",
            "6   0.828571  0.828571  0.828571       35  0.815385\n",
            "7   0.692308  0.514286  0.590164       35  0.615385\n",
            "8   0.928571  0.371429  0.530612       35  0.646154\n",
            "9   0.736842  0.411765  0.528302       34  0.609375\n",
            "c =  0.1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.06      0.12        31\n",
            "         1.0       0.54      1.00      0.70        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.77      0.53      0.41        65\n",
            "weighted avg       0.76      0.55      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.23      0.36        30\n",
            "         1.0       0.59      0.94      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.68      0.59      0.54        65\n",
            "weighted avg       0.68      0.62      0.56        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.67      0.75        30\n",
            "         1.0       0.76      0.91      0.83        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.82      0.79      0.79        65\n",
            "weighted avg       0.81      0.80      0.80        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.30      0.42        30\n",
            "         1.0       0.60      0.89      0.71        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.64      0.59      0.57        65\n",
            "weighted avg       0.64      0.62      0.58        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      0.43      0.50        30\n",
            "         1.0       0.60      0.74      0.67        35\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.60      0.59      0.58        65\n",
            "weighted avg       0.60      0.60      0.59        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.77      0.77        30\n",
            "         1.0       0.80      0.80      0.80        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.78      0.78      0.78        65\n",
            "weighted avg       0.78      0.78      0.78        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.80      0.81        30\n",
            "         1.0       0.83      0.86      0.85        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.83      0.83      0.83        65\n",
            "weighted avg       0.83      0.83      0.83        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.67      0.65        30\n",
            "         1.0       0.70      0.66      0.68        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.66      0.66      0.66        65\n",
            "weighted avg       0.66      0.66      0.66        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      0.90      0.71        30\n",
            "         1.0       0.84      0.46      0.59        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.71      0.68      0.65        65\n",
            "weighted avg       0.72      0.66      0.65        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.57      0.77      0.66        30\n",
            "         1.0       0.71      0.50      0.59        34\n",
            "\n",
            "    accuracy                           0.62        64\n",
            "   macro avg       0.64      0.63      0.62        64\n",
            "weighted avg       0.65      0.62      0.62        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.539683  1.000000  0.701031       34  0.553846\n",
            "1   0.589286  0.942857  0.725275       35  0.615385\n",
            "2   0.761905  0.914286  0.831169       35  0.800000\n",
            "3   0.596154  0.885714  0.712644       35  0.615385\n",
            "4   0.604651  0.742857  0.666667       35  0.600000\n",
            "5   0.800000  0.800000  0.800000       35  0.784615\n",
            "6   0.833333  0.857143  0.845070       35  0.830769\n",
            "7   0.696970  0.657143  0.676471       35  0.661538\n",
            "8   0.842105  0.457143  0.592593       35  0.661538\n",
            "9   0.708333  0.500000  0.586207       34  0.625000\n",
            "c =  0.25\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.03      0.06        31\n",
            "         1.0       0.53      1.00      0.69        34\n",
            "\n",
            "    accuracy                           0.54        65\n",
            "   macro avg       0.77      0.52      0.38        65\n",
            "weighted avg       0.75      0.54      0.39        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.20      0.32        30\n",
            "         1.0       0.59      0.97      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.72      0.59      0.53        65\n",
            "weighted avg       0.71      0.62      0.54        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.63      0.73        30\n",
            "         1.0       0.74      0.91      0.82        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.80      0.77      0.78        65\n",
            "weighted avg       0.80      0.78      0.78        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.20      0.31        30\n",
            "         1.0       0.57      0.91      0.70        35\n",
            "\n",
            "    accuracy                           0.58        65\n",
            "   macro avg       0.62      0.56      0.51        65\n",
            "weighted avg       0.62      0.58      0.52        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.40      0.49        30\n",
            "         1.0       0.61      0.80      0.69        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.62      0.60      0.59        65\n",
            "weighted avg       0.62      0.62      0.60        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.67      0.74        30\n",
            "         1.0       0.76      0.89      0.82        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.79      0.78      0.78        65\n",
            "weighted avg       0.79      0.78      0.78        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.73      0.80        30\n",
            "         1.0       0.80      0.91      0.85        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.84      0.82      0.83        65\n",
            "weighted avg       0.84      0.83      0.83        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.63      0.68        30\n",
            "         1.0       0.72      0.80      0.76        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.72      0.72      0.72        65\n",
            "weighted avg       0.72      0.72      0.72        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.90      0.73        30\n",
            "         1.0       0.86      0.51      0.64        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.74      0.71      0.69        65\n",
            "weighted avg       0.74      0.69      0.68        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.67      0.65        30\n",
            "         1.0       0.69      0.65      0.67        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.531250  1.000000  0.693878       34  0.538462\n",
            "1   0.586207  0.971429  0.731183       35  0.615385\n",
            "2   0.744186  0.914286  0.820513       35  0.784615\n",
            "3   0.571429  0.914286  0.703297       35  0.584615\n",
            "4   0.608696  0.800000  0.691358       35  0.615385\n",
            "5   0.756098  0.885714  0.815789       35  0.784615\n",
            "6   0.800000  0.914286  0.853333       35  0.830769\n",
            "7   0.717949  0.800000  0.756757       35  0.723077\n",
            "8   0.857143  0.514286  0.642857       35  0.692308\n",
            "9   0.687500  0.647059  0.666667       34  0.656250\n",
            "c =  0.5\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.03      0.06        31\n",
            "         1.0       0.53      1.00      0.69        34\n",
            "\n",
            "    accuracy                           0.54        65\n",
            "   macro avg       0.77      0.52      0.38        65\n",
            "weighted avg       0.75      0.54      0.39        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.20      0.32        30\n",
            "         1.0       0.59      0.97      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.72      0.59      0.53        65\n",
            "weighted avg       0.71      0.62      0.54        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.53      0.65        30\n",
            "         1.0       0.70      0.91      0.79        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.77      0.72      0.72        65\n",
            "weighted avg       0.76      0.74      0.73        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.13      0.22        30\n",
            "         1.0       0.56      0.94      0.70        35\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.61      0.54      0.46        65\n",
            "weighted avg       0.61      0.57      0.48        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.40      0.51        30\n",
            "         1.0       0.62      0.86      0.72        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.67      0.63      0.62        65\n",
            "weighted avg       0.66      0.65      0.62        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.63      0.75        30\n",
            "         1.0       0.75      0.94      0.84        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.83      0.79      0.79        65\n",
            "weighted avg       0.82      0.80      0.79        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.73      0.83        30\n",
            "         1.0       0.81      0.97      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.88      0.85      0.86        65\n",
            "weighted avg       0.88      0.86      0.86        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.53      0.63        30\n",
            "         1.0       0.68      0.86      0.76        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.72      0.70      0.69        65\n",
            "weighted avg       0.72      0.71      0.70        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.64      0.90      0.75        30\n",
            "         1.0       0.87      0.57      0.69        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.76      0.74      0.72        65\n",
            "weighted avg       0.76      0.72      0.72        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.65      0.67      0.66        30\n",
            "         1.0       0.70      0.68      0.69        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.67      0.67      0.67        64\n",
            "weighted avg       0.67      0.67      0.67        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.531250  1.000000  0.693878       34  0.538462\n",
            "1   0.586207  0.971429  0.731183       35  0.615385\n",
            "2   0.695652  0.914286  0.790123       35  0.738462\n",
            "3   0.559322  0.942857  0.702128       35  0.569231\n",
            "4   0.625000  0.857143  0.722892       35  0.646154\n",
            "5   0.750000  0.942857  0.835443       35  0.800000\n",
            "6   0.809524  0.971429  0.883117       35  0.861538\n",
            "7   0.681818  0.857143  0.759494       35  0.707692\n",
            "8   0.869565  0.571429  0.689655       35  0.723077\n",
            "9   0.696970  0.676471  0.686567       34  0.671875\n",
            "c =  0.75\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        31\n",
            "         1.0       0.52      1.00      0.69        34\n",
            "\n",
            "    accuracy                           0.52        65\n",
            "   macro avg       0.26      0.50      0.34        65\n",
            "weighted avg       0.27      0.52      0.36        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.20      0.32        30\n",
            "         1.0       0.59      0.97      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.72      0.59      0.53        65\n",
            "weighted avg       0.71      0.62      0.54        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.50      0.62        30\n",
            "         1.0       0.68      0.91      0.78        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.76      0.71      0.70        65\n",
            "weighted avg       0.75      0.72      0.71        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.13      0.22        30\n",
            "         1.0       0.56      0.94      0.70        35\n",
            "\n",
            "    accuracy                           0.57        65\n",
            "   macro avg       0.61      0.54      0.46        65\n",
            "weighted avg       0.61      0.57      0.48        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.37      0.49        30\n",
            "         1.0       0.62      0.89      0.73        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.68      0.63      0.61        65\n",
            "weighted avg       0.67      0.65      0.62        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.60      0.72        30\n",
            "         1.0       0.73      0.94      0.83        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.82      0.77      0.77        65\n",
            "weighted avg       0.81      0.78      0.78        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.70      0.82        30\n",
            "         1.0       0.80      1.00      0.89        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.90      0.85      0.85        65\n",
            "weighted avg       0.89      0.86      0.86        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.87      0.75        30\n",
            "         1.0       0.85      0.63      0.72        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.76      0.75      0.74        65\n",
            "weighted avg       0.76      0.74      0.74        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.63      0.63        30\n",
            "         1.0       0.68      0.68      0.68        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.65      0.65      0.65        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.523077  1.000000  0.686869       34  0.523077\n",
            "1   0.586207  0.971429  0.731183       35  0.615385\n",
            "2   0.680851  0.914286  0.780488       35  0.723077\n",
            "3   0.559322  0.942857  0.702128       35  0.569231\n",
            "4   0.620000  0.885714  0.729412       35  0.646154\n",
            "5   0.733333  0.942857  0.825000       35  0.784615\n",
            "6   0.795455  1.000000  0.886076       35  0.861538\n",
            "7   0.666667  0.857143  0.750000       35  0.692308\n",
            "8   0.846154  0.628571  0.721311       35  0.738462\n",
            "9   0.676471  0.676471  0.676471       34  0.656250\n",
            "c =  1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        31\n",
            "         1.0       0.52      1.00      0.69        34\n",
            "\n",
            "    accuracy                           0.52        65\n",
            "   macro avg       0.26      0.50      0.34        65\n",
            "weighted avg       0.27      0.52      0.36        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.20      0.32        30\n",
            "         1.0       0.59      0.97      0.73        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.72      0.59      0.53        65\n",
            "weighted avg       0.71      0.62      0.54        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.47      0.60        30\n",
            "         1.0       0.67      0.91      0.77        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.75      0.69      0.68        65\n",
            "weighted avg       0.74      0.71      0.69        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.10      0.17        30\n",
            "         1.0       0.55      0.94      0.69        35\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.57      0.52      0.43        65\n",
            "weighted avg       0.57      0.55      0.45        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.37      0.50        30\n",
            "         1.0       0.63      0.91      0.74        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.71      0.64      0.62        65\n",
            "weighted avg       0.70      0.66      0.63        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.53      0.67        30\n",
            "         1.0       0.70      0.94      0.80        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.80      0.74      0.74        65\n",
            "weighted avg       0.79      0.75      0.74        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.70      0.82        30\n",
            "         1.0       0.80      1.00      0.89        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.90      0.85      0.85        65\n",
            "weighted avg       0.89      0.86      0.86        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.43      0.54        30\n",
            "         1.0       0.64      0.86      0.73        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.68      0.65      0.64        65\n",
            "weighted avg       0.68      0.66      0.64        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.83      0.77        30\n",
            "         1.0       0.83      0.71      0.77        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.77      0.77      0.77        65\n",
            "weighted avg       0.78      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.63      0.64        30\n",
            "         1.0       0.69      0.71      0.70        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.67      0.67      0.67        64\n",
            "weighted avg       0.67      0.67      0.67        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.523077  1.000000  0.686869       34  0.523077\n",
            "1   0.586207  0.971429  0.731183       35  0.615385\n",
            "2   0.666667  0.914286  0.771084       35  0.707692\n",
            "3   0.550000  0.942857  0.694737       35  0.553846\n",
            "4   0.627451  0.914286  0.744186       35  0.661538\n",
            "5   0.702128  0.942857  0.804878       35  0.753846\n",
            "6   0.795455  1.000000  0.886076       35  0.861538\n",
            "7   0.638298  0.857143  0.731707       35  0.661538\n",
            "8   0.833333  0.714286  0.769231       35  0.769231\n",
            "9   0.685714  0.705882  0.695652       34  0.671875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFhxcmvRyVap",
        "colab_type": "text"
      },
      "source": [
        "Resultado dos k-fold para cada valor de c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "5BhEkmnkyVaq",
        "colab_type": "code",
        "outputId": "88be87e8-83e8-4f46-cb8c-ca8ae0279ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "result_pca"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.566667  1.000000  0.723404       34  0.600000     0.001    PCA\n",
              "  1   0.615385  0.914286  0.735632       35  0.646154     0.001    PCA\n",
              "  2   0.810811  0.857143  0.833333       35  0.815385     0.001    PCA\n",
              "  3   0.617021  0.828571  0.707317       35  0.630769     0.001    PCA\n",
              "  4   0.666667  0.685714  0.676056       35  0.646154     0.001    PCA\n",
              "  5   0.823529  0.800000  0.811594       35  0.800000     0.001    PCA\n",
              "  6   0.828571  0.828571  0.828571       35  0.815385     0.001    PCA\n",
              "  7   0.692308  0.514286  0.590164       35  0.615385     0.001    PCA\n",
              "  8   0.928571  0.371429  0.530612       35  0.646154     0.001    PCA\n",
              "  9   0.736842  0.411765  0.528302       34  0.609375     0.001    PCA],\n",
              " [0.1,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.539683  1.000000  0.701031       34  0.553846       0.1    PCA\n",
              "  1   0.589286  0.942857  0.725275       35  0.615385       0.1    PCA\n",
              "  2   0.761905  0.914286  0.831169       35  0.800000       0.1    PCA\n",
              "  3   0.596154  0.885714  0.712644       35  0.615385       0.1    PCA\n",
              "  4   0.604651  0.742857  0.666667       35  0.600000       0.1    PCA\n",
              "  5   0.800000  0.800000  0.800000       35  0.784615       0.1    PCA\n",
              "  6   0.833333  0.857143  0.845070       35  0.830769       0.1    PCA\n",
              "  7   0.696970  0.657143  0.676471       35  0.661538       0.1    PCA\n",
              "  8   0.842105  0.457143  0.592593       35  0.661538       0.1    PCA\n",
              "  9   0.708333  0.500000  0.586207       34  0.625000       0.1    PCA],\n",
              " [0.25,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.531250  1.000000  0.693878       34  0.538462      0.25    PCA\n",
              "  1   0.586207  0.971429  0.731183       35  0.615385      0.25    PCA\n",
              "  2   0.744186  0.914286  0.820513       35  0.784615      0.25    PCA\n",
              "  3   0.571429  0.914286  0.703297       35  0.584615      0.25    PCA\n",
              "  4   0.608696  0.800000  0.691358       35  0.615385      0.25    PCA\n",
              "  5   0.756098  0.885714  0.815789       35  0.784615      0.25    PCA\n",
              "  6   0.800000  0.914286  0.853333       35  0.830769      0.25    PCA\n",
              "  7   0.717949  0.800000  0.756757       35  0.723077      0.25    PCA\n",
              "  8   0.857143  0.514286  0.642857       35  0.692308      0.25    PCA\n",
              "  9   0.687500  0.647059  0.666667       34  0.656250      0.25    PCA],\n",
              " [0.5,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.531250  1.000000  0.693878       34  0.538462       0.5    PCA\n",
              "  1   0.586207  0.971429  0.731183       35  0.615385       0.5    PCA\n",
              "  2   0.695652  0.914286  0.790123       35  0.738462       0.5    PCA\n",
              "  3   0.559322  0.942857  0.702128       35  0.569231       0.5    PCA\n",
              "  4   0.625000  0.857143  0.722892       35  0.646154       0.5    PCA\n",
              "  5   0.750000  0.942857  0.835443       35  0.800000       0.5    PCA\n",
              "  6   0.809524  0.971429  0.883117       35  0.861538       0.5    PCA\n",
              "  7   0.681818  0.857143  0.759494       35  0.707692       0.5    PCA\n",
              "  8   0.869565  0.571429  0.689655       35  0.723077       0.5    PCA\n",
              "  9   0.696970  0.676471  0.686567       34  0.671875       0.5    PCA],\n",
              " [0.75,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.523077  1.000000  0.686869       34  0.523077      0.75    PCA\n",
              "  1   0.586207  0.971429  0.731183       35  0.615385      0.75    PCA\n",
              "  2   0.680851  0.914286  0.780488       35  0.723077      0.75    PCA\n",
              "  3   0.559322  0.942857  0.702128       35  0.569231      0.75    PCA\n",
              "  4   0.620000  0.885714  0.729412       35  0.646154      0.75    PCA\n",
              "  5   0.733333  0.942857  0.825000       35  0.784615      0.75    PCA\n",
              "  6   0.795455  1.000000  0.886076       35  0.861538      0.75    PCA\n",
              "  7   0.666667  0.857143  0.750000       35  0.692308      0.75    PCA\n",
              "  8   0.846154  0.628571  0.721311       35  0.738462      0.75    PCA\n",
              "  9   0.676471  0.676471  0.676471       34  0.656250      0.75    PCA],\n",
              " [1,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.523077  1.000000  0.686869       34  0.523077         1    PCA\n",
              "  1   0.586207  0.971429  0.731183       35  0.615385         1    PCA\n",
              "  2   0.666667  0.914286  0.771084       35  0.707692         1    PCA\n",
              "  3   0.550000  0.942857  0.694737       35  0.553846         1    PCA\n",
              "  4   0.627451  0.914286  0.744186       35  0.661538         1    PCA\n",
              "  5   0.702128  0.942857  0.804878       35  0.753846         1    PCA\n",
              "  6   0.795455  1.000000  0.886076       35  0.861538         1    PCA\n",
              "  7   0.638298  0.857143  0.731707       35  0.661538         1    PCA\n",
              "  8   0.833333  0.714286  0.769231       35  0.769231         1    PCA\n",
              "  9   0.685714  0.705882  0.695652       34  0.671875         1    PCA]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPSXY9-tyVau",
        "colab_type": "text"
      },
      "source": [
        "Calcula a média das medidas de cada parâmetro c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tTeRvhucyVaw",
        "colab_type": "code",
        "outputId": "39f53e17-c69d-497a-e34c-63e512c7b386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "result_pca_mean = calcula_media(result_pca)\n",
        "result_pca_mean"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.728637</td>\n",
              "      <td>0.721176</td>\n",
              "      <td>0.696499</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.682476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.697242</td>\n",
              "      <td>0.775714</td>\n",
              "      <td>0.713713</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.674808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250</td>\n",
              "      <td>0.686046</td>\n",
              "      <td>0.836134</td>\n",
              "      <td>0.737563</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.682548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.680531</td>\n",
              "      <td>0.870504</td>\n",
              "      <td>0.749448</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.687188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.668754</td>\n",
              "      <td>0.881933</td>\n",
              "      <td>0.748894</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.681010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.660833</td>\n",
              "      <td>0.896303</td>\n",
              "      <td>0.751560</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.677957</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "0  0.001        0.728637  ...          34.8       0.682476\n",
              "1  0.100        0.697242  ...          34.8       0.674808\n",
              "2  0.250        0.686046  ...          34.8       0.682548\n",
              "3  0.500        0.680531  ...          34.8       0.687188\n",
              "4  0.750        0.668754  ...          34.8       0.681010\n",
              "5  1.000        0.660833  ...          34.8       0.677957\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n21hC2IuyVa2",
        "colab_type": "text"
      },
      "source": [
        "Obtém o resultado da maior média de acurácia "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7slQvTAkyVa3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "81b60bd4-196f-4a20-e7b0-22b6c68195b3"
      },
      "source": [
        "best_accuracy_pca = pd.Series(result_pca_mean.iloc[result_pca_mean['accuracy_mean'].idxmax()], \n",
        "                          name='PCA')\n",
        "best_pca = pd.DataFrame(best_accuracy_pca)\n",
        "best_pca"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PCA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_mean</th>\n",
              "      <td>0.680531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_mean</th>\n",
              "      <td>0.870504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score_mean</th>\n",
              "      <td>0.749448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support_mean</th>\n",
              "      <td>34.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_mean</th>\n",
              "      <td>0.687188</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      PCA\n",
              "c                0.500000\n",
              "precision_mean   0.680531\n",
              "recall_mean      0.870504\n",
              "f1_score_mean    0.749448\n",
              "support_mean    34.800000\n",
              "accuracy_mean    0.687188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ale_cv-gyVa8",
        "colab_type": "text"
      },
      "source": [
        "# Execução Base: Chi Squared (K-Best)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9dnzjgTyVa9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "40dc7dde-5835-49fc-ee2f-007c7ff1c3e4"
      },
      "source": [
        "df = pd.read_csv('results/dataset-fs-chi-squared.csv', header = 0)\n",
        "X = df.drop('is_approved', axis=1).to_numpy() # DATASET\n",
        "y = df['is_approved'].to_numpy() # target\n",
        "result_chi , result_chi_g  = stratified_k_fold(X, y, list_c, k=k_folds, name='CHI')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 10, Dataset: 348 positivas, 301 negativas (53% x 46%)\n",
            "Fold 0: Pos: 34, Neg: 31, Total: 65, Proporção: 52% x 47%\n",
            "Fold 1: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 2: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 3: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 4: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 5: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 6: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 7: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 8: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 9: Pos: 34, Neg: 30, Total: 64, Proporção: 53% x 46%\n",
            "c =  0.001\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.16      0.27        31\n",
            "         1.0       0.56      0.97      0.71        34\n",
            "\n",
            "    accuracy                           0.58        65\n",
            "   macro avg       0.70      0.57      0.49        65\n",
            "weighted avg       0.69      0.58      0.50        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.40      0.55        30\n",
            "         1.0       0.65      0.94      0.77        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.75      0.67      0.66        65\n",
            "weighted avg       0.74      0.69      0.66        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.77      0.79        30\n",
            "         1.0       0.81      0.86      0.83        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.82      0.81      0.81        65\n",
            "weighted avg       0.82      0.82      0.81        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.53      0.64        30\n",
            "         1.0       0.69      0.89      0.78        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.74      0.71      0.71        65\n",
            "weighted avg       0.74      0.72      0.71        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67        30\n",
            "         1.0       0.71      0.77      0.74        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.71        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.80      0.84        30\n",
            "         1.0       0.84      0.91      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.86      0.86        65\n",
            "weighted avg       0.86      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.83      0.89        30\n",
            "         1.0       0.87      0.97      0.92        35\n",
            "\n",
            "    accuracy                           0.91        65\n",
            "   macro avg       0.92      0.90      0.91        65\n",
            "weighted avg       0.91      0.91      0.91        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.70      0.70        30\n",
            "         1.0       0.74      0.74      0.74        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.72      0.72      0.72        65\n",
            "weighted avg       0.72      0.72      0.72        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.97      0.75        30\n",
            "         1.0       0.94      0.49      0.64        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.78      0.73      0.70        65\n",
            "weighted avg       0.79      0.71      0.69        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.64      0.77      0.70        30\n",
            "         1.0       0.75      0.62      0.68        34\n",
            "\n",
            "    accuracy                           0.69        64\n",
            "   macro avg       0.69      0.69      0.69        64\n",
            "weighted avg       0.70      0.69      0.69        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.559322  0.970588  0.709677       34  0.584615\n",
            "1   0.647059  0.942857  0.767442       35  0.692308\n",
            "2   0.810811  0.857143  0.833333       35  0.815385\n",
            "3   0.688889  0.885714  0.775000       35  0.723077\n",
            "4   0.710526  0.771429  0.739726       35  0.707692\n",
            "5   0.842105  0.914286  0.876712       35  0.861538\n",
            "6   0.871795  0.971429  0.918919       35  0.907692\n",
            "7   0.742857  0.742857  0.742857       35  0.723077\n",
            "8   0.944444  0.485714  0.641509       35  0.707692\n",
            "9   0.750000  0.617647  0.677419       34  0.687500\n",
            "c =  0.1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.33      0.47        30\n",
            "         1.0       0.62      0.91      0.74        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.69      0.62      0.60        65\n",
            "weighted avg       0.69      0.65      0.61        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.70      0.72        30\n",
            "         1.0       0.76      0.80      0.78        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.75      0.75      0.75        65\n",
            "weighted avg       0.75      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67        30\n",
            "         1.0       0.71      0.77      0.74        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.71        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.80      0.84        30\n",
            "         1.0       0.84      0.91      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.86      0.86        65\n",
            "weighted avg       0.86      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.97      0.97        30\n",
            "         1.0       0.97      0.97      0.97        35\n",
            "\n",
            "    accuracy                           0.97        65\n",
            "   macro avg       0.97      0.97      0.97        65\n",
            "weighted avg       0.97      0.97      0.97        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.77      0.73        30\n",
            "         1.0       0.78      0.71      0.75        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.74      0.74      0.74        65\n",
            "weighted avg       0.74      0.74      0.74        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      0.97      0.73        30\n",
            "         1.0       0.94      0.43      0.59        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.76      0.70      0.66        65\n",
            "weighted avg       0.78      0.68      0.66        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.64      0.77      0.70        30\n",
            "         1.0       0.75      0.62      0.68        34\n",
            "\n",
            "    accuracy                           0.69        64\n",
            "   macro avg       0.69      0.69      0.69        64\n",
            "weighted avg       0.70      0.69      0.69        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.540984  0.970588  0.694737       34  0.553846\n",
            "1   0.615385  0.914286  0.735632       35  0.646154\n",
            "2   0.756757  0.800000  0.777778       35  0.753846\n",
            "3   0.666667  0.857143  0.750000       35  0.692308\n",
            "4   0.710526  0.771429  0.739726       35  0.707692\n",
            "5   0.842105  0.914286  0.876712       35  0.861538\n",
            "6   0.971429  0.971429  0.971429       35  0.969231\n",
            "7   0.781250  0.714286  0.746269       35  0.738462\n",
            "8   0.937500  0.428571  0.588235       35  0.676923\n",
            "9   0.750000  0.617647  0.677419       34  0.687500\n",
            "c =  0.25\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.27      0.39        30\n",
            "         1.0       0.59      0.91      0.72        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.66      0.59      0.55        65\n",
            "weighted avg       0.65      0.62      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.70      0.72        30\n",
            "         1.0       0.76      0.80      0.78        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.75      0.75      0.75        65\n",
            "weighted avg       0.75      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.50      0.60        30\n",
            "         1.0       0.67      0.86      0.75        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.71      0.68      0.68        65\n",
            "weighted avg       0.71      0.69      0.68        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67        30\n",
            "         1.0       0.71      0.77      0.74        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.71      0.70      0.70        65\n",
            "weighted avg       0.71      0.71      0.71        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.77      0.81        30\n",
            "         1.0       0.82      0.89      0.85        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.83      0.83      0.83        65\n",
            "weighted avg       0.83      0.83      0.83        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.97      0.97        30\n",
            "         1.0       0.97      0.97      0.97        35\n",
            "\n",
            "    accuracy                           0.97        65\n",
            "   macro avg       0.97      0.97      0.97        65\n",
            "weighted avg       0.97      0.97      0.97        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.77      0.74        30\n",
            "         1.0       0.79      0.74      0.76        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.75      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.97      0.74        30\n",
            "         1.0       0.94      0.46      0.62        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.77      0.71      0.68        65\n",
            "weighted avg       0.79      0.69      0.67        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.73      0.67        30\n",
            "         1.0       0.71      0.59      0.65        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.67      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.540984  0.970588  0.694737       34  0.553846\n",
            "1   0.592593  0.914286  0.719101       35  0.615385\n",
            "2   0.756757  0.800000  0.777778       35  0.753846\n",
            "3   0.666667  0.857143  0.750000       35  0.692308\n",
            "4   0.710526  0.771429  0.739726       35  0.707692\n",
            "5   0.815789  0.885714  0.849315       35  0.830769\n",
            "6   0.971429  0.971429  0.971429       35  0.969231\n",
            "7   0.787879  0.742857  0.764706       35  0.753846\n",
            "8   0.941176  0.457143  0.615385       35  0.692308\n",
            "9   0.714286  0.588235  0.645161       34  0.656250\n",
            "c =  0.5\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.27      0.39        30\n",
            "         1.0       0.59      0.91      0.72        35\n",
            "\n",
            "    accuracy                           0.62        65\n",
            "   macro avg       0.66      0.59      0.55        65\n",
            "weighted avg       0.65      0.62      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.67      0.71        30\n",
            "         1.0       0.74      0.83      0.78        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.75      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.43      0.55        30\n",
            "         1.0       0.65      0.89      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.71      0.66      0.65        65\n",
            "weighted avg       0.70      0.68      0.66        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.57      0.63        30\n",
            "         1.0       0.68      0.80      0.74        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.70      0.68      0.68        65\n",
            "weighted avg       0.69      0.69      0.69        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.77      0.84        30\n",
            "         1.0       0.82      0.94      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.85      0.86        65\n",
            "weighted avg       0.87      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.93      0.97        30\n",
            "         1.0       0.95      1.00      0.97        35\n",
            "\n",
            "    accuracy                           0.97        65\n",
            "   macro avg       0.97      0.97      0.97        65\n",
            "weighted avg       0.97      0.97      0.97        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.80      0.77        30\n",
            "         1.0       0.82      0.77      0.79        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.78      0.79      0.78        65\n",
            "weighted avg       0.79      0.78      0.78        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.97      0.74        30\n",
            "         1.0       0.94      0.46      0.62        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.77      0.71      0.68        65\n",
            "weighted avg       0.79      0.69      0.67        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.73      0.68        30\n",
            "         1.0       0.72      0.62      0.67        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.68      0.68      0.67        64\n",
            "weighted avg       0.68      0.67      0.67        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.540984  0.970588  0.694737       34  0.553846\n",
            "1   0.592593  0.914286  0.719101       35  0.615385\n",
            "2   0.743590  0.828571  0.783784       35  0.753846\n",
            "3   0.645833  0.885714  0.746988       35  0.676923\n",
            "4   0.682927  0.800000  0.736842       35  0.692308\n",
            "5   0.825000  0.942857  0.880000       35  0.861538\n",
            "6   0.945946  1.000000  0.972222       35  0.969231\n",
            "7   0.818182  0.771429  0.794118       35  0.784615\n",
            "8   0.941176  0.457143  0.615385       35  0.692308\n",
            "9   0.724138  0.617647  0.666667       34  0.671875\n",
            "c =  0.75\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.27      0.40        30\n",
            "         1.0       0.60      0.94      0.73        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.70      0.60      0.57        65\n",
            "weighted avg       0.69      0.63      0.58        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.63      0.69        30\n",
            "         1.0       0.72      0.83      0.77        35\n",
            "\n",
            "    accuracy                           0.74        65\n",
            "   macro avg       0.74      0.73      0.73        65\n",
            "weighted avg       0.74      0.74      0.74        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.43      0.55        30\n",
            "         1.0       0.65      0.89      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.71      0.66      0.65        65\n",
            "weighted avg       0.70      0.68      0.66        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.53      0.60        30\n",
            "         1.0       0.67      0.80      0.73        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.77      0.84        30\n",
            "         1.0       0.82      0.94      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.85      0.86        65\n",
            "weighted avg       0.87      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.90      0.95        30\n",
            "         1.0       0.92      1.00      0.96        35\n",
            "\n",
            "    accuracy                           0.95        65\n",
            "   macro avg       0.96      0.95      0.95        65\n",
            "weighted avg       0.96      0.95      0.95        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.77      0.77        30\n",
            "         1.0       0.80      0.80      0.80        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.78      0.78      0.78        65\n",
            "weighted avg       0.78      0.78      0.78        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.97      0.74        30\n",
            "         1.0       0.94      0.46      0.62        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.77      0.71      0.68        65\n",
            "weighted avg       0.79      0.69      0.67        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.540984  0.970588  0.694737       34  0.553846\n",
            "1   0.600000  0.942857  0.733333       35  0.630769\n",
            "2   0.725000  0.828571  0.773333       35  0.738462\n",
            "3   0.645833  0.885714  0.746988       35  0.676923\n",
            "4   0.666667  0.800000  0.727273       35  0.676923\n",
            "5   0.825000  0.942857  0.880000       35  0.861538\n",
            "6   0.921053  1.000000  0.958904       35  0.953846\n",
            "7   0.800000  0.800000  0.800000       35  0.784615\n",
            "8   0.941176  0.457143  0.615385       35  0.692308\n",
            "9   0.700000  0.617647  0.656250       34  0.656250\n",
            "c =  1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.10      0.17        31\n",
            "         1.0       0.54      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.65      0.53      0.43        65\n",
            "weighted avg       0.64      0.55      0.45        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.27      0.40        30\n",
            "         1.0       0.60      0.94      0.73        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.70      0.60      0.57        65\n",
            "weighted avg       0.69      0.63      0.58        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.60      0.67        30\n",
            "         1.0       0.71      0.83      0.76        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.73      0.71      0.71        65\n",
            "weighted avg       0.73      0.72      0.72        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.37      0.49        30\n",
            "         1.0       0.62      0.89      0.73        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.68      0.63      0.61        65\n",
            "weighted avg       0.67      0.65      0.62        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.53      0.60        30\n",
            "         1.0       0.67      0.80      0.73        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.67      0.67        65\n",
            "weighted avg       0.68      0.68      0.67        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.77      0.84        30\n",
            "         1.0       0.82      0.94      0.88        35\n",
            "\n",
            "    accuracy                           0.86        65\n",
            "   macro avg       0.87      0.85      0.86        65\n",
            "weighted avg       0.87      0.86      0.86        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.90      0.95        30\n",
            "         1.0       0.92      1.00      0.96        35\n",
            "\n",
            "    accuracy                           0.95        65\n",
            "   macro avg       0.96      0.95      0.95        65\n",
            "weighted avg       0.96      0.95      0.95        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.73      0.76        30\n",
            "         1.0       0.78      0.83      0.81        35\n",
            "\n",
            "    accuracy                           0.78        65\n",
            "   macro avg       0.78      0.78      0.78        65\n",
            "weighted avg       0.78      0.78      0.78        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.93      0.73        30\n",
            "         1.0       0.89      0.46      0.60        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.74      0.70      0.67        65\n",
            "weighted avg       0.75      0.68      0.66        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66        30\n",
            "         1.0       0.70      0.62      0.66        34\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.66      0.66      0.66        64\n",
            "weighted avg       0.66      0.66      0.66        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.540984  0.970588  0.694737       34  0.553846\n",
            "1   0.600000  0.942857  0.733333       35  0.630769\n",
            "2   0.707317  0.828571  0.763158       35  0.723077\n",
            "3   0.620000  0.885714  0.729412       35  0.646154\n",
            "4   0.666667  0.800000  0.727273       35  0.676923\n",
            "5   0.825000  0.942857  0.880000       35  0.861538\n",
            "6   0.921053  1.000000  0.958904       35  0.953846\n",
            "7   0.783784  0.828571  0.805556       35  0.784615\n",
            "8   0.888889  0.457143  0.603774       35  0.676923\n",
            "9   0.700000  0.617647  0.656250       34  0.656250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xKkaHx0yVbE",
        "colab_type": "text"
      },
      "source": [
        "Resultado dos k-fold para cada valor de c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_d3uSAFyVbF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39ddff48-6d74-408e-f64c-36178345d498"
      },
      "source": [
        "result_chi"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.559322  0.970588  0.709677       34  0.584615     0.001    CHI\n",
              "  1   0.647059  0.942857  0.767442       35  0.692308     0.001    CHI\n",
              "  2   0.810811  0.857143  0.833333       35  0.815385     0.001    CHI\n",
              "  3   0.688889  0.885714  0.775000       35  0.723077     0.001    CHI\n",
              "  4   0.710526  0.771429  0.739726       35  0.707692     0.001    CHI\n",
              "  5   0.842105  0.914286  0.876712       35  0.861538     0.001    CHI\n",
              "  6   0.871795  0.971429  0.918919       35  0.907692     0.001    CHI\n",
              "  7   0.742857  0.742857  0.742857       35  0.723077     0.001    CHI\n",
              "  8   0.944444  0.485714  0.641509       35  0.707692     0.001    CHI\n",
              "  9   0.750000  0.617647  0.677419       34  0.687500     0.001    CHI],\n",
              " [0.1,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846       0.1    CHI\n",
              "  1   0.615385  0.914286  0.735632       35  0.646154       0.1    CHI\n",
              "  2   0.756757  0.800000  0.777778       35  0.753846       0.1    CHI\n",
              "  3   0.666667  0.857143  0.750000       35  0.692308       0.1    CHI\n",
              "  4   0.710526  0.771429  0.739726       35  0.707692       0.1    CHI\n",
              "  5   0.842105  0.914286  0.876712       35  0.861538       0.1    CHI\n",
              "  6   0.971429  0.971429  0.971429       35  0.969231       0.1    CHI\n",
              "  7   0.781250  0.714286  0.746269       35  0.738462       0.1    CHI\n",
              "  8   0.937500  0.428571  0.588235       35  0.676923       0.1    CHI\n",
              "  9   0.750000  0.617647  0.677419       34  0.687500       0.1    CHI],\n",
              " [0.25,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846      0.25    CHI\n",
              "  1   0.592593  0.914286  0.719101       35  0.615385      0.25    CHI\n",
              "  2   0.756757  0.800000  0.777778       35  0.753846      0.25    CHI\n",
              "  3   0.666667  0.857143  0.750000       35  0.692308      0.25    CHI\n",
              "  4   0.710526  0.771429  0.739726       35  0.707692      0.25    CHI\n",
              "  5   0.815789  0.885714  0.849315       35  0.830769      0.25    CHI\n",
              "  6   0.971429  0.971429  0.971429       35  0.969231      0.25    CHI\n",
              "  7   0.787879  0.742857  0.764706       35  0.753846      0.25    CHI\n",
              "  8   0.941176  0.457143  0.615385       35  0.692308      0.25    CHI\n",
              "  9   0.714286  0.588235  0.645161       34  0.656250      0.25    CHI],\n",
              " [0.5,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846       0.5    CHI\n",
              "  1   0.592593  0.914286  0.719101       35  0.615385       0.5    CHI\n",
              "  2   0.743590  0.828571  0.783784       35  0.753846       0.5    CHI\n",
              "  3   0.645833  0.885714  0.746988       35  0.676923       0.5    CHI\n",
              "  4   0.682927  0.800000  0.736842       35  0.692308       0.5    CHI\n",
              "  5   0.825000  0.942857  0.880000       35  0.861538       0.5    CHI\n",
              "  6   0.945946  1.000000  0.972222       35  0.969231       0.5    CHI\n",
              "  7   0.818182  0.771429  0.794118       35  0.784615       0.5    CHI\n",
              "  8   0.941176  0.457143  0.615385       35  0.692308       0.5    CHI\n",
              "  9   0.724138  0.617647  0.666667       34  0.671875       0.5    CHI],\n",
              " [0.75,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846      0.75    CHI\n",
              "  1   0.600000  0.942857  0.733333       35  0.630769      0.75    CHI\n",
              "  2   0.725000  0.828571  0.773333       35  0.738462      0.75    CHI\n",
              "  3   0.645833  0.885714  0.746988       35  0.676923      0.75    CHI\n",
              "  4   0.666667  0.800000  0.727273       35  0.676923      0.75    CHI\n",
              "  5   0.825000  0.942857  0.880000       35  0.861538      0.75    CHI\n",
              "  6   0.921053  1.000000  0.958904       35  0.953846      0.75    CHI\n",
              "  7   0.800000  0.800000  0.800000       35  0.784615      0.75    CHI\n",
              "  8   0.941176  0.457143  0.615385       35  0.692308      0.75    CHI\n",
              "  9   0.700000  0.617647  0.656250       34  0.656250      0.75    CHI],\n",
              " [1,    precision    recall  f1-score  support  accuracy  Param(c) method\n",
              "  0   0.540984  0.970588  0.694737       34  0.553846         1    CHI\n",
              "  1   0.600000  0.942857  0.733333       35  0.630769         1    CHI\n",
              "  2   0.707317  0.828571  0.763158       35  0.723077         1    CHI\n",
              "  3   0.620000  0.885714  0.729412       35  0.646154         1    CHI\n",
              "  4   0.666667  0.800000  0.727273       35  0.676923         1    CHI\n",
              "  5   0.825000  0.942857  0.880000       35  0.861538         1    CHI\n",
              "  6   0.921053  1.000000  0.958904       35  0.953846         1    CHI\n",
              "  7   0.783784  0.828571  0.805556       35  0.784615         1    CHI\n",
              "  8   0.888889  0.457143  0.603774       35  0.676923         1    CHI\n",
              "  9   0.700000  0.617647  0.656250       34  0.656250         1    CHI]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxKFeV75yVbK",
        "colab_type": "text"
      },
      "source": [
        "Calcula a média das medidas de cada parâmetro c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiwgWhz2yVbL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "b34d512d-eb55-4dfd-df22-00a2226c9eb6"
      },
      "source": [
        "result_chi_mean = calcula_media(result_chi)\n",
        "result_chi_mean"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.756781</td>\n",
              "      <td>0.815966</td>\n",
              "      <td>0.768260</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.741058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.757260</td>\n",
              "      <td>0.795966</td>\n",
              "      <td>0.755794</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.728750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250</td>\n",
              "      <td>0.749808</td>\n",
              "      <td>0.795882</td>\n",
              "      <td>0.752734</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.722548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.746037</td>\n",
              "      <td>0.818824</td>\n",
              "      <td>0.760984</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.727187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.736571</td>\n",
              "      <td>0.824538</td>\n",
              "      <td>0.758620</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.722548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.725369</td>\n",
              "      <td>0.827395</td>\n",
              "      <td>0.755240</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.716394</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "0  0.001        0.756781  ...          34.8       0.741058\n",
              "1  0.100        0.757260  ...          34.8       0.728750\n",
              "2  0.250        0.749808  ...          34.8       0.722548\n",
              "3  0.500        0.746037  ...          34.8       0.727187\n",
              "4  0.750        0.736571  ...          34.8       0.722548\n",
              "5  1.000        0.725369  ...          34.8       0.716394\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0PrGh7-yVbP",
        "colab_type": "text"
      },
      "source": [
        "Obtém o resultado da maior média de acurácia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZIuVq2JyVbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "c38fc24a-0086-424c-f9d9-43c3878f10c1"
      },
      "source": [
        "best_accuracy_chi = pd.Series(result_chi_mean.iloc[result_chi_mean['accuracy_mean'].idxmax()], \n",
        "                          name='Chi Squared')\n",
        "best_chi = pd.DataFrame(best_accuracy_chi)\n",
        "best_chi"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Chi Squared</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_mean</th>\n",
              "      <td>0.756781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_mean</th>\n",
              "      <td>0.815966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score_mean</th>\n",
              "      <td>0.768260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support_mean</th>\n",
              "      <td>34.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_mean</th>\n",
              "      <td>0.741058</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Chi Squared\n",
              "c                  0.001000\n",
              "precision_mean     0.756781\n",
              "recall_mean        0.815966\n",
              "f1_score_mean      0.768260\n",
              "support_mean      34.800000\n",
              "accuracy_mean      0.741058"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9NNSbziyVbU",
        "colab_type": "text"
      },
      "source": [
        "# Execução Base: recursive-feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1WD345tyVbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6dde0ec8-637f-4531-b5de-33ec0891f504"
      },
      "source": [
        "df = pd.read_csv('results/dataset-fs-recursive-feature.csv', header = 0)\n",
        "X = df.drop('is_approved', axis=1).to_numpy() # DATASET\n",
        "y = df['is_approved'].to_numpy() # target\n",
        "result_recursive , result_recursive_g  = stratified_k_fold(X, y, list_c, k=k_folds, name='Recursive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k = 10, Dataset: 348 positivas, 301 negativas (53% x 46%)\n",
            "Fold 0: Pos: 34, Neg: 31, Total: 65, Proporção: 52% x 47%\n",
            "Fold 1: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 2: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 3: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 4: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 5: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 6: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 7: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 8: Pos: 35, Neg: 30, Total: 65, Proporção: 53% x 46%\n",
            "Fold 9: Pos: 34, Neg: 30, Total: 64, Proporção: 53% x 46%\n",
            "c =  0.001\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.13      0.23        31\n",
            "         1.0       0.56      1.00      0.72        34\n",
            "\n",
            "    accuracy                           0.58        65\n",
            "   macro avg       0.78      0.56      0.47        65\n",
            "weighted avg       0.77      0.58      0.48        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.40      0.53        30\n",
            "         1.0       0.64      0.91      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.72      0.66      0.64        65\n",
            "weighted avg       0.71      0.68      0.65        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.67      0.75        30\n",
            "         1.0       0.76      0.91      0.83        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.82      0.79      0.79        65\n",
            "weighted avg       0.81      0.80      0.80        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.47      0.55        30\n",
            "         1.0       0.64      0.80      0.71        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.65      0.63      0.63        65\n",
            "weighted avg       0.65      0.65      0.64        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.60      0.71        30\n",
            "         1.0       0.73      0.91      0.81        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.79      0.76      0.76        65\n",
            "weighted avg       0.79      0.77      0.76        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.67      0.75        30\n",
            "         1.0       0.76      0.91      0.83        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.82      0.79      0.79        65\n",
            "weighted avg       0.81      0.80      0.80        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.80      0.87        30\n",
            "         1.0       0.85      0.97      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.91      0.89      0.89        65\n",
            "weighted avg       0.90      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.47      0.56        30\n",
            "         1.0       0.64      0.83      0.73        35\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.67      0.65      0.64        65\n",
            "weighted avg       0.67      0.66      0.65        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.87      0.78        30\n",
            "         1.0       0.86      0.69      0.76        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.78      0.78      0.77        65\n",
            "weighted avg       0.79      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.73      0.71        30\n",
            "         1.0       0.75      0.71      0.73        34\n",
            "\n",
            "    accuracy                           0.72        64\n",
            "   macro avg       0.72      0.72      0.72        64\n",
            "weighted avg       0.72      0.72      0.72        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.557377  1.000000  0.715789       34  0.584615\n",
            "1   0.640000  0.914286  0.752941       35  0.676923\n",
            "2   0.761905  0.914286  0.831169       35  0.800000\n",
            "3   0.636364  0.800000  0.708861       35  0.646154\n",
            "4   0.727273  0.914286  0.810127       35  0.769231\n",
            "5   0.761905  0.914286  0.831169       35  0.800000\n",
            "6   0.850000  0.971429  0.906667       35  0.892308\n",
            "7   0.644444  0.828571  0.725000       35  0.661538\n",
            "8   0.857143  0.685714  0.761905       35  0.769231\n",
            "9   0.750000  0.705882  0.727273       34  0.718750\n",
            "c =  0.1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.06      0.12        31\n",
            "         1.0       0.53      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.54        65\n",
            "   macro avg       0.60      0.52      0.40        65\n",
            "weighted avg       0.60      0.54      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.33      0.49        30\n",
            "         1.0       0.63      0.97      0.76        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.77      0.65      0.63        65\n",
            "weighted avg       0.76      0.68      0.64        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.57      0.68        30\n",
            "         1.0       0.71      0.91      0.80        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.78      0.74      0.74        65\n",
            "weighted avg       0.78      0.75      0.74        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.40      0.53        30\n",
            "         1.0       0.64      0.91      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.72      0.66      0.64        65\n",
            "weighted avg       0.71      0.68      0.65        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.63      0.72        30\n",
            "         1.0       0.74      0.89      0.81        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.78      0.76      0.76        65\n",
            "weighted avg       0.78      0.77      0.76        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.67      0.78        30\n",
            "         1.0       0.77      0.97      0.86        35\n",
            "\n",
            "    accuracy                           0.83        65\n",
            "   macro avg       0.86      0.82      0.82        65\n",
            "weighted avg       0.86      0.83      0.83        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.80      0.87        30\n",
            "         1.0       0.85      0.97      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.91      0.89      0.89        65\n",
            "weighted avg       0.90      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.47      0.57        30\n",
            "         1.0       0.65      0.86      0.74        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.69      0.66      0.66        65\n",
            "weighted avg       0.69      0.68      0.66        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.93      0.79        30\n",
            "         1.0       0.92      0.63      0.75        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.80      0.78      0.77        65\n",
            "weighted avg       0.81      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.70      0.69        30\n",
            "         1.0       0.73      0.71      0.72        34\n",
            "\n",
            "    accuracy                           0.70        64\n",
            "   macro avg       0.70      0.70      0.70        64\n",
            "weighted avg       0.70      0.70      0.70        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.532258  0.970588  0.687500       34  0.538462\n",
            "1   0.629630  0.971429  0.764045       35  0.676923\n",
            "2   0.711111  0.914286  0.800000       35  0.753846\n",
            "3   0.640000  0.914286  0.752941       35  0.676923\n",
            "4   0.738095  0.885714  0.805195       35  0.769231\n",
            "5   0.772727  0.971429  0.860759       35  0.830769\n",
            "6   0.850000  0.971429  0.906667       35  0.892308\n",
            "7   0.652174  0.857143  0.740741       35  0.676923\n",
            "8   0.916667  0.628571  0.745763       35  0.769231\n",
            "9   0.727273  0.705882  0.716418       34  0.703125\n",
            "c =  0.25\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.06      0.12        31\n",
            "         1.0       0.53      0.97      0.69        34\n",
            "\n",
            "    accuracy                           0.54        65\n",
            "   macro avg       0.60      0.52      0.40        65\n",
            "weighted avg       0.60      0.54      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.23      0.37        30\n",
            "         1.0       0.60      0.97      0.74        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.74      0.60      0.55        65\n",
            "weighted avg       0.73      0.63      0.57        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.57      0.68        30\n",
            "         1.0       0.71      0.91      0.80        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.78      0.74      0.74        65\n",
            "weighted avg       0.78      0.75      0.74        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.37      0.51        30\n",
            "         1.0       0.63      0.94      0.76        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.74      0.65      0.64        65\n",
            "weighted avg       0.73      0.68      0.64        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.57      0.68        30\n",
            "         1.0       0.71      0.91      0.80        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.78      0.74      0.74        65\n",
            "weighted avg       0.78      0.75      0.74        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.60      0.75        30\n",
            "         1.0       0.74      1.00      0.85        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.87      0.80      0.80        65\n",
            "weighted avg       0.86      0.82      0.81        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.77      0.85        30\n",
            "         1.0       0.83      0.97      0.89        35\n",
            "\n",
            "    accuracy                           0.88        65\n",
            "   macro avg       0.89      0.87      0.87        65\n",
            "weighted avg       0.89      0.88      0.87        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.47      0.60        30\n",
            "         1.0       0.67      0.91      0.77        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.75      0.69      0.68        65\n",
            "weighted avg       0.74      0.71      0.69        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.90      0.78        30\n",
            "         1.0       0.88      0.66      0.75        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.79      0.78      0.77        65\n",
            "weighted avg       0.80      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.73      0.71        30\n",
            "         1.0       0.75      0.71      0.73        34\n",
            "\n",
            "    accuracy                           0.72        64\n",
            "   macro avg       0.72      0.72      0.72        64\n",
            "weighted avg       0.72      0.72      0.72        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.532258  0.970588  0.687500       34  0.538462\n",
            "1   0.596491  0.971429  0.739130       35  0.630769\n",
            "2   0.711111  0.914286  0.800000       35  0.753846\n",
            "3   0.634615  0.942857  0.758621       35  0.676923\n",
            "4   0.711111  0.914286  0.800000       35  0.753846\n",
            "5   0.744681  1.000000  0.853659       35  0.815385\n",
            "6   0.829268  0.971429  0.894737       35  0.876923\n",
            "7   0.666667  0.914286  0.771084       35  0.707692\n",
            "8   0.884615  0.657143  0.754098       35  0.769231\n",
            "9   0.750000  0.705882  0.727273       34  0.718750\n",
            "c =  0.5\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.06      0.12        31\n",
            "         1.0       0.54      1.00      0.70        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.77      0.53      0.41        65\n",
            "weighted avg       0.76      0.55      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.17      0.28        30\n",
            "         1.0       0.58      0.97      0.72        35\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.70      0.57      0.50        65\n",
            "weighted avg       0.69      0.60      0.52        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.47      0.61        30\n",
            "         1.0       0.67      0.94      0.79        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.77      0.70      0.70        65\n",
            "weighted avg       0.77      0.72      0.70        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.30      0.44        30\n",
            "         1.0       0.61      0.94      0.74        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.71      0.62      0.59        65\n",
            "weighted avg       0.71      0.65      0.60        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.53      0.68        30\n",
            "         1.0       0.71      0.97      0.82        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.82      0.75      0.75        65\n",
            "weighted avg       0.82      0.77      0.76        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.60      0.75        30\n",
            "         1.0       0.74      1.00      0.85        35\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.87      0.80      0.80        65\n",
            "weighted avg       0.86      0.82      0.81        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.77      0.87        30\n",
            "         1.0       0.83      1.00      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.92      0.88      0.89        65\n",
            "weighted avg       0.91      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.47      0.60        30\n",
            "         1.0       0.67      0.91      0.77        35\n",
            "\n",
            "    accuracy                           0.71        65\n",
            "   macro avg       0.75      0.69      0.68        65\n",
            "weighted avg       0.74      0.71      0.69        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.83      0.76        30\n",
            "         1.0       0.83      0.69      0.75        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.76      0.75        65\n",
            "weighted avg       0.77      0.75      0.75        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.70      0.69        30\n",
            "         1.0       0.73      0.71      0.72        34\n",
            "\n",
            "    accuracy                           0.70        64\n",
            "   macro avg       0.70      0.70      0.70        64\n",
            "weighted avg       0.70      0.70      0.70        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.539683  1.000000  0.701031       34  0.553846\n",
            "1   0.576271  0.971429  0.723404       35  0.600000\n",
            "2   0.673469  0.942857  0.785714       35  0.723077\n",
            "3   0.611111  0.942857  0.741573       35  0.646154\n",
            "4   0.708333  0.971429  0.819277       35  0.769231\n",
            "5   0.744681  1.000000  0.853659       35  0.815385\n",
            "6   0.833333  1.000000  0.909091       35  0.892308\n",
            "7   0.666667  0.914286  0.771084       35  0.707692\n",
            "8   0.827586  0.685714  0.750000       35  0.753846\n",
            "9   0.727273  0.705882  0.716418       34  0.703125\n",
            "c =  0.75\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.06      0.12        31\n",
            "         1.0       0.54      1.00      0.70        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.77      0.53      0.41        65\n",
            "weighted avg       0.76      0.55      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.17      0.28        30\n",
            "         1.0       0.58      0.97      0.72        35\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.70      0.57      0.50        65\n",
            "weighted avg       0.69      0.60      0.52        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.43      0.59        30\n",
            "         1.0       0.67      0.97      0.79        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.80      0.70      0.69        65\n",
            "weighted avg       0.79      0.72      0.70        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.30      0.44        30\n",
            "         1.0       0.61      0.94      0.74        35\n",
            "\n",
            "    accuracy                           0.65        65\n",
            "   macro avg       0.71      0.62      0.59        65\n",
            "weighted avg       0.71      0.65      0.60        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.50      0.65        30\n",
            "         1.0       0.69      0.97      0.81        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.82      0.74      0.73        65\n",
            "weighted avg       0.81      0.75      0.74        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.57      0.72        30\n",
            "         1.0       0.73      1.00      0.84        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.86      0.78      0.78        65\n",
            "weighted avg       0.85      0.80      0.79        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.77      0.87        30\n",
            "         1.0       0.83      1.00      0.91        35\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.92      0.88      0.89        65\n",
            "weighted avg       0.91      0.89      0.89        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.43      0.57        30\n",
            "         1.0       0.65      0.91      0.76        35\n",
            "\n",
            "    accuracy                           0.69        65\n",
            "   macro avg       0.73      0.67      0.66        65\n",
            "weighted avg       0.73      0.69      0.67        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.80      0.75        30\n",
            "         1.0       0.81      0.71      0.76        35\n",
            "\n",
            "    accuracy                           0.75        65\n",
            "   macro avg       0.76      0.76      0.75        65\n",
            "weighted avg       0.76      0.75      0.75        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.67      0.67        30\n",
            "         1.0       0.71      0.71      0.71        34\n",
            "\n",
            "    accuracy                           0.69        64\n",
            "   macro avg       0.69      0.69      0.69        64\n",
            "weighted avg       0.69      0.69      0.69        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.539683  1.000000  0.701031       34  0.553846\n",
            "1   0.576271  0.971429  0.723404       35  0.600000\n",
            "2   0.666667  0.971429  0.790698       35  0.723077\n",
            "3   0.611111  0.942857  0.741573       35  0.646154\n",
            "4   0.693878  0.971429  0.809524       35  0.753846\n",
            "5   0.729167  1.000000  0.843373       35  0.800000\n",
            "6   0.833333  1.000000  0.909091       35  0.892308\n",
            "7   0.653061  0.914286  0.761905       35  0.692308\n",
            "8   0.806452  0.714286  0.757576       35  0.753846\n",
            "9   0.705882  0.705882  0.705882       34  0.687500\n",
            "c =  1\n",
            "fold_k: 1\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.06      0.12        31\n",
            "         1.0       0.54      1.00      0.70        34\n",
            "\n",
            "    accuracy                           0.55        65\n",
            "   macro avg       0.77      0.53      0.41        65\n",
            "weighted avg       0.76      0.55      0.42        65\n",
            "\n",
            "fold_k: 2\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.17      0.28        30\n",
            "         1.0       0.58      0.97      0.72        35\n",
            "\n",
            "    accuracy                           0.60        65\n",
            "   macro avg       0.70      0.57      0.50        65\n",
            "weighted avg       0.69      0.60      0.52        65\n",
            "\n",
            "fold_k: 3\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.40      0.57        30\n",
            "         1.0       0.66      1.00      0.80        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.83      0.70      0.68        65\n",
            "weighted avg       0.82      0.72      0.69        65\n",
            "\n",
            "fold_k: 4\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.27      0.40        30\n",
            "         1.0       0.60      0.94      0.73        35\n",
            "\n",
            "    accuracy                           0.63        65\n",
            "   macro avg       0.70      0.60      0.57        65\n",
            "weighted avg       0.69      0.63      0.58        65\n",
            "\n",
            "fold_k: 5\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.43      0.59        30\n",
            "         1.0       0.67      0.97      0.79        35\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.80      0.70      0.69        65\n",
            "weighted avg       0.79      0.72      0.70        65\n",
            "\n",
            "fold_k: 6\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.57      0.72        30\n",
            "         1.0       0.73      1.00      0.84        35\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.86      0.78      0.78        65\n",
            "weighted avg       0.85      0.80      0.79        65\n",
            "\n",
            "fold_k: 7\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.73      0.85        30\n",
            "         1.0       0.81      1.00      0.90        35\n",
            "\n",
            "    accuracy                           0.88        65\n",
            "   macro avg       0.91      0.87      0.87        65\n",
            "weighted avg       0.90      0.88      0.87        65\n",
            "\n",
            "fold_k: 8\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.40      0.53        30\n",
            "         1.0       0.64      0.91      0.75        35\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.72      0.66      0.64        65\n",
            "weighted avg       0.71      0.68      0.65        65\n",
            "\n",
            "fold_k: 9\n",
            "\n",
            "TRAIN: 584  TEST: 65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.77      0.75        30\n",
            "         1.0       0.79      0.77      0.78        35\n",
            "\n",
            "    accuracy                           0.77        65\n",
            "   macro avg       0.77      0.77      0.77        65\n",
            "weighted avg       0.77      0.77      0.77        65\n",
            "\n",
            "fold_k: 10\n",
            "\n",
            "TRAIN: 585  TEST: 64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.63      0.64        30\n",
            "         1.0       0.69      0.71      0.70        34\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.67      0.67      0.67        64\n",
            "weighted avg       0.67      0.67      0.67        64\n",
            "\n",
            "   precision    recall  f1-score  support  accuracy\n",
            "0   0.539683  1.000000  0.701031       34  0.553846\n",
            "1   0.576271  0.971429  0.723404       35  0.600000\n",
            "2   0.660377  1.000000  0.795455       35  0.723077\n",
            "3   0.600000  0.942857  0.733333       35  0.630769\n",
            "4   0.666667  0.971429  0.790698       35  0.723077\n",
            "5   0.729167  1.000000  0.843373       35  0.800000\n",
            "6   0.813953  1.000000  0.897436       35  0.876923\n",
            "7   0.640000  0.914286  0.752941       35  0.676923\n",
            "8   0.794118  0.771429  0.782609       35  0.769231\n",
            "9   0.685714  0.705882  0.695652       34  0.671875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Hx8mYEyVbZ",
        "colab_type": "text"
      },
      "source": [
        "Resultado dos k-fold para cada valor de c "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACFrA5kCyVba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9bd0efb9-91e9-48a5-9836-8e29d0ee1deb"
      },
      "source": [
        "result_recursive"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)     method\n",
              "  0   0.557377  1.000000  0.715789       34  0.584615     0.001  Recursive\n",
              "  1   0.640000  0.914286  0.752941       35  0.676923     0.001  Recursive\n",
              "  2   0.761905  0.914286  0.831169       35  0.800000     0.001  Recursive\n",
              "  3   0.636364  0.800000  0.708861       35  0.646154     0.001  Recursive\n",
              "  4   0.727273  0.914286  0.810127       35  0.769231     0.001  Recursive\n",
              "  5   0.761905  0.914286  0.831169       35  0.800000     0.001  Recursive\n",
              "  6   0.850000  0.971429  0.906667       35  0.892308     0.001  Recursive\n",
              "  7   0.644444  0.828571  0.725000       35  0.661538     0.001  Recursive\n",
              "  8   0.857143  0.685714  0.761905       35  0.769231     0.001  Recursive\n",
              "  9   0.750000  0.705882  0.727273       34  0.718750     0.001  Recursive],\n",
              " [0.1,    precision    recall  f1-score  support  accuracy  Param(c)     method\n",
              "  0   0.532258  0.970588  0.687500       34  0.538462       0.1  Recursive\n",
              "  1   0.629630  0.971429  0.764045       35  0.676923       0.1  Recursive\n",
              "  2   0.711111  0.914286  0.800000       35  0.753846       0.1  Recursive\n",
              "  3   0.640000  0.914286  0.752941       35  0.676923       0.1  Recursive\n",
              "  4   0.738095  0.885714  0.805195       35  0.769231       0.1  Recursive\n",
              "  5   0.772727  0.971429  0.860759       35  0.830769       0.1  Recursive\n",
              "  6   0.850000  0.971429  0.906667       35  0.892308       0.1  Recursive\n",
              "  7   0.652174  0.857143  0.740741       35  0.676923       0.1  Recursive\n",
              "  8   0.916667  0.628571  0.745763       35  0.769231       0.1  Recursive\n",
              "  9   0.727273  0.705882  0.716418       34  0.703125       0.1  Recursive],\n",
              " [0.25,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)     method\n",
              "  0   0.532258  0.970588  0.687500       34  0.538462      0.25  Recursive\n",
              "  1   0.596491  0.971429  0.739130       35  0.630769      0.25  Recursive\n",
              "  2   0.711111  0.914286  0.800000       35  0.753846      0.25  Recursive\n",
              "  3   0.634615  0.942857  0.758621       35  0.676923      0.25  Recursive\n",
              "  4   0.711111  0.914286  0.800000       35  0.753846      0.25  Recursive\n",
              "  5   0.744681  1.000000  0.853659       35  0.815385      0.25  Recursive\n",
              "  6   0.829268  0.971429  0.894737       35  0.876923      0.25  Recursive\n",
              "  7   0.666667  0.914286  0.771084       35  0.707692      0.25  Recursive\n",
              "  8   0.884615  0.657143  0.754098       35  0.769231      0.25  Recursive\n",
              "  9   0.750000  0.705882  0.727273       34  0.718750      0.25  Recursive],\n",
              " [0.5,    precision    recall  f1-score  support  accuracy  Param(c)     method\n",
              "  0   0.539683  1.000000  0.701031       34  0.553846       0.5  Recursive\n",
              "  1   0.576271  0.971429  0.723404       35  0.600000       0.5  Recursive\n",
              "  2   0.673469  0.942857  0.785714       35  0.723077       0.5  Recursive\n",
              "  3   0.611111  0.942857  0.741573       35  0.646154       0.5  Recursive\n",
              "  4   0.708333  0.971429  0.819277       35  0.769231       0.5  Recursive\n",
              "  5   0.744681  1.000000  0.853659       35  0.815385       0.5  Recursive\n",
              "  6   0.833333  1.000000  0.909091       35  0.892308       0.5  Recursive\n",
              "  7   0.666667  0.914286  0.771084       35  0.707692       0.5  Recursive\n",
              "  8   0.827586  0.685714  0.750000       35  0.753846       0.5  Recursive\n",
              "  9   0.727273  0.705882  0.716418       34  0.703125       0.5  Recursive],\n",
              " [0.75,\n",
              "     precision    recall  f1-score  support  accuracy  Param(c)     method\n",
              "  0   0.539683  1.000000  0.701031       34  0.553846      0.75  Recursive\n",
              "  1   0.576271  0.971429  0.723404       35  0.600000      0.75  Recursive\n",
              "  2   0.666667  0.971429  0.790698       35  0.723077      0.75  Recursive\n",
              "  3   0.611111  0.942857  0.741573       35  0.646154      0.75  Recursive\n",
              "  4   0.693878  0.971429  0.809524       35  0.753846      0.75  Recursive\n",
              "  5   0.729167  1.000000  0.843373       35  0.800000      0.75  Recursive\n",
              "  6   0.833333  1.000000  0.909091       35  0.892308      0.75  Recursive\n",
              "  7   0.653061  0.914286  0.761905       35  0.692308      0.75  Recursive\n",
              "  8   0.806452  0.714286  0.757576       35  0.753846      0.75  Recursive\n",
              "  9   0.705882  0.705882  0.705882       34  0.687500      0.75  Recursive],\n",
              " [1,    precision    recall  f1-score  support  accuracy  Param(c)     method\n",
              "  0   0.539683  1.000000  0.701031       34  0.553846         1  Recursive\n",
              "  1   0.576271  0.971429  0.723404       35  0.600000         1  Recursive\n",
              "  2   0.660377  1.000000  0.795455       35  0.723077         1  Recursive\n",
              "  3   0.600000  0.942857  0.733333       35  0.630769         1  Recursive\n",
              "  4   0.666667  0.971429  0.790698       35  0.723077         1  Recursive\n",
              "  5   0.729167  1.000000  0.843373       35  0.800000         1  Recursive\n",
              "  6   0.813953  1.000000  0.897436       35  0.876923         1  Recursive\n",
              "  7   0.640000  0.914286  0.752941       35  0.676923         1  Recursive\n",
              "  8   0.794118  0.771429  0.782609       35  0.769231         1  Recursive\n",
              "  9   0.685714  0.705882  0.695652       34  0.671875         1  Recursive]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdJkXuJPyVbe",
        "colab_type": "text"
      },
      "source": [
        "Calcula a média das medidas de cada parâmetro c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acLGXhWCyVbf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "6893a514-7509-4ecf-ddec-ab481aa227f7"
      },
      "source": [
        "result_recursive_mean = calcula_media(result_recursive)\n",
        "result_recursive_mean"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.718641</td>\n",
              "      <td>0.864874</td>\n",
              "      <td>0.777090</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.731875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.716993</td>\n",
              "      <td>0.879076</td>\n",
              "      <td>0.778003</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.728774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.250</td>\n",
              "      <td>0.706082</td>\n",
              "      <td>0.896218</td>\n",
              "      <td>0.778610</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.724183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.690841</td>\n",
              "      <td>0.913445</td>\n",
              "      <td>0.777125</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.716466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.681550</td>\n",
              "      <td>0.919160</td>\n",
              "      <td>0.774406</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.710288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.670595</td>\n",
              "      <td>0.927731</td>\n",
              "      <td>0.771593</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.702572</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "0  0.001        0.718641  ...          34.8       0.731875\n",
              "1  0.100        0.716993  ...          34.8       0.728774\n",
              "2  0.250        0.706082  ...          34.8       0.724183\n",
              "3  0.500        0.690841  ...          34.8       0.716466\n",
              "4  0.750        0.681550  ...          34.8       0.710288\n",
              "5  1.000        0.670595  ...          34.8       0.702572\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68ZPmfziyVbk",
        "colab_type": "text"
      },
      "source": [
        "Obtém o resultado da maior média de acurácia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2q7ZKnsyVbl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "fd76d5db-0616-4cc6-c54d-02b8a576ddcd"
      },
      "source": [
        "best_accuracy_recursive = pd.Series(result_recursive_mean.iloc[result_recursive_mean['accuracy_mean'].idxmax()], \n",
        "                          name='Recursive Feature')\n",
        "best_recursive = pd.DataFrame(best_accuracy_recursive)\n",
        "best_recursive"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Recursive Feature</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_mean</th>\n",
              "      <td>0.718641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_mean</th>\n",
              "      <td>0.864874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score_mean</th>\n",
              "      <td>0.777090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support_mean</th>\n",
              "      <td>34.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy_mean</th>\n",
              "      <td>0.731875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Recursive Feature\n",
              "c                        0.001000\n",
              "precision_mean           0.718641\n",
              "recall_mean              0.864874\n",
              "f1_score_mean            0.777090\n",
              "support_mean            34.800000\n",
              "accuracy_mean            0.731875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zd_HCB-yVbr",
        "colab_type": "text"
      },
      "source": [
        "# Junta todos os resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdvIGYH5yVbs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "40e56508-bffd-41c8-f82b-e36d3263922f"
      },
      "source": [
        "result = pd.concat([best_all_features, best_pca, best_chi, best_recursive], axis=1)\n",
        "\n",
        "print(\"Média das métricas geradas pelo processamento de cada dataset\")\n",
        "result.transpose()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Média das métricas geradas pelo processamento de cada dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>precision_mean</th>\n",
              "      <th>recall_mean</th>\n",
              "      <th>f1_score_mean</th>\n",
              "      <th>support_mean</th>\n",
              "      <th>accuracy_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>All Features</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.733687</td>\n",
              "      <td>0.781597</td>\n",
              "      <td>0.739795</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.714880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PCA</th>\n",
              "      <td>0.500</td>\n",
              "      <td>0.680531</td>\n",
              "      <td>0.870504</td>\n",
              "      <td>0.749448</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.687188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Chi Squared</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.756781</td>\n",
              "      <td>0.815966</td>\n",
              "      <td>0.768260</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.741058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Recursive Feature</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.718641</td>\n",
              "      <td>0.864874</td>\n",
              "      <td>0.777090</td>\n",
              "      <td>34.8</td>\n",
              "      <td>0.731875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       c  precision_mean  ...  support_mean  accuracy_mean\n",
              "All Features       0.100        0.733687  ...          34.8       0.714880\n",
              "PCA                0.500        0.680531  ...          34.8       0.687188\n",
              "Chi Squared        0.001        0.756781  ...          34.8       0.741058\n",
              "Recursive Feature  0.001        0.718641  ...          34.8       0.731875\n",
              "\n",
              "[4 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nPWiXY29cEu",
        "colab_type": "text"
      },
      "source": [
        "# Gerar Graficos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFhU-Fp2yVbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "12246aa3-8482-4ea3-afdd-5d48b80e8625"
      },
      "source": [
        "frames = result_all_features_g +  result_pca_g + result_chi_g + result_recursive_g\n",
        "\n",
        "tips = pd.concat(frames)\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "fig, ax = plt.subplots(3,1,figsize=[10,25])\n",
        "\n",
        "g = sns.barplot(ax=ax[0], x=\"method\", y=\"accuracy\", hue=\"Param(c)\", data=tips,capsize=.05)\n",
        "g.set(ylim=(0,1),yticks=np.arange(0,1.1,0.1).tolist())\n",
        "g.set_ylabel(\"Accuracy\",fontsize=30)\n",
        "g.set_title('Classificador Naive Bayes',fontsize=30)\n",
        "g = sns.barplot(ax=ax[1], x=\"method\", y=\"recall\", hue=\"Param(c)\", data=tips,capsize=.05);\n",
        "g.set(ylim=(0,1),yticks=np.arange(0,1.1,0.1).tolist())\n",
        "g.set_ylabel(\"Recall\",fontsize=30)\n",
        "g = sns.barplot(ax=ax[2], x=\"method\", y=\"precision\", hue=\"Param(c)\", data=tips,capsize=.05);\n",
        "g.set(ylim=(0,1),yticks=np.arange(0,1.1,0.1).tolist())\n",
        "g.set_ylabel(\"Precision\",fontsize=30)\n",
        "plt.close(2)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x1800 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}